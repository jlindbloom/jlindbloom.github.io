[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Hierarchical regularization via an auxiliary variable magic trick.\n\n\n\n\n\n\nJul 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome useful formulas for Gaussian posteriors.\n\n\n\n\n\n\nJul 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFun with BCCB matrices.\n\n\n\n\n\n\nJul 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nLet’s see if we can sample this directly using a hierarchical representation.\n\n\n\n\n\n\nJun 15, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jonathan Lindbloom",
    "section": "",
    "text": "I am an applied mathematics PhD student at Dartmouth College, originally from Dallas, Texas. I like learning about computational methods for solving (inverse) problems and quantifying uncertainty."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jonathan Lindbloom",
    "section": "Education",
    "text": "Education\nDartmouth College | Hanover, NH\nPhD in Mathematics\nSept 2021 - Present\nSouthern Methodist University | Dallas, TX\nBS in Mathematics, BBA in Finance\nAug 2017 - May 2021"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Multiplicative Denoising with Uncertainty Quantification for Synthetic Aperture Radar Imaging. SIAM Conference on Uncertainty Quantification. Atlanta, GA. Apr 14, 2022. Slides"
  },
  {
    "objectID": "misc/rendering_research_page.html",
    "href": "misc/rendering_research_page.html",
    "title": "Jonathan Lindbloom",
    "section": "",
    "text": "def readable_list(_s):\n  if len(_s) < 3:\n    return ' and '.join(map(str, _s))\n  *a, b = _s\n  return f\"{', '.join(map(str, a))}, and {b}\"\n\ndef button(url, str, icon):\n    icon_base = icon[:2]\n    return f\"\"\"<a class=\"btn btn-outline-dark btn-sm\", href=\"{url}\" target=\"_blank\" rel=\"noopener noreferrer\">\n        <i class=\"{icon_base} {icon}\" role='img' aria-label='{str}'></i>\n        {str}\n    </a>\"\"\"\nos.chdir(\"..\")  \n# Load all data\ntalks_data = yaml.safe_load(open(\"talks.yaml\"))\n# Dict for storing all info\npub_strs = {\n    \"talks\": []\n}\n\n# Iterate over talks\nfor _, data in talks_data.items():\n\n    # Required attributes\n    talk_title = data[\"title\"]\n    date = data[\"date\"]\n    me = data[\"me\"]\n    \n    # Make author string\n    coauthors = data.get(\"coauthors\")\n    if coauthors is None:\n        authors = me\n    else:\n        authors = [me]\n        authors += coauthors\n        authors = readable_list(authors)\n\n    # List for storing buttons\n    buttons = []\n\n    # If we have slides, add a button\n    slides_link = data.get(\"slides_link\")\n    if slides_link is not None:\n        buttons.append(button(slides_link, \"Slides\", \"bi-file-earmark-pdf\"))\n\n    \n    talk_str = f\"{authors} {talk_title}. {date}\"\n    \n\n    pub_strs[\"talks\"].append(\n        \"<li class='list-group-item'>\" + talk_str + \"<br>\" + \" \".join(buttons) + \"</li>\"\n    )\n\n\n\n    print(authors)\n\nLindbloom, J., Gelb, A., and Parno, M.\n\n\nbuttons = []\npreprint = data.get(\"preprint\")\nif preprint is not None:\n    buttons.append(button(preprint, \"Preprint\", \"bi-file-earmark-pdf\"))\nfor year in sorted(pub_strs[\"pubs\"].keys(), reverse=True):\n    display(Markdown(f\"### {year}\" + \"{#\" + f\"published-{year}\" + \"}\"))\n    display(HTML(\n        \"<ul class='list-group list-group-flush'>\" + '\\n'.join(pub_strs[\"pubs\"][year]) + \"</ul>\"\n    ))\npub_str = f'{author_str}. ({year_str}) \"{title_str}.\"'\nfor year in sorted(pub_strs[\"pubs\"].keys(), reverse=True):\n    display(Markdown(f\"### {year}\" + \"{#\" + f\"published-{year}\" + \"}\"))\n    display(HTML(\n        \"<ul class='list-group list-group-flush'>\" + '\\n'.join(pub_strs[\"pubs\"][year]) + \"</ul>\"\n    ))\nfor _, data in yaml_data.items():\n    title_str = data[\"title\"]\n    authors = data.get(\"authors\", [\"me\"])\n    authors = [\n        aut if aut != \"me\" else \"<strong>Drew Dimmery</strong>\" for aut in authors\n    ]\n    author_str = readable_list(authors)\n    year_str = data[\"year\"]\n\n    buttons = []\n    preprint = data.get(\"preprint\")\n    if preprint is not None:\n        buttons.append(button(preprint, \"Preprint\", \"bi-file-earmark-pdf\"))\n\n    github = data.get(\"github\")\n    if github is not None:\n        buttons.append(button(github, \"Github\", \"bi-github\"))\n\n    pub_url = data.get(\"published_url\")\n    venue = data.get(\"venue\")\n    working_paper = pub_url is None\n    \n    pub_str = f'{author_str}. ({year_str}) \"{title_str}.\"'\n\n    if venue is not None:\n        pub_str += f\" <em>{venue}</em>\"\n\n    if working_paper:\n        if year_str not in pub_strs[\"wps\"]:\n            pub_strs[\"wps\"][year_str] = []\n        pub_strs[\"wps\"][year_str].append(\n            \"<li class='list-group-item'>\" + pub_str + \"<br>\" + \" \".join(buttons) + \"</li>\"\n        )\n    else:\n        if year_str not in pub_strs[\"pubs\"]:\n            pub_strs[\"pubs\"][year_str] = []\n        buttons.append(button(pub_url, \"Published\", \"ai-archive\"))\n        pub_strs[\"pubs\"][year_str].append(\n            \"<li class='list-group-item'>\" + pub_str + \"<br>\" + \" \".join(buttons) + \"</li>\"\n        )"
  },
  {
    "objectID": "posts/auxiliary_magic.html",
    "href": "posts/auxiliary_magic.html",
    "title": "Auxiliary Magic",
    "section": "",
    "text": "\\[\nx^\\star = \\text{argmin}_x \\,\\, \\| A x - y \\|_2^2 + \\mathcal{R}(x)\n\\tag{1}\\]\nwhere the regularization term was\n\\[\n\\mathcal{R}(x) = \\gamma \\| L x \\|_2^2 = \\gamma x^T L^T L x.\n\\]\nIn this post, our goal is to: - describe a hierarchical prior and a method that can give us a better image reconstruction, - walk through a “magic” trick that will speed up our method, - and look at using CuPy to accelerate our reconstruction using a GPU.\n\nProbabilistic Inverse Problems\nWhile the problem posed in Equation 1 is completely deterministic, we can actually think of it as haven arisen from a probabilistic model. Suppose that\n\\[\n\\begin{align*}\nx &\\sim \\mathcal{N}\\left( 0, \\left(\\gamma L^T L \\right)^{-1} \\right), \\\\\ny \\, | \\, x &\\sim \\mathcal{N}\\left( A x, I \\right).\n\\end{align*}\n\\]\nThen our corresponding density functions are\n\\[\n\\begin{align*}\n\\pi(x) &\\propto \\exp\\left\\{ - \\gamma x^T L^T L  x \\right\\}, \\\\\n\\pi(y \\, | \\, x) &\\propto \\exp\\left\\{ - \\| A x - y \\|_2^2 \\right\\},\n\\end{align*}\n\\]\nand by Bayes’ theorem the posterior density for \\(x \\, | \\, y\\) is given as\n\\[\n\\pi(x \\, | \\, y) \\propto \\exp\\left\\{ - \\| A x - y \\|_2^2 \\right\\} \\times \\exp\\left\\{ - \\gamma x^T L^T L  x \\right\\}.\n\\]\nThe MAP estimate of \\(x\\) is then given as\n\\[\nx^\\star = \\text{argmax}_x \\,\\, \\pi(x \\, | \\, y),\n\\]\nwhich is equivalent to\n\\[\n\\begin{align*}\nx^\\star &= \\text{argmin}_x \\,\\, - \\log \\pi(x \\, | \\, y) \\\\\n&= \\text{argmin}_x \\,\\, \\gamma \\| L x \\|_2^2 = \\gamma x^T L^T L x,\n\\end{align*}\n\\]\nwhich is exactly Equation 1. The role of \\(\\mathcal{R}(x)\\) can then be seen as contributing a prior of\n\\[\n\\pi(x) \\propto \\exp\\left\\{ - \\mathcal{R}(x) \\right\\}\n\\]\ninto the inference problem.\n\n\nA Hierarchical Prior\nOne reason it can be useful to think probabilistically is because we can motivate different choices of the regularizer \\(\\mathcal{R}(x)\\). If we pick\n\\[\nLx \\sim \\mathcal{N}\\left(0, \\frac{1}{\\gamma} I \\right)\n\\]\nas our prior (which corresponds to \\(\\mathcal{R}(x) = \\gamma x^T L^T L x\\)), then we are saying that we believe that the discrete gradients in our image are distributed according to zero-mean Gaussian with variance \\(\\gamma^{-1}\\). We can tweak the strength of the prior by adjusting \\(\\gamma\\) and in turn its influence on our reconstructed image, but note that the same \\(\\gamma\\) governs of the entire discrete gradient in the image. Thus we might think to introduce a hierarchical prior on the discrete gradient that could try to (loosely) capture the fact that in some regions in an image the discrete gradient will be much larger than it is elsewhere. Define the prior\n\\[\n\\begin{align*}\n\\beta^H_{i,j}, \\beta^V_{i,j} &\\sim \\Gamma \\left( c, d \\right), \\\\\nL x &\\sim \\mathcal{N} \\left(0, B_{\\beta} \\right),\n\\end{align*}\n\\tag{2}\\]\nwhich has density\n\\[\n\\pi(x, \\beta) = \\pi(x \\, | \\, \\beta) \\pi(\\beta) \\propto \\det \\left( B_{\\beta} \\right)^{1/2} \\exp\\left\\{ - x^T L^T B_{\\beta} L x  \\right\\} \\times \\pi(\\beta).\n\\]\nHere \\(\\left( \\cdot \\right)^{V/H}\\) represent the fact that we are assigining two different hyper-parameter to govern the gradient in each the vertical and horizontal directions, \\(\\Gamma\\left(c, d \\right)\\) represents the gamma density function,\n\\[\nB_{\\beta} = \\text{diag}\\left( \\beta^V_{1,1}, \\ldots, \\beta^V_{m,n}, \\beta^H_{1,1}, \\ldots, \\beta^H_{m,n} \\right),\n\\]\nand\n\\[\n\\pi(\\beta) \\propto \\left( \\prod_{i,j}^{mn} \\Gamma( \\beta_{i,j}^H | c, d) \\right) \\times \\left( \\prod_{i,j}^{mn} \\Gamma( \\beta_{i,j}^V | c, d) \\right)\n\\]\nmeaning that all hyper-parameters are assumed to be independent of one another. The reason we use a \\(\\Gamma\\) distribution for the hyper-parameter is because it is a conjugate prior for a Gaussian, meaning that we can determine certain relevant conditional distributions analytically.\n\n\nThe New Posterior\nUsing our prior Equation 2, the full posterior in our original model (with an extra factor of \\(\\frac{1}{2}\\), as well as a noise precision parameter \\(\\alpha\\)) is given as\n\\[\n\\pi(x, \\beta \\, | \\, y) \\propto \\exp\\left\\{ - \\frac{\\alpha}{2} \\| A x - y \\|_2^2 \\right\\} \\times \\det \\left( B_{\\beta} \\right)^{1/2} \\exp\\left\\{ - \\frac{1}{2} x^T L^T B_{\\beta} L x  \\right\\} \\times \\pi(\\beta).\n\\tag{3}\\]\nIn the sampling setting, a common approach that could be used to draw samples from this posterior is Gibbs sampling, which involves iteratively drawing from the conditionals of each variable in the density given all of the others. In this case, our conditionals would be\n\\[\n\\begin{align*}\n\\pi(x \\, | \\, \\beta, y) &\\propto \\exp\\left\\{- \\frac{\\alpha}{2}\\| A x - y \\|_2^2 \\right\\} \\times  \\exp\\left\\{ - x^T L^T B_{\\beta} L x  \\right\\}, \\\\\n\\pi(\\beta \\, | \\, x, y) &\\propto \\det \\left( B_{\\beta} \\right)^{1/2} \\exp\\left\\{ - x^T L^T B_{\\beta} L x  \\right\\} \\times \\pi(\\beta).\n\\end{align*}\n\\]\nDue to conjugacy, we can recognize the first conditional to be a Gaussian and the second to be a Gamma distribution. Specifically, we have \\(x \\, | \\, \\beta, y \\sim \\mathcal{N}\\left(\\mu_1, Q_1^{-1} \\right)\\) with\n\\[\n\\begin{align*}\nQ_1 &= \\frac{1}{\\alpha} A^T A + L^T B_{\\beta} L, \\\\\n\\mu_1 &= Q_1^{-1} \\left( \\frac{1}{\\alpha} A^T y \\right).\n\\end{align*}\n\\]\nFor \\(\\beta \\, | \\, x, y\\) we have\n\\[\n\\begin{align*}\n\\beta_{i,j} = \\Gamma\\left( \\frac{1}{2} + c, \\frac{1}{2}[L x]_{i,j}^2 + d \\right),\n\\end{align*}\n\\]\nwhere how you deal with \\(\\beta^V\\)/\\(\\beta^H\\) becomes clear when you think about the shapes of these operations.\nRather than code a Gibbs sampling algorithm, we will consider the (BCD) of [1], which is essentially an optimization technique that iteratively draws from the mean of each the conditionals. We iterate\n\\[\n\\begin{align*}\nx^{k+1} &= \\mathbb{E}_{\\pi}\\left( x \\, | \\, \\beta^k, y \\right), \\\\\n\\beta^{k+1} &= \\mathbb{E}_{\\pi}\\left( \\beta \\, | \\, x^{k+1}, y \\right),\n\\end{align*}\n\\]\nuntil the solution converges. One thing to note is that we will generally not settle into a global minimum, since the posterior (with no restrictions on \\(c\\) and \\(d\\)) is not log-concave due to the use of a Gamma hyper-prior on the \\(\\beta_{i,j}\\).\n\n\nAn Inconvenience\nFor the purposes of this post, we will assume that \\(A\\) is a BCCB blurring matrix and that \\(L\\) is an approximation to the discrete gradient with periodic boundary conditions such that \\(L^T L\\) is also BCCB. These were the nice assumptions in the previous post that allowed us to reduce all of our linear system solves to FFTs/IFFs and diagonal matrix operations, as well as efficiently sample in the Fourier domain.\nNote that while we have an analytic expression for the distribution \\(x \\, | \\beta, y\\), we can no longer use our BCCB assumptions to avoid linear system solves. Letting\n\\[\n\\begin{align*}\nA &= F^H \\Lambda F, \\\\\nL^T L &= F^H \\Pi F,\n\\end{align*}\n\\]\ninserting this into \\(Q_1^{-1} = \\left( A^T A + L^T B_{\\beta} L \\right)^{-1}\\) we now get\n\\[\n\\begin{align*}\nQ_1^{-1} &= \\left( \\frac{1}{\\alpha} A^T A +  L^T B_{\\beta} L \\right)^{-1} \\\\\n&= \\left( \\frac{1}{\\alpha} F^H \\Lambda F F^H \\Lambda F +  L^T B_{\\beta} L \\right)^{-1} \\\\\n&= \\left( \\frac{1}{\\alpha} F^H \\Lambda^2 F +  L^T B_{\\beta} L \\right)^{-1}  \\\\\n\\end{align*}\n\\]\nwhere unlike before we are now stuck. The problem is that \\(B_{\\beta}\\) is “sandwiched” in-between \\(L^T\\) and \\(L\\), which prevents us from using the diagonalization of \\(L^T L\\). While it is not the end of the world to have to solve a linear system using a sparse solver or some other method, we still really would like to use the BCCB assumption to our advantage if we can.\n\n\nAn Auxiliary Variable Magic Trick\nIt turns out there is a way to get around this, via a “magic trick” of [2]. I also discuss this in this post on Gaussian posterior formulas. For the moment let us fix \\(B_{\\beta}\\) to be a constant matrix. Out of the blue, define a new random variable via\n\\[\n\\begin{align*}\n    u \\, | \\, x, y &\\sim \\mathcal{N}\\left( H x, Q^{-1}  \\right),\n\\end{align*}\n\\]\nwhere we are free to choose \\(H\\) and \\(Q\\) (as long as they define a valid distribution). Then, if we consider the joint density of \\(u, x \\, | \\, y\\) we have that\n\\[\n\\begin{align*}\n\\pi(u, x \\, | \\, y) &= \\pi(u \\, | \\, x, y) \\times \\pi(x \\, | \\, y) \\times \\pi(x) \\\\\n&\\propto \\exp\\left\\{ -\\frac{1}{2} \\left( H x - u \\right)^T Q \\left( H x - u \\right)  \\right\\} \\times \\exp\\left\\{ -\\frac{1}{2} x^T L^T B L x  \\right\\} \\times \\pi(x \\, | \\, y) \\\\\n&\\propto \\exp\\left\\{ -\\frac{1}{2} \\left[ \\left( H x - u \\right)^T Q \\left( H x - u \\right) + x^T L^T B L x  \\right] \\right\\} \\times \\pi(x \\, | \\, y) \\\\\n&\\propto \\exp\\left\\{ -\\frac{1}{2} \\left[ x^T H^T Q H x + u^T Q u - 2 x^T H^T Q u + x^T L^T B L x  \\right] \\right\\} \\times \\pi(x \\, | \\, y) \\\\\n&\\propto \\exp\\left\\{ -\\frac{1}{2} \\left[ x^T P x + u^T Q u - 2 x^T H^T Q u  \\right] \\right\\} \\times \\pi(x \\, | \\, y) \\\\\n\\end{align*}\n\\]\nwhere we have defined\n\\[\nP = H^T Q H + L^T B L.\n\\]\nWe are free to choose \\(H\\) and \\(Q\\), so let’s make the magic choice\n\\[\n\\begin{align*}\nH &= L, \\\\\nQ &= \\frac{1}{\\lambda} I  - B.\n\\end{align*}\n\\]\nHere \\(\\lambda > 0\\) is a constant we are free to choose, so long as \\(\\lambda < \\frac{1}{\\| B \\|}\\) which makes \\(Q\\) positive semi-definite as is required of a valid precision matrix. Inserting this choice and conditioning on \\(u\\), we obtain\n\\[\nP = \\frac{1}{\\lambda} L^T L\n\\]\nand\n\\[\n\\begin{align*}\n\\pi(x \\, | \\, u,  y) &\\propto \\exp\\left\\{ -\\frac{1}{2} \\left[ \\frac{1}{\\lambda} x^T L^T L x - 2 x^T L^T Q u  \\right] \\right\\} \\times \\pi(x \\, | \\, y). \\\\\n\\end{align*}\n\\]\nNow, we define yet another random variable\n\\[\nv = Qu = \\left( \\frac{1}{\\lambda} I  - B \\right) u,\n\\]\ninsert into the density above to get\n\\[\n\\begin{align*}\n\\pi(x \\, | \\, v,  y) &\\propto \\exp\\left\\{ -\\frac{1}{2} \\left[ \\frac{1}{\\lambda} x^T L^T L x - 2 x^T L^T v \\right] \\right\\} \\times \\pi(x \\, | \\, y), \\\\\n\\end{align*}\n\\]\nand finally complete the square in the exponential to finally arrive at\n\\[\n\\begin{align*}\n\\pi(x \\, | \\, v,  y) &\\propto \\exp\\left\\{ -\\frac{1}{2 \\lambda} \\| L x - \\lambda v  \\|_2^2  \\right\\} \\times \\pi(x \\, | \\, y), \\\\\n\\end{align*}\n\\]\nwhich is the density of the Gaussian \\(\\mathcal{N}\\left( \\mu_2, Q_2^{-1} \\right)\\) where\n\\[\n\\begin{align*}\nQ_2 &= \\frac{1}{\\lambda} L^T L + \\frac{1}{\\alpha} A^T A, \\\\\n\\mu_2 &= Q_2^{-1} \\left( \\frac{1}{\\alpha} A^T y +  L^T \\left( \\frac{1}{\\lambda} I \\right) \\left( \\lambda v \\right)  \\right) \\\\\n&= Q_2^{-1} \\left( \\frac{1}{\\alpha} A^T y +  L^T  v \\right).\n\\end{align*}\n\\]\nNote that we could do this since the missing term in the square was independent of \\(x\\). Now we have what we wanted: the matrix \\(B\\) has disappeared after conditioning on \\(v\\), and we can now take advantage of our BCCB assumption for this conditional. This is all only useful so long as the conditional distribution $v , | , x, y $ is also nice, which is the Gaussian \\(\\mathcal{N}\\left( \\mu_3, Q_3^{-1} \\right)\\) where\n\\[\n\\begin{align*}\nQ_3^{-1} &= \\frac{1}{\\lambda} I - B, \\\\\n\\mu_3 &= \\left( \\frac{1}{\\lambda} I - B \\right) L x,\n\\end{align*}\n\\]\nwhich is nice to work with.\n\n\nThe Augmented Hierarchical Model\nIt turns out that when we allow \\(B = B_{\\beta}\\) to vary with hyper-parameters, everything we just did stays exactly the same, except we must choose a new \\(\\lambda= \\lambda(B_{\\beta})\\) such that \\(\\lambda < \\frac{1}{\\| B_\\beta \\|}\\) for the current values of the hyper-parameters. We have already discussed the conditionals \\(x \\, | \\, v, \\beta, y\\) and \\(v \\, | \\, x, \\beta, y\\) which are the same as before, but it turns out that the conditional \\(\\beta \\, | \\, x, v, y\\) is not trivial. As far as I know, this conditional is no longer known directly. So is all of our work for nothing? Fortunately, no, due to a technique called partially collapsed Gibbs sampling (PCGS) [3]. Note that while we don’t know an analytic form for \\(\\beta \\, | \\, x, v, y\\), we do know the form of \\(\\beta \\, | \\, x, y\\) which is the same as it was in the model before we introduced \\(v\\). PCGS is designed to take advantage of this by allowing us to only need access to \\(\\beta \\, | \\, x, y\\), with the price being that the order in which we iterate through the conditionals now matters.\nThus our discussion leads us to a modified version of the BCD algorithm that uses an auxiliary variable to simplify the calculations for the update of each variable:\n\\[\n\\begin{align*}\nx^{k+1} &= \\mathbb{E}_{\\pi}\\left( x \\, | \\, v^k, \\beta^k, y \\right), \\\\\nv^{k+1} &= \\mathbb{E}_{\\pi}\\left( v \\, | \\, x^k, \\beta^k, y \\right), \\\\\n\\beta^{k+1} &= \\mathbb{E}_{\\pi}\\left( \\beta \\, | \\, x^{k+1}, y \\right).\n\\end{align*}\n\\]\nWe will test this out by building off our code from my previous post on de-blurring.\n\n\n\nThe Code\nThe first thing we will do is some initial set-up for the problem such as setting up our differencing matrix and computing the diagonalizations of \\(A\\) and \\(L^T L\\). We will use the same blurred image from the previous post.\n%%capture\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.ndimage import gaussian_filter\nfrom PIL import Image, ImageOps\n\n\n# Load blurred image\nblurred_img = Image.open(\"blurred_grand_canyon.jpg\")\nblurred_img = ImageOps.grayscale(blurred_img)\nblurred_img = np.asarray(blurred_img)\nblurred_img = blurred_img/255\nM, N = blurred_img.shape\n\n\n# Define blur operator using sigma\nblur_sigma = 20.0\nblur_op = lambda x: gaussian_filter(x, blur_sigma, mode='wrap')\n\n\n# Compute diagonalization for the blurring operator\nrand_img = np.random.randn(M,N)\nmat_prod_rand_img = np.fft.fft2(blur_op(np.fft.ifft2(rand_img, norm='ortho')), norm='ortho')\nlam = mat_prod_rand_img/rand_img\nlam = np.real(lam) # We know it should be real\nlam_inv = 1/lam\n\n\n# Construct our differencing matrices\nd_mat_vert = sp.eye(N)\nd_mat_vert.setdiag(-1,k=-1)\nd_mat_vert = sp.csc_matrix(d_mat_vert)\nd_mat_vert[0,-1] = -1 # For periodic BCs\neye_horiz = sp.eye(M)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(M)\nd_mat_horiz.setdiag(-1,k=-1)\nd_mat_horiz = sp.csc_matrix(d_mat_horiz)\nd_mat_horiz[0,-1] = -1 # For periodic BCs\neye_vert = sp.eye(N)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\n\n# Make L, L^T, and L^T L\nL = sp.vstack([d_mat_one, d_mat_two])\nLt = L.T\nLtL = Lt @ L \n\n\n# Next, we need to compute $\\Pi$ like we did earlier with $\\Lambda$.\nrand_img = np.random.randn(M,N)\nmat_prod_rand_img = np.fft.fft2( (LtL @ np.fft.ifft2(rand_img, norm='ortho').flatten() ).reshape((M,N)), norm='ortho')\npi = mat_prod_rand_img/rand_img\npi = np.real(pi) # We know it should be real\npi_inv = 1/pi\nIn order to quickly perform the FFTs/IFFTs needed for the algorithm, we will leverage a GPU via CuPy. I had previously tried doing this algorithm on CPUs with just NumPy alone and found that it took about one iteration per second, whereas with CuPy we can get about 500 iterations per 30 seconds (about 17 iterations per second).\nIn terms of NumPy vs. CuPy, the main things to keep in mind are that 1. you must explicitly transfer arrays between the CPU and GPU, and 2. while on the GPU, you must use functions that act on CuPy arrays.\nBoth of these are easy to do. For this post, I will suffix all arrays that live on the GPU with _gpu. Let’s bring all of the things we just computed onto the GPU:\nimport cupy as cp\n\n# Bring diagonalizations Pi and Lambda onto GPU\npi_gpu = cp.asarray(pi)\nlam_gpu = cp.asarray(lam)\n\n# Bring L, Lt, and LtL onto the GPU\nL_gpu = cp.sparse.csc_matrix(L)\nLt_gpu = cp.sparse.csc_matrix(Lt)\nLtL_gpu = cp.sparse.csc_matrix(LtL)\nNote that scipy.sparse matrices are different from cupy.sparse matrices, and that we bring them onto the GPU by calling the CuPy constructor on the SciPy (CPU) version of the matrix.\nTo apply our blurring operator to a vector on the GPU, we will also need to use the CuPy version of gaussian_filter:\nfrom cupyx.scipy.ndimage import gaussian_filter as cupy_gaussian_filter \n\nblur_op_gpu = lambda x: cupy_gaussian_filter(x, blur_sigma, mode='wrap')\nLet’s make our arrays for \\(x\\), \\(v\\), \\(y\\), and \\(\\beta\\):\nx_gpu = cp.asarray(blurred_img) # Start x at the observed image\ny_gpu = cp.asarray(blurred_img) # The observation\nv_gpu = cp.asarray( L @ blurred_img.flatten() ) # Start v at Lx, this gives it the correct shape\nbetas_gpu = 10*cp.ones(2*M*N)\nRemember that we need to pick a new value of \\(\\lambda\\) each time we update \\(\\beta\\). We will make the choice\n\\[\n\\lambda(\\beta) = \\frac{1}{2}\\left( \\frac{1}{\\| B_{\\beta} \\|} + 0\\right),\n\\]\nwhich should be half of the maximum value we would be allowed to pick for \\(\\lambda\\). I am not sure how picking \\(\\lambda\\) on the high or low side affects the performance, but I would guess that higher is better. In our code, I will refer to this \\(\\lambda\\) as lam_aux.\nlam_aux = ((1/cp.amax(betas_gpu)) + 0)/2\nWe also need to set some noise precision \\(\\alpha = \\sigma^{-2}\\), and we can go ahead and pre-compute \\(\\frac{1}{\\alpha} A^T y\\) which shows up in our formula for the conditional for \\(x\\).\nnoise_sigma = 0.1\nnoise_prec = 1/(noise_sigma**2)\nAty_gpu = noise_prec*blur_op_gpu(y_gpu)\nAnd finally, we must select the hyper-hyper-parametes \\(c\\) and \\(d\\). We will examine two choices, the first being the “uniformative” choice\n\\[\n\\begin{align*}\nc &= 1.0, \\\\\nd &= 10^{-20},\n\\end{align*}\n\\]\nwhich corresponds to a flat prior.\nc = 1.0\nd = 1e-20\nNow we are ready to perform the BCD algorithm for the reconstruction of \\(x\\) from \\(y\\). I will not go into the details, as all of the operations are just the implementation of the steps we outlined earlier.\n\nfrom fastprogress import progress_bar\n\n# Set some number of iterations\nn_iterations = 500\n\n# Now do BCD\nfor j in progress_bar(range(n_iterations)):\n    \n    ### Update x\n    Ltv_gpu = ( Lt_gpu @ v_gpu ).reshape((M,N))\n    b = Aty_gpu + Ltv_gpu\n    mu_x = cp.fft.fft2( b, norm='ortho')\n    diag_fourier_vec = 1/( (1/lam_aux)*(pi_gpu) + (noise_prec)*(lam_gpu**2) )\n    mu_x = diag_fourier_vec*mu_x\n    mu_x = cp.real(cp.fft.ifft2(mu_x, norm='ortho'))\n    x_gpu = mu_x\n    \n    ### Update betas\n    gamma_alpha = c + 0.5\n    gamma_beta = 0.5*( ( L_gpu @ x_gpu.flatten() )**2) + d \n    betas_gpu = gamma_alpha/gamma_beta\n\n    ### Update lam_aux\n    lam_aux = ( (1/cp.amax(betas_gpu) ) + 0)/2\n    \n    # Make sure that this is always true\n    assert np.all((1/lam_aux) - betas_gpu > 0), \"invalid choice of lam_aux was made\"\n    \n    ### Update v\n    v_gpu = ( (1/lam_aux) - betas_gpu )*( L_gpu @ x_gpu.flatten() )\n    \n# Bring arrays back to the cpu\nx = cp.asnumpy(x_gpu)\nv = cp.asnumpy(v_gpu)\nbetas = cp.asnumpy(betas_gpu)\n\n\n    \n        \n      \n      100.00% [500/500 00:29<00:00]\n    \n    \n\n\nLet’s look at the reconstruction, as well as \\(\\beta\\). Here I have plotted the betas on an inverse scale, so brighter pixels should correspond to a higher variance parameter. I have also artificially clipped the array to its 95th percentile, since there are some extremely large precision values that make the entire plot monotone.\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.facecolor'] = '#0F2537'\nplt.rcParams['text.color'] = 'white'\nplt.rcParams['axes.labelcolor'] = 'white'\nplt.rcParams['xtick.color'] = 'white'\nplt.rcParams['ytick.color'] = 'white'\n\nplot_name = \"uninformative_prior\"\nplot_which = \"Uniformative prior\"\n\nplt.figure(figsize=(13,13))\nplt.imshow(np.real(x), cmap='gray')\nplt.title(plot_which + \" \" + \"BCD reconstruction\")\nplt.show()\n\n\n\n\nbetas = betas.reshape((2,M,N))\nbetas_one, betas_two = betas[0,:,:], betas[1,:,:]\n\nplt.figure(figsize=(13,13))\nplt.imshow(np.clip(betas_one, a_min=None, a_max = np.quantile(betas_one.flatten(), 0.95)), cmap='Greens_r')\nplt.title(plot_which + \" \" + \"betas (horizontal)\")\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(13,13))\nplt.imshow(np.clip(betas_two, a_min=None, a_max = np.quantile(betas_two.flatten(), 0.95)), cmap='Greens_r')\nplt.title(plot_which + \" \" + \"betas (vertical)\")\nplt.show()\n\n\n\n\nNotice how you can (somewhat) trace out some of the edges in the image by the inferred variance parameters. Here is a comparison of the original, blurred, and reconstructed images:\n\n# Load original image\nimg = Image.open(\"grand_canyon.jpg\")\nimg = ImageOps.grayscale(img)\nimg = np.asarray(img)\nimg = img/255\n\nfig, axs = plt.subplots(1, 3, figsize=(26,26))\n\naxs[0].imshow(img, cmap='gray')\naxs[0].set_title(\"Original image\")\naxs[1].imshow(np.real(x), cmap='gray')\naxs[1].set_title(plot_which + \" \" + \"BCD reconstruction\")\naxs[2].imshow(blurred_img, cmap='gray')\naxs[2].set_title(\"Blurred image\")\n\nplt.show()\n\n\n\n\nThe last thing I will point out is that by changing the \\(c\\) and \\(d\\) hyper-hyper-parameters, you can change the degree of sparsity in the prior on \\(L x\\). As we did in a previous post, we can recover a prior that is marginally Cauchy with a special choice of \\(c\\) and \\(d\\). Let’s repeat the reconstruction with the choice\n\\[\n\\begin{align*}\nc &= 0.5, \\\\\nd &= 0.5(0.0005^2).\n\\end{align*}\n\\]\nnoise_sigma = 0.01\nc = 0.5\nd = 0.5*(0.0005**2)\n\nx_gpu = cp.asarray(blurred_img) # Start x at the observed image\ny_gpu = cp.asarray(blurred_img) # The observation\nv_gpu = cp.asarray( L @ blurred_img.flatten() ) # Start v at Lx, this gives it the correct shape\nbetas_gpu = 10*cp.ones(2*M*N)\n\nlam_aux = ((1/cp.amax(betas_gpu)) + 0)/2\nnoise_prec = 1/(noise_sigma**2)\nAty_gpu = noise_prec*blur_op_gpu(y_gpu)\n\n# Set some number of iterations\nn_iterations = 500\n\n# Now do BCD\nfor j in progress_bar(range(n_iterations)):\n    \n    ### Update x\n    Ltv_gpu = ( Lt_gpu @ v_gpu ).reshape((M,N))\n    b = Aty_gpu + Ltv_gpu\n    mu_x = cp.fft.fft2( b, norm='ortho')\n    diag_fourier_vec = 1/( (1/lam_aux)*(pi_gpu) + (noise_prec)*(lam_gpu**2) )\n    mu_x = diag_fourier_vec*mu_x\n    mu_x = cp.real(cp.fft.ifft2(mu_x, norm='ortho'))\n    x_gpu = mu_x\n    \n    ### Update betas\n    gamma_alpha = c + 0.5\n    gamma_beta = 0.5*( ( L_gpu @ x_gpu.flatten() )**2) + d \n    betas_gpu = gamma_alpha/gamma_beta\n\n    ### Update lam_aux\n    lam_aux = ( (1/cp.amax(betas_gpu) ) + 0)/2\n    \n    # Make sure that this is always true\n    assert np.all((1/lam_aux) - betas_gpu > 0), \"invalid choice of lam_aux was made\"\n    \n    ### Update v\n    v_gpu = ( (1/lam_aux) - betas_gpu )*( L_gpu @ x_gpu.flatten() )\n    \n# Bring arrays back to the cpu\nx = cp.asnumpy(x_gpu)\nv = cp.asnumpy(v_gpu)\nbetas = cp.asnumpy(betas_gpu)\n\n\n    \n        \n      \n      100.00% [500/500 00:29<00:00]\n    \n    \n\n\n\nplot_name = \"strong_cauchy_prior\"\nplot_which = \"Strong Cauchy prior\"\n\nplt.figure(figsize=(13,13))\nplt.imshow(np.real(x), cmap='gray')\nplt.title(plot_which + \" \" + \"BCD reconstruction\")\nplt.show()\n\n\n\n\nbetas = betas.reshape((2,M,N))\nbetas_one, betas_two = betas[0,:,:], betas[1,:,:]\n\nplt.figure(figsize=(13,13))\nplt.imshow(np.clip(betas_one, a_min=None, a_max = np.quantile(betas_one.flatten(), 0.95)), cmap='Greens')\nplt.title(plot_which + \" \" + \"betas (horizontal)\")\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(13,13))\nplt.imshow(np.clip(betas_two, a_min=None, a_max = np.quantile(betas_two.flatten(), 0.95)), cmap='Greens')\nplt.title(plot_which + \" \" + \"betas (vertical)\")\nplt.show()\n\n\n\n\n\nfig, axs = plt.subplots(1, 3, figsize=(26,26))\n\naxs[0].imshow(img, cmap='gray')\naxs[0].set_title(\"Original image\")\naxs[1].imshow(np.real(x), cmap='gray')\naxs[1].set_title(plot_which + \" \" + \"BCD reconstruction\")\naxs[2].imshow(blurred_img, cmap='gray')\naxs[2].set_title(\"Blurred image\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nReferences\n\n[1] Glaubitz, J., Gelb, A. and Song, G. (2022). Generalized sparse bayesian learning and application to image reconstruction.\n\n\n[2] Marnissi, Y., Chouzenoux, E., Benazza-Benyahia, A. and Pesquet, J.-C. (2018). An auxiliary variable method for markov chain monte carlo algorithms in high dimension. Entropy 20.\n\n\n[3] Dyk, D. A. van and Park, T. (2008). Partially collapsed gibbs samplers. Journal of the American Statistical Association 103 790–6."
  },
  {
    "objectID": "posts/gaussian_posterior_formulas.html",
    "href": "posts/gaussian_posterior_formulas.html",
    "title": "Gaussian Posterior Formulas",
    "section": "",
    "text": "This post is a (working) collection of formulas useful for computing posteriors involving Gaussian likelihoods and priors. Here I give all formulas in terms of precisions rather than covariances, and I assume that all precisions are nonsingular."
  },
  {
    "objectID": "posts/gaussian_posterior_formulas.html#diagonal-constant-precisions",
    "href": "posts/gaussian_posterior_formulas.html#diagonal-constant-precisions",
    "title": "Gaussian Posterior Formulas",
    "section": "Diagonal Constant Precisions",
    "text": "Diagonal Constant Precisions\nSuppose that \\(Q_x = \\frac{1}{\\gamma^2} I\\) and \\(Q_{y} = \\frac{1}{\\sigma^2} I\\). Then our formulas become: \\[\\begin{align*}\nx &\\sim \\mathcal{N}\\left( \\mu_x, \\gamma^2 I \\right), \\\\\ny \\, | \\, x &\\sim \\mathcal{N} \\left( G_x x, \\sigma^2 I \\right), \\\\\nQ_{x \\, | \\, y} &=  \\frac{1}{\\gamma^2} I + \\frac{1}{\\sigma^2} G_x^T G_x,  \\\\\nb_{x \\, | \\, y} &= \\frac{1}{\\gamma^2}  \\mu_x +  \\frac{1}{\\sigma^2} G_x^T  y, \\\\\n\\mu_{x \\, | \\, y} &= \\left(  \\frac{1}{\\gamma^2} I + \\frac{1}{\\sigma^2} G_x^T G_x    \\right)^{-1}  b_{x \\, | \\, y}, \\\\\nx \\, | \\, y &\\sim \\mathcal{N}_C \\left( b_{x \\, | \\, y}, Q_{x \\, | \\, y} \\right), \\\\\nx \\, | \\, y &\\sim \\mathcal{N} \\left(  \\mu_{x \\, | \\, y}  , Q_{x \\, | \\, y}^{-1} \\right),\n\\end{align*}\\]"
  },
  {
    "objectID": "posts/gaussian_posterior_formulas.html#diagonal-non-constant-precisions",
    "href": "posts/gaussian_posterior_formulas.html#diagonal-non-constant-precisions",
    "title": "Gaussian Posterior Formulas",
    "section": "Diagonal Non-Constant Precisions",
    "text": "Diagonal Non-Constant Precisions\nSuppose that \\(Q_x = \\Pi\\) and \\(Q_{y} = \\Lambda\\). Then these formulas become:\n\\[\n\\begin{align*}\nx &\\sim \\mathcal{N}\\left( \\mu_x, \\Pi^{-1} \\right), \\\\\ny \\, | \\, x &\\sim \\mathcal{N} \\left( G_x x, \\Lambda^{-1} \\right), \\\\\nQ_{x \\, | \\, y} &=  \\Pi + G_x^T \\Lambda G_x,  \\\\\nb_{x \\, | \\, y} &= \\Pi \\mu_x + G_x^T \\Lambda y, \\\\\n\\mu_{x \\, | \\, y} &= \\left( \\Pi + G_x^T \\Lambda G_x \\right)^{-1} b_{x \\, | \\, y}, \\\\\nx \\, | \\, y &\\sim \\mathcal{N}_C \\left( b_{x \\, | \\, y}, Q_{x \\, | \\, y} \\right), \\\\\nx \\, | \\, y &\\sim \\mathcal{N} \\left(  \\mu_{x \\, | \\, y}  , Q_{x \\, | \\, y}^{-1} \\right).\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/gaussian_posterior_formulas.html#eda-for-the-noise-precision",
    "href": "posts/gaussian_posterior_formulas.html#eda-for-the-noise-precision",
    "title": "Gaussian Posterior Formulas",
    "section": "EDA for the Noise Precision",
    "text": "EDA for the Noise Precision\nSuppose that \\(Q_{y} = \\Lambda\\) is a diagonal matrix. Define a new variable\n\\[\nu \\, | \\, x \\sim \\mathcal{N}\\left( G_u x, Q_u^{-1} \\right).\n\\]\nThen the joint density is proportional to\n\\[\n\\begin{align*}\n\\pi(u, x \\, | \\, y) &\\propto \\pi(u \\, | \\, x ) \\pi(y \\, | \\, x ) \\pi(x) \\\\\n&\\propto \\exp\\left\\{ -\\frac{1}{2} \\left( G_x x - y \\right)^T \\Lambda \\left( G_x x - y \\right) \\right\\} \\\\\n&\\quad \\times \\exp\\left\\{  -\\frac{1}{2} \\left( u - G_u x  \\right)^T Q_u \\left( u - G_u x \\right)   \\right\\} \\times \\pi(x) \\\\\n&\\propto \\exp\\left\\{ -\\frac{1}{2} \\left( x^T G_x^T \\Lambda G_x x + y^T \\Lambda y - 2 y^T \\Lambda G_x x + u^T Q_u u + x^T G_u^T Q_u G_u x - 2 u^T Q_u G_u x     \\right)  \\right\\} \\times \\pi(x) \\\\\n&\\propto \\exp\\left\\{ -\\frac{1}{2} \\left( x^T \\left( G_x^T \\Lambda G_x +  G_u^T Q_u G_u  \\right) x + u^T Q_u u - 2 x^T \\left(  G_x^T \\Lambda y + G_u^T Q_u u \\right)    \\right)  \\right\\} \\times \\pi(x).\n\\end{align*}                                                                                                                        \n\\]\nIf we pick\n\\[\n\\begin{align*}\nG_u &= G_x, \\\\\nQ_u &= \\left( \\frac{1}{\\lambda} I - \\Lambda \\right),\n\\end{align*}\n\\]\nwhere \\(\\lambda < \\frac{1}{\\| \\Lambda \\|}\\), then\n\\[\n\\begin{align*}\nG_x^T \\Lambda G_x +  G_u^T Q_u G_u &= G_x^T \\Lambda G_x +  G_x^T \\left( \\frac{1}{\\lambda} I - \\Lambda \\right) G_x \\\\\n&= \\frac{1}{\\lambda} G_x^T G_x,\n\\end{align*}\n\\]\nand the joint density becomes\n\\[\n\\begin{align*}\n\\pi(u, x \\, | \\, y) &\\propto \\exp\\left\\{ -\\frac{1}{2} \\left( \\frac{1}{\\lambda} x^T G_x^T G_x x + u^T Q_u u - 2 x^T G_x^T \\left(  \\Lambda y + Q_u u \\right)    \\right)  \\right\\} \\times \\pi(x).\n\\end{align*}\n\\]\nThis motivates us to introduce a second variable\n\\[\nv \\sim \\mathcal{N}\\left( \\left( \\frac{1}{\\lambda} I - \\Lambda \\right) G_x x , \\left( \\frac{1}{\\lambda} I - \\Lambda \\right) \\right),\n\\]\nwith which \\(x\\) has the conditional density\n\\[\n\\pi(x \\, | \\, v) \\propto \\exp\\left\\{ -\\frac{1}{2\\lambda} \\| G_x x - \\lambda \\left( \\Lambda y + v  \\right) \\|_2^2  \\right\\} \\times \\pi(x).\n\\]\nIf we pick\n\\[\nx \\sim \\mathcal{N}\\left(0, \\left(R^T \\Pi R \\right)^{-1} \\right),\n\\]\nthen from our formula in Equation 1 the conditional for \\(x\\) becomes\n\\[\n\\begin{align*}\nQ_{x \\, | \\, v} &= R^T \\Pi R + \\frac{1}{\\lambda} I,  \\\\\n\\mu_{x \\, | \\, v} &= Q_{x \\, | \\, v}^{-1} \\left( G_{x}^T \\left( \\Lambda y + v \\right) \\right)  , \\\\\nx &\\sim \\mathcal{N}\\left( \\mu_{x \\, | \\, v}, Q_{x \\, | \\, v}^{-1} \\right).\n\\end{align*}\n\\]\nSome algebra will show that the joint density is given as\n\\[\n\\pi(v, x \\, | \\, y) \\propto \\exp\\left\\{ - \\frac{1}{2}  \\left(  \\frac{1}{\\lambda} x^T G_x^T G_x x + v^T \\Gamma^{-1} v +  y^T \\Lambda y - 2 x^T G_x^T \\left( \\Lambda y + v     \\right)     \\right) \\right\\} \\times \\pi(x)\n\\]"
  },
  {
    "objectID": "posts/gaussian_posterior_formulas.html#eda-for-the-prior-precision",
    "href": "posts/gaussian_posterior_formulas.html#eda-for-the-prior-precision",
    "title": "Gaussian Posterior Formulas",
    "section": "EDA for the Prior Precision",
    "text": "EDA for the Prior Precision\nSuppose that \\(\\Pi\\) is a diagonal matrix and \\(Q_{x} = R^T \\Pi R\\). Define a new variable\n\\[\nu \\, | \\, x \\sim \\mathcal{N}\\left( G_u x, Q_u^{-1} \\right).\n\\]\nThen the joint density is proportional to\n\\[\n\\begin{align*}\n\\pi(u, x \\, | \\, y) &\\propto \\pi(u \\, | \\, x) \\pi(y \\, | \\, x ) \\pi(x) \\\\\n&\\propto \\exp\\left\\{ - \\frac{1}{2} x^T R^T B R x \\right\\} \\times \\exp\\left\\{  -\\frac{1}{2} \\left( u - G_u x  \\right)^T Q_u \\left( u - G_u x \\right)   \\right\\} \\times \\pi(y \\, | \\, x) \\\\\n&\\propto \\exp\\left\\{ -\\frac{1}{2} \\left( x^T \\left( R^T \\Pi R +  G_u^T Q_u G_u  \\right) x + u^T Q_u u - 2 x^T G_x^T Q_u u \\right)   \\right\\} \\times \\pi(y \\, | \\, x).\n\\end{align*}                                                                                                                        \n\\]\nIf we pick\n\\[\n\\begin{align*}\nG_u &= R, \\\\\nQ_u &= \\left( \\frac{1}{\\lambda} I - \\Pi \\right),\n\\end{align*}\n\\]\nwhere \\(\\lambda < \\frac{1}{\\| \\Pi \\|}\\), then\n\\[\n\\begin{align*}\nR^T \\Pi R +  G_u^T Q_u G_u &= R^T \\Pi R +  R^T \\left( \\frac{1}{\\lambda} I - \\Pi \\right) R \\\\\n&= \\frac{1}{\\lambda} R^T R,\n\\end{align*}\n\\]\nand the joint density becomes\n\\[\n\\begin{align*}\n\\pi(u, x \\, | \\, y) &\\propto \\exp\\left\\{ -\\frac{1}{2} \\left( \\frac{1}{\\lambda} x^T R^T R x + u^T Q_u u - 2 x^T R^T Q_u u  \\right)  \\right\\} \\times \\pi(y \\, | \\, x).\n\\end{align*}\n\\]\nThis motivates us to introduce a second variable\n\\[\nv \\sim \\mathcal{N}\\left( \\left( \\frac{1}{\\lambda} I - \\Pi \\right) R x , \\left( \\frac{1}{\\lambda} I - \\Pi \\right) \\right),\n\\]\nwith which \\(x\\) has the conditional density\n\\[\n\\pi(x \\, | \\, v) \\propto \\exp\\left\\{ -\\frac{1}{2\\lambda} \\| R x - \\lambda v \\|_2^2  \\right\\} \\times \\pi(y \\, | \\, x).\n\\]\nIf we pick\n\\[\n\\pi(y \\, | \\, x) \\propto \\exp\\left\\{  -\\frac{1}{2} \\left( G_x x - y \\right)^T \\Lambda \\left( G_x x - y \\right)   \\right\\},\n\\]\nthen some algebra will show that the conditional density satisfies\n\\[\n\\pi(x \\, | \\, v) \\propto \\exp\\left\\{ -\\frac{1}{2} \\left( x^T \\left( \\frac{1}{\\lambda} R^T R + G_x^T \\Lambda G \\right) x \\right) + x^T \\left( R^T v + G_x^T \\Lambda y \\right) \\right\\}.\n\\]\nMatching this to the canonical parameterization, this is the density for\n\\[\n\\begin{align*}\nQ_{x \\, | \\, v} &= \\frac{1}{\\lambda} R^T R + G_x^T \\Lambda G, \\\\\nb_{x \\, | \\, v} &= R^T v + G_x^T \\Lambda y, \\\\\nx \\, | \\, v &\\sim \\mathcal{N}_C\\left( b_{x \\, | \\, v}, Q_{x \\, | \\, v} \\right),\n\\end{align*}\n\\]\nand in terms of the standard parameterization is\n\\[\nx \\, | \\, v \\sim \\mathcal{N}\\left( Q_{x \\, | \\, v}^{-1} \\mu_{x \\, | \\, v} , Q_{x \\, | \\, v}^{-1} \\right).\n\\]\nSome algebra will show that the joint density is given as\n\\[\n\\pi(v, x \\, | \\, y) \\propto \\exp\\left\\{ -\\frac{1}{2} \\left(   \\frac{1}{\\lambda} R^T R +  v^T \\Gamma^{-1} v + y^T \\Lambda y - 2 v^T R x - 2 y^T \\Lambda G_x x  \\right) \\right\\} \\times \\pi(y \\, | \\, x).\n\\]"
  },
  {
    "objectID": "posts/hierarchical_cmrf.html",
    "href": "posts/hierarchical_cmrf.html",
    "title": "Sampling a Cauchy Markov Random Field",
    "section": "",
    "text": "Our main reference for this post will be [3] which is a comprehensive text on the subject. Our plan of attack is to:\n\nSample from a 1D GMRF,\nBuild off of this to sample a 1D CMRF using a hierarchical representation of the Cauchy distribution,\nBuild off of this to sample from a 2D GMRF and CMRF.\n\n\nGaussian Markov Random Fields\nA GMRF can be thought of as a multivariate random variable\n\\[\nX \\sim \\mathcal{N}\\left( \\mu, Q^{-1} \\right)\n\\]\nwith mean vector \\(\\mu\\) and precision \\(Q\\), where \\(Q\\) is a sparse banded matrix (or is under some re-ordering of the components). This is just one view, as there are many other perspectives/interpretations (see [3]). Here the precision \\(Q\\) is the inverse of the covariance matrix, \\(Q = \\Sigma^{-1}\\). The reason we care about the sparsity of \\(Q\\) rather than that of the covariance \\(\\Sigma\\) is because the precision matrix encodes the conditional independence structure of the field, whereas the covariance matrix will be dense even if the precision is sparse. The sparsity of \\(Q\\) is what allows us to do efficient computations with the corresponding GMRF and avoid the “Big \\(N\\) Problem” that more general Gaussian fields suffer from.\n\n\nCode\nimport numpy as np\nimport scipy.sparse as sp\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.facecolor'] = '#0F2537'\nplt.rcParams['text.color'] = 'white'\nplt.rcParams['axes.labelcolor'] = 'white'\nplt.rcParams['xtick.color'] = 'white'\nplt.rcParams['ytick.color'] = 'white'\n\n# Build precision and covariance\nprec_mat = 2*sp.eye(10)\nprec_mat.setdiag(-1,k=-1)\nprec_mat.setdiag(-1,k=1)\nprec_mat = prec_mat.toarray()\ncov_mat = np.linalg.inv(prec_mat)\n\nfig, axs = plt.subplots(1, 2, figsize=(10,5))\naxs[0].imshow(prec_mat, cmap='Greens_r')\naxs[0].set_title(\"Precision\")\naxs[1].imshow(cov_mat, cmap='Greens_r')\naxs[1].set_title(\"Covariance\")\n\nplt.show()\n\n\n\n\n\nFigure 1: A sparse precision matrix will generally invert to a dense covariance matrix.\n\n\n\n\nThe unnormalized density function for the GMRF is given as\n\\[\n\\pi(x) \\propto \\exp\\left[ - \\frac{1}{2} \\left( x - \\mu \\right)^T Q \\left( x - \\mu \\right) \\right].\n\\]\n\n\n1. Sampling a 1D GMRF\nThe GMRF we will consider is that with \\(\\mu = 0\\) and precision \\(Q = D^T D\\), where\n\\[\nD = \\left(\\begin{array}{ccccc}\n1  &    &          &          &   \\\\\n-1 & 1  &          &          &   \\\\\n   & -1 & 1        &          &   \\\\\n   &    & \\ddots & \\ddots &   \\\\\n   &    &          & -1       & 1\n\\end{array}\\right)\n\\]\nis an \\(N \\times N\\) matrix. Intuitively, this precision corresponds to a Gaussian random-walk on the line whose increments and initial value have distribution \\(\\Delta x_i, x_0 \\sim \\mathcal{N}(0,1)\\). One way to sample this GMRF is to use the “covariance method”, which is to compute the Cholesky factorization \\(\\Sigma = L L^T\\) and then get a sample via\n\\[\nx = \\mu + L z\n\\]\nwhere \\(z \\sim \\mathcal{N}(0, I)\\). Let’s try this with \\(N = 100\\) points.\n\n\nCode\nN = 100\n\n# Build D matrix\nd_mat = sp.eye(N)\nd_mat.setdiag(-1, k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\n# Build precision and covariance\nprec_mat = d_mat.T @ d_mat\nprec_mat = prec_mat.toarray()\ncov_mat = np.linalg.inv(prec_mat)\n\n# Get Cholesky factor of covariance\nchol_fac = np.linalg.cholesky(cov_mat)\n\n# Compute sample\nnp.random.seed(0)\nz = np.random.normal(size=N)\nsample_cov_method = chol_fac @ z\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.plot(sample_cov_method, \"o-\", color=\"#A5D75E\")\n#axs.set_title(\"A single draw from the random walk on the line\")\nplt.show()\n\n\n\n\n\nFigure 2: A single draw from a 1D GMRF, using the covariance method.\n\n\n\n\nEasy enough. However, a central concern when working with GMRFs is how well the method scales with the dimension \\(N\\). Now let’s do a naive experiment where we time how long it takes to compute a single sample as we increase \\(N\\).\n\n\nCode\nimport time\nfrom sklearn import datasets, linear_model\n\nns = np.logspace(2.5, 4, num=50)\nns = np.rint(ns).astype(np.int32)\n\n\ndef draw_single_sample_cov_method(n):\n    \"\"\"Draws a single sample from the RW model of size n,\n    using the covariance method.\n    \"\"\"\n    # Build D matrix\n    d_mat = sp.eye(n)\n    d_mat.setdiag(-1,k=-1)\n\n    # Build precision and covariance\n    prec_mat = d_mat.T @ d_mat\n    prec_mat = prec_mat.toarray()\n    cov_mat = np.linalg.inv(prec_mat)\n\n    # Draw a sample\n    np.random.seed(0)\n    chol_fac = np.linalg.cholesky(cov_mat)\n    z = np.random.normal(size=n)\n    sample = chol_fac @ z\n    \n    return None\n\ntimes = []\nfor n in ns:\n    start = time.time()\n    draw_single_sample_cov_method(n)\n    end = time.time()\n    elapsed = end - start\n    times.append(elapsed)\n\nregr = linear_model.LinearRegression()\nregr.fit(np.log(ns)[:, None], np.log(times)[:, None])\nslope = regr.coef_[0][0]\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.loglog(ns, times, \"o-\", color=\"#A5D75E\")\naxs.set_xlabel(\"N\")\naxs.set_ylabel(\"Time (milliseconds)\")\naxs.set_title(f\"Slope = {slope:.1f}\")\nplt.show()\n\n\n\n\n\nFigure 3: Time to generate a single sample vs. number of points N, using the covariance method.\n\n\n\n\nGenerating a sample appears to cost \\(\\mathcal{O}(N^\\alpha)\\) with \\(\\alpha \\approx 2.4\\). The bottleneck in this method comes from the matrix inverse and Cholesky factorizations, which since the covariance is dense we should each expect to cost \\(\\mathcal{O}(N^3)\\) – for whatever reason it seems we manage to do better than this though.\nWe can do better than this by instead using the “precision method” that takes advantage of the sparsity of \\(Q\\). Inserting the Cholesky factorization \\(Q = L L ^T\\) into the previous covariance method, the precision method is to compute a sample via\n\\[\nx = \\mu + L^{-T} z\n\\]\nwith \\(z \\sim \\mathcal{N}(0,I)\\). However, it will be important how we compute the Cholesky factorization. If we just use np.linalg.cholesky which handles dense matrices, we shouldn’t expect to do any better. A nice property of sparse matrices is that if a matrix has a lower bandwidth of \\(p\\), then its lower Cholesky factor \\(L\\) also has bandwidth \\(p\\). There are specific algorithms that have been designed to take advantage of this fact, such as those implemented in sksparse.cholmod which we will use.\n\n\nCode\nfrom sksparse.cholmod import cholesky\n\nN = 100\n\n# Build D matrix\nd_mat = sp.eye(N)\nd_mat.setdiag(-1, k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\n# Build precision\nprec_mat = d_mat.T @ d_mat\nprec_mat = sp.csc_matrix(prec_mat)\n\n# Compute (sparse) cholesky factorization\n# The natural ordering method doesn't permute the nodes (our precision is already sparse)\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n# Solve L^T x = z\nnp.random.seed(0)\nz = np.random.normal(size=N)\nsample_prec_method = factor.solve_Lt(z, use_LDLt_decomposition=False)\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.plot(sample_prec_method, \"o-\", color=\"#A5D75E\")\n#axs.set_title(\"A single draw from the random walk on the line\")\nplt.show()\n\n\n\n\n\nFigure 4: A single draw from a 1D GMRF, using the precision method.\n\n\n\n\n\n\nCode\nimport time\nfrom sklearn import datasets, linear_model\n\nns = np.logspace(2.5, 7, num=50)\nns = np.rint(ns).astype(np.int32)\n\n\ndef draw_single_sample_prec_method(n):\n    \"\"\"Draws a single sample from the RW model of size n,\n    using the covariance method.\n    \"\"\"\n    d_mat = sp.eye(n)\n    d_mat.setdiag(-1, k=-1)\n    d_mat = sp.csr_matrix(d_mat)\n\n    prec_mat = d_mat.T @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n\n    factor = cholesky(prec_mat, ordering_method='natural')\n\n    np.random.seed(0)\n    z = np.random.normal(size=n)\n    sample = factor.solve_Lt(z, use_LDLt_decomposition=False)\n    \n    return None\n\ntimes = []\nfor n in ns:\n    start = time.time()\n    draw_single_sample_prec_method(n)\n    end = time.time()\n    elapsed = end - start\n    times.append(elapsed)\n\nregr = linear_model.LinearRegression()\nregr.fit(np.log(ns)[:, None], np.log(times)[:, None])\nslope = regr.coef_[0][0]\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.loglog(ns, times, \"o-\", color=\"#A5D75E\")\naxs.set_xlabel(\"N\")\naxs.set_ylabel(\"Time (milliseconds)\")\naxs.set_title(f\"Slope = {slope:.1f}\")\nplt.show()\n\n\n\n\n\nFigure 5: Time to generate a single sample vs. number of points N, using the precision method.\n\n\n\n\nNote that the precision method is significantly faster and has cost about \\(\\mathcal{O}(N)\\), as it leverages a sparse Cholesky factorization and a sparse linear system solver. For the rest of this post, we will use the precision method for drawing samples.\n\n\nHierarchical representation of the Cauchy distribution\nIn order to sample from a CMRF, we will use a scale mixture of normal representation for the Cauchy distribution2. Suppose we wish to sample\n\\[\nx \\sim \\text{Cauchy}(1).\n\\]\nwhere \\(s = 1\\) is the scale parameter of the Cauchy distribution. We can achieve this using the hierarchical representation\n\\[\n\\begin{align*}\n\\lambda &\\sim \\text{Gamma}(\\nu/2,\\nu/2) \\\\\nx \\, | \\, \\lambda &\\sim \\mathcal{N}(0, \\lambda^{-1})\n\\end{align*}\n\\]\nwith the choice \\(\\nu=1\\). For general \\(\\nu\\), this gives us \\(x \\sim \\text{Student-}t_\\nu\\) for which the Cauchy distribution is a special case. With this sampling scheme, we have that the marginal density for \\(x\\),\n\\[\n\\pi(x) = \\int \\pi(x \\, | \\lambda) \\, \\pi(\\lambda) \\, d\\lambda,\n\\]\nis the density for a \\(\\text{Cauchy}(1)\\)-distributed random variable. If we want to control the scale parameter of the Cauchy distribution, we can modify this as \\[\\begin{align*}\n\\lambda &\\sim \\text{Gamma}(1/2,s^2/2) \\\\\nx \\, | \\, \\lambda &\\sim \\mathcal{N}(0, \\lambda^{-1})\n\\end{align*}\\]\n\n\n\n\n2. Sampling a 1D CMRF\nNow let’s put this hierarchical representation to work. To obtain a CMRF from a GMRF, we will sample \\[\\begin{align*}\n\\lambda_1, \\ldots, \\lambda_N &\\sim \\text{Gamma}(1/2, s^2/2) \\\\\nH &= \\text{diag}(\\lambda_1, \\ldots, \\lambda_N) \\\\\nx \\, | \\, H &\\sim \\mathcal{N}\\left(\\mu, Q^{-1} = \\left( D^T H D \\right)^{-1} \\right)\n\\end{align*}\\] where \\(H\\) is a diagonal matrix collecting the hyper-parameters and \\(D\\) is the same differencing matrix as before. The marginal density for \\(x\\), \\[\\begin{equation*}\n\\pi(x) = \\int \\pi(x | \\lambda_1, \\ldots, \\lambda_N) \\pi(\\lambda_1, \\ldots, \\lambda_N) \\, d\\lambda_1 \\, \\ldots \\, d\\lambda_N\n\\end{equation*}\\] is the density function for a CMRF.\n\n\nCode\nfrom sksparse.cholmod import cholesky\nfrom scipy.stats import gamma\n\nN = 100\ns = 1\n\n# Build D matrix\nd_mat = sp.eye(N)\nd_mat.setdiag(-1, k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\n# Sample the hyperparameters\nnp.random.seed(0)\nh_mat = gamma.rvs(1/2, scale=(s**2)/2, size=N)\nh_mat = sp.diags(h_mat)\n\n# Build precision\nprec_mat = d_mat.T @ h_mat @ d_mat\nprec_mat = sp.csc_matrix(prec_mat)\n\n# Compute (sparse) cholesky factorization\n# The natural ordering method doesn't permute the nodes (our precision is already sparse)\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n# Solve L^T x = z\nz = np.random.normal(size=N)\nsample_prec_method = factor.solve_Lt(z, use_LDLt_decomposition=False)\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.plot(sample_prec_method, \"o-\", color=\"#A5D75E\")\n#axs.set_title(\"A single draw from the random walk on the line\")\nplt.show()\n\n\n\n\n\nFigure 6: A single draw from a 1D CMRF.\n\n\n\n\n\n\nCode\nN = 100\ns = 1\n\nd_mat = sp.eye(N)\nd_mat.setdiag(-1,k=-1)\n\nnp.random.seed(0)\nh_mat = gamma.rvs(1/2, scale=(s**2)/2, size=N)\nh_mat = sp.diags(h_mat)\n\nprec_mat = d_mat.T @  h_mat @ d_mat\nprec_mat = prec_mat.toarray()\ncov_mat = np.linalg.inv(prec_mat)\n\nfig, axs = plt.subplots(1, 2, figsize=(10,5))\naxs[0].imshow(prec_mat, cmap='Greens_r')\naxs[0].set_title(\"Precision\")\naxs[1].imshow(cov_mat, cmap='Greens_r')\naxs[1].set_title(\"Covariance\")\n\nplt.show()\n\n\n\n\n\nFigure 7: Precision and covariance matrices corresponding to the sample in Figure 6\n\n\n\n\nCompared to our sample from a GMRF from earlier, the sample from the CMRF has infrequent massive jumps. This makes sense, since our CMRF is modeling increments as being Cauchy-distributed and the Cauchy distribution has infinite variance. Let’s look at some more samples from each.\n\n\nCode\nfrom sksparse.cholmod import cholesky\n\nN = 100\nn_samples = 10\nnp.random.seed(1)\n\n# Arrays for storing samples\ngmrf_samples = np.zeros((n_samples, N))\ncmrf_samples = np.zeros((n_samples, N))\n\n# Build D matrix\nd_mat = sp.eye(N)\nd_mat.setdiag(-1, k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\nfig, axs = plt.subplots(n_samples, 2, figsize=(20,20))\n\nfor j in range(n_samples):\n\n    # Draw normal increments\n    z = np.random.normal(size=N)\n\n    # Get GMRF sample\n    prec_mat = d_mat.T @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n    factor = cholesky(prec_mat, ordering_method='natural')\n    gmrf_samples[j,:] = factor.solve_Lt(z, use_LDLt_decomposition=False)\n\n    # Get CMRF sample\n    h_mat = gamma.rvs(1/2, scale=(s**2)/2, size=N)\n    h_mat = sp.diags(h_mat)\n    prec_mat = d_mat.T @  h_mat @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n    factor = cholesky(prec_mat, ordering_method='natural')\n    cmrf_samples[j,:] = factor.solve_Lt(z, use_LDLt_decomposition=False)\n\n    # Plot\n    axs[j,0].plot(gmrf_samples[j,:], color=\"#A5D75E\")\n    axs[j,1].plot(cmrf_samples[j,:], color=\"#A5D75E\")\n    axs[j,0].set_xticks([])\n    axs[j,1].set_xticks([])\n\naxs[0,0].set_title(\"GMRF Samples\")\naxs[0,1].set_title(\"CMRF Samples\")\n\nplt.show()\n\n\n\n\n\nFigure 8: 10 samples each from our GMRF/CMRF. Each row uses the same draw \\(z \\sim \\mathcal{N}(0,I)\\).\n\n\n\n\n\n\n3. Sampling a 2D GMRF/CMRF\nMaking the jump to a 2D GMRF is not too difficult. Whereas in our earlier examples for a 1D random walk with \\(N\\) points our precision matrix \\(Q\\) was \\(N \\times N\\), in 2D on an \\(M \\times N\\) grid our precision matrix will need to be an \\(M^2 \\times N^2\\) matrix. The 2D GMRF we will consider is an “anisotropic difference prior” that is an extension of our 1D GMRF from earlier, with \\(\\mu = 0\\) and precision \\[\\begin{equation*}\nQ = F^T F.\n\\end{equation*}\\] Here \\[\\begin{equation*}\nF = \\begin{bmatrix}\n           D_1 \\\\\n           D_2\n         \\end{bmatrix},\n\\end{equation*}\\] where the \\(D_{1/2}\\) are given by \\[\\begin{align*}\nD_1 &= I_N \\otimes D, \\\\\nD_2 &= D \\otimes I_M,\n\\end{align*}\\] and \\(D\\) is the same matrix from our 1D example.\n\n\nCode\nM, N = 5, 5\n\n# Build D matrices\nd_mat_vert = sp.eye(M)\nd_mat_vert.setdiag(-1,k=-1)\neye_horiz = sp.eye(N)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(N)\nd_mat_horiz.setdiag(-1,k=-1)\neye_vert = sp.eye(M)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\nd_mat = sp.vstack([d_mat_one, d_mat_two])\nprec_mat = d_mat.T @ d_mat\n\nfig, axs = plt.subplots(1,2,figsize=(10,10))\naxs[0].imshow(d_mat.toarray(), cmap='Greens_r')\naxs[0].set_title('F')\naxs[1].imshow(prec_mat.toarray(), cmap='Greens_r')\naxs[1].set_title('Q')\n\nplt.show()\n\n\n\n\n\nFigure 9: The matrices \\(F\\) and \\(Q\\) for a \\(20 \\times 20\\) lattice.\n\n\n\n\nNow that we have our precision matrix, we can sample the GMRF just as we did in the 1D case.\n\n\nCode\nM, N = 200, 200\n\n# Build D matrices\nd_mat_vert = sp.eye(M)\nd_mat_vert.setdiag(-1,k=-1)\neye_horiz = sp.eye(N)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(N)\nd_mat_horiz.setdiag(-1,k=-1)\neye_vert = sp.eye(M)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\nd_mat = sp.vstack([d_mat_one, d_mat_two])\n\n# Build precision\nprec_mat = d_mat.T @ d_mat\nprec_mat = sp.csc_matrix(prec_mat)\n\n# Get Cholesky factor\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n# Draw a sample\nnp.random.seed(0)\nz = np.random.normal(size=(M,N)).flatten()\nsample = factor.solve_Lt(z, use_LDLt_decomposition=False)\nsample = sample.reshape((M,N))\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nplt.figure(figsize=(8,8))\nax = plt.gca()\nim = ax.imshow(sample)\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\nplt.colorbar(im, cax=cax)\nplt.show()\n\n\n\n\n\nFigure 10: A sample of our GMRF on a \\(200 \\times 200\\) lattice.\n\n\n\n\nLet’s also make a “movie” for a sequence of samples.\n\nCode\nfrom moviepy.editor import ImageSequenceClip\n\nn_samples = 10\nsamples = np.zeros((n_samples, M, N))\n\n# Draw samples\nnp.random.seed(0)\nfor j in range(n_samples):\n    z = np.random.normal(size=(M,N)).flatten()\n    samples[j,:,:] = factor.solve_Lt(z, use_LDLt_decomposition=False).reshape((M,N))\n\nfps = int(0.1*n_samples)\nmin_val = np.amin(samples)\nmax_val = np.amax(samples)\n\nsamples -= min_val\nsamples /= max_val - min_val\n\ncolored_samples = np.zeros((n_samples, M, N, 4))\ncmap = plt.get_cmap()\nfor j in range(n_samples):\n    #print(cmap(samples[j,:,:]))\n    colored_samples[j,:,:,:] = cmap(samples[j,:,:])\n\ncolored_samples *= 255\n\nclip = ImageSequenceClip(list(colored_samples), fps=fps)\nclip.write_gif('gmrf_samples.gif', fps=fps, verbose=False, logger=None)\n\n\nNow that we can sample from a 2D GMRF, it is straightforward to add in hyper-parameters that turn our GMRF into a CMRF. To do this, we sample \\[\\begin{align*}\n\\lambda_1, \\ldots, \\lambda_{2N^2} &\\sim \\text{Gamma}(1/2, s^2/2) \\\\\nH \\, | \\, \\lambda_1, \\ldots, \\lambda_{2N^2} &= \\text{diag}(\\lambda_1, \\ldots, \\lambda_{2N^2}) \\\\\nx \\, | \\, H &\\sim \\mathcal{N}\\left(\\mu, Q^{-1} = \\left( F^T H F \\right)^{-1} \\right).\n\\end{align*}\\] Let’s try sampling this.\n\n\nCode\nfrom scipy.stats import gamma\n\nM, N = 200, 200\ns = 1.0\n\n# Build D matrices\nd_mat_vert = sp.eye(M)\nd_mat_vert.setdiag(-1,k=-1)\neye_horiz = sp.eye(N)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(N)\nd_mat_horiz.setdiag(-1,k=-1)\neye_vert = sp.eye(M)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\nd_mat = sp.vstack([d_mat_one, d_mat_two])\n\n# Sample hyperparamters\nh_mat = gamma.rvs(1/2, scale=1/((s**2)/2), size=2*M*N)\nh_mat = sp.diags(h_mat)\n\n# Build precision\n    prec_mat = d_mat.T @ h_mat @ d_mat\nprec_mat = sp.csc_matrix(prec_mat)\n\n# Get Cholesky factor\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n# Draw a sample\nnp.random.seed(0)\nz = np.random.normal(size=(M,N)).flatten()\nsample = factor.solve_Lt(z, use_LDLt_decomposition=False)\nsample = sample.reshape((M,N))\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nplt.figure(figsize=(8,8))\nax = plt.gca()\nim = ax.imshow(sample)\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\nplt.colorbar(im, cax=cax)\nplt.show()\n\n\n\n\n\nFigure 11: A sample of our CMRF on a \\(200 \\times 200\\) lattice.\n\n\n\n\nLet’s make another movie while we’re at it.\n\nCode\nfrom moviepy.editor import ImageSequenceClip\n\nn_samples = 10\nsamples = np.zeros((n_samples, M, N))\n\n# Draw samples\nnp.random.seed(0)\nfor j in range(n_samples):\n\n    # Sample hyperparamters\n    h_mat = gamma.rvs(1/2, scale=1/((s**2)/2), size=2*M*N)\n    h_mat = sp.diags(h_mat)\n\n    # Build precision\n    prec_mat = d_mat.T @ h_mat @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n\n    # Get Cholesky factor\n    factor = cholesky(prec_mat, ordering_method='natural')\n\n    z = np.random.normal(size=(M,N)).flatten()\n    samples[j,:,:] = factor.solve_Lt(z, use_LDLt_decomposition=False).reshape((M,N))\n\nfps = int(0.1*n_samples)\nmin_val = np.amin(samples)\nmax_val = np.amax(samples)\n\nsamples -= min_val\nsamples /= max_val - min_val\n\ncolored_samples = np.zeros((n_samples, M, N, 4))\ncmap = plt.get_cmap()\nfor j in range(n_samples):\n    #print(cmap(samples[j,:,:]))\n    colored_samples[j,:,:,:] = cmap(samples[j,:,:])\n\ncolored_samples *= 255\n\nclip = ImageSequenceClip(list(colored_samples), fps=fps)\nclip.write_gif('cmrf_samples.gif', fps=fps, verbose=False, logger=None)\n\n\nSo we have achieved our goal. In a future post, we will put this to work as a prior in an inverse problem.\n\n\nBonus: some extra MRFs\nWhile we used a scale mixture of normal representation for the Cauchy distribution, we can also obtain other distributions as scale mixtures of normals. For example, we can simulate a MRF where the increments follow a Laplace distribution by sampling \\[\\begin{align*}\n\\beta_1, \\ldots, \\beta_{2N^2} &\\sim \\text{Exponential} \\\\\n\\lambda_i \\, | \\, \\beta_i &= \\frac{1}{2\\beta_i} \\\\\nH \\, | \\, \\lambda_1, \\ldots, \\lambda_{2N^2} &= \\text{diag}(\\lambda_1, \\ldots, \\lambda_{2N^2}) \\\\\nx \\, | \\, H &\\sim \\mathcal{N}\\left(\\mu, Q^{-1} = \\left( F^T H F \\right)^{-1} \\right).\n\\end{align*}\\]\nSimilarly, we can get a MRF increments with Horshoe-distributed [4] increments by sampling \\[\\begin{align*}\n\\beta_1, \\ldots, \\beta_{2N^2} &\\sim \\text{C}^+(0, 1) \\\\\n\\lambda_i \\, | \\, \\beta_i, \\tau &= \\frac{1}{\\beta_i^2 \\tau^2} \\\\\nH \\, | \\, \\lambda_1, \\ldots, \\lambda_{2N^2} &= \\text{diag}(\\lambda_1, \\ldots, \\lambda_{2N^2}) \\\\\nx \\, | \\, H &\\sim \\mathcal{N}\\left(\\mu, Q^{-1} = \\left( F^T H F \\right)^{-1} \\right)\n\\end{align*}\\] where \\(\\tau\\) is some fixed hyper-hyper-parameter and \\(C^+\\) denotes the half-Cauchy distribution. Below are some movies of samples from a Laplace MRF and Horshoe MRF, respectively.\n\nCode\nfrom moviepy.editor import ImageSequenceClip\n\nn_samples = 10\nsamples = np.zeros((n_samples, M, N))\n\n# Draw samples\nnp.random.seed(0)\nfor j in range(n_samples):\n\n    # Sample hyperparamters\n    h_mat = (1/(2*np.random.exponential(size=2*M*N)))\n    h_mat = sp.diags(h_mat)\n\n    # Build precision\n    prec_mat = d_mat.T @ h_mat @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n\n    # Get Cholesky factor\n    factor = cholesky(prec_mat, ordering_method='natural')\n\n    z = np.random.normal(size=(M,N)).flatten()\n    samples[j,:,:] = factor.solve_Lt(z, use_LDLt_decomposition=False).reshape((M,N))\n\nfps = int(0.1*n_samples)\nmin_val = np.amin(samples)\nmax_val = np.amax(samples)\n\nsamples -= min_val\nsamples /= max_val - min_val\n\ncolored_samples = np.zeros((n_samples, M, N, 4))\ncmap = plt.get_cmap()\nfor j in range(n_samples):\n    #print(cmap(samples[j,:,:]))\n    colored_samples[j,:,:,:] = cmap(samples[j,:,:])\n\ncolored_samples *= 255\n\nclip = ImageSequenceClip(list(colored_samples), fps=fps)\nclip.write_gif('lmrf_samples.gif', fps=fps, verbose=False, logger=None)\n\n\nfrom scipy.stats import halfcauchy\n\ntau = 1.0\n\nn_samples = 10\nsamples = np.zeros((n_samples, M, N))\n\n# Draw samples\nnp.random.seed(0)\nfor j in range(n_samples):\n\n    # Sample hyperparamters\n    h_mat = (  1/(  (tau*halfcauchy.rvs(size=2*M*N))**2)  )\n    h_mat = sp.diags(h_mat)\n\n    # Build precision\n    prec_mat = d_mat.T @ h_mat @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n\n    # Get Cholesky factor\n    factor = cholesky(prec_mat, ordering_method='natural')\n\n    z = np.random.normal(size=(M,N)).flatten()\n    samples[j,:,:] = factor.solve_Lt(z, use_LDLt_decomposition=False).reshape((M,N))\n\nfps = int(0.1*n_samples)\nmin_val = np.amin(samples)\nmax_val = np.amax(samples)\n\nsamples -= min_val\nsamples /= max_val - min_val\n\ncolored_samples = np.zeros((n_samples, M, N, 4))\ncmap = plt.get_cmap()\nfor j in range(n_samples):\n    #print(cmap(samples[j,:,:]))\n    colored_samples[j,:,:,:] = cmap(samples[j,:,:])\n\ncolored_samples *= 255\n\nclip = ImageSequenceClip(list(colored_samples), fps=fps)\nclip.write_gif('hmrf_samples.gif', fps=fps, verbose=False, logger=None)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n[1] Chada, N. K., Roininen, L. and Suuronen, J. (2021). Cauchy markov random field priors for bayesian inversion.\n\n\n[2] Markkanen, M., Roininen, L., Huttunen, J. M.  J. and Lasanen, S. (2019). Cauchy difference priors for edge-preserving bayesian inversion. Journal of Inverse and Ill-posed Problems 27 225–40.\n\n\n[3] Rue, H. and Held, L. (2005). Gaussian markov random fields: Theory and applications.\n\n\n[4] Carvalho, C. M., Polson, N. G. and Scott, J. G. (2009). Handling sparsity via the horseshoe. In AISTATS.\n\nFootnotes\n\n\nNOT using a Markov Chain Monte Carlo (MCMC) method.↩︎\nSee chapter 4 of [3], also here↩︎"
  },
  {
    "objectID": "posts/simple_image_deblurring.html",
    "href": "posts/simple_image_deblurring.html",
    "title": "A Simple Image De-blurring Example",
    "section": "",
    "text": "Problem Setup\nWe will start by converting the following 3024 x 4032 pixel image of the Grand Canyon\n\nto grayscale to get\n\nand then apply a Gaussian blur to get\n\n\nCode\nimport numpy as np\nfrom scipy.ndimage import gaussian_filter\nfrom PIL import Image, ImageOps\n\n# Load image\nimg = Image.open(\"grand_canyon.jpg\")\nimg = ImageOps.grayscale(img)\nimg = np.asarray(img)\nimg = img/255\nM, N = img.shape\n\nim = Image.fromarray(255*img)\nim = im.convert('RGB')\nim.save(\"grayscale_grand_canyon.jpg\")\n\n# Define blur operator using sigma\nblur_sigma = 20.0\nblur_op = lambda x: gaussian_filter(x, blur_sigma, mode='wrap')\n\n# Apply blur and save\nblurred_img = blur_op(img)\nim = Image.fromarray(255*blurred_img)\nim = im.convert('RGB')\nim.save(\"blurred_grand_canyon.jpg\")\n\n\n\nBCCB Blurring\nLet’s describe what we just did. We began with an initial image \\(x\\), and then applied a Gaussian blurring operator \\(A\\) to obtain the observation\n\\[\ny = A x.\n\\]\nNote that the blurring is a linear operator and thus can be represented by a matrix \\(A\\), even though we never actually form the matrix \\(A\\). Note that we have also used the mode = 'wrap' option, which specifies a periodic boundary condition for the blurring. We choose this because this choice makes \\(A\\) a block circulant with circulant blocks (BCCB) matrix, which have several nice properties (see [1] for a nice overview of structured matrices and image de-blurring). One such property is that we can diagonalize the matrix \\(A\\) via the two-dimensional discrete Fourier transform, i.e.,\n\\[\nA = F^H  \\Lambda  F\n\\]\nwhere \\(\\Lambda\\) is a diagonal matrix. This makes solving systems involving \\(A\\) fairly straightforward and cheap to do.\nIf we want to de-blur the image \\(y\\), one thing we might try is to do\n\\[\nx = A^{-1}  y.\n\\]\nFrom our diagonalization and the fact that \\(F^{-1} = F^H\\), we can compute this as\n\\[\n\\begin{align*}\nx &= \\left( F^H  \\Lambda F \\right)^{-1} y \\\\\n&= F^H  \\Lambda^{-1}  F y.\n\\end{align*}\n\\]\nBut what is \\(\\Lambda^{-1}\\)? We can compute this by picking a random vector \\(v\\), and doing elementwise division between \\(v\\) and \\(F A F^H v\\). Let’s do this, and then use the result to compute \\(x = A^{-1} y\\).\n\n\nCode\n# Draw a random image\nrand_img = np.random.randn(M,N)\nmat_prod_rand_img = np.fft.fft2(blur_op(np.fft.ifft2(rand_img, norm='ortho')), norm='ortho')\nlam = mat_prod_rand_img/rand_img\nlam = np.real(lam) # We know it should be real\nlam_inv = 1/lam\n\n# Now use to invert, and save result\nx_approx = np.fft.ifft2(lam_inv*np.fft.fft2(blurred_img, norm='ortho'), norm='ortho')\nx_approx = np.real(x_approx)\n\nim = Image.fromarray(255*x_approx)\nim = im.convert('RGB')\nim.save(\"deblurred_grand_canyon_direct_inversion.jpg\")\n\n\nSo that actually worked out pretty well. In reality, our image would likely be contaminated by at least some small amount of noise. Let’s see what happens if we apply our same inversion procedure to\n\\[\n\\begin{align*}\ny &= A x + \\varepsilon, \\\\\n\\varepsilon &\\sim \\mathcal{N}\\left(0, \\sigma^2 I \\right)\n\\end{align*}\n\\]\nwith a comically small noise standard deviation of \\(\\sigma = 0.000000001\\). Our new observation is\n\nwhich is visually indistinguishable from our previous one with no noise added to the blurred image. However, if we compute \\(x = A^{-1} y\\) in this case we obtain\n\nwhich is nonsense.\n\nCode\nnoise_sigma = 0.000000001\n\n# Add a little bit of noise and save\nnp.random.seed(0) # Set seed\nnoisy_blurred_img = blur_op(img) + noise_sigma*np.random.randn(M,N) \n\nim = Image.fromarray(255*noisy_blurred_img)\nim = im.convert('RGB')\nim.save(\"noisy_blurred_grand_canyon.jpg\")\n\n# Now invert using the same method as before and save result\nx_approx = np.fft.ifft2(lam_inv*np.fft.fft2(noisy_blurred_img, norm='ortho'), norm='ortho')\nx_approx = np.real(x_approx)\n\nim = Image.fromarray(255*x_approx)\nim = im.convert('RGB')\nim.save(\"noisy_deblurred_grand_canyon_direct_inversion.jpg\")\n\nThe reason this happens is because although our blurring operator \\(A\\) is invertible, it is extremely ill-conditioned and thus will yield unexpected results such as this. Note that the diagonal entries of \\(\\Lambda\\) are in fact the eigenvalues of \\(A\\), so examining these in Figure 1 can give us a sense of why the matrix is ill-conditioned.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.facecolor'] = '#0F2537'\nplt.rcParams['text.color'] = 'white'\nplt.rcParams['axes.labelcolor'] = 'white'\nplt.rcParams['xtick.color'] = 'white'\nplt.rcParams['ytick.color'] = 'white'\n\n# Compute eigvalues in descending order\neigvals = np.flip(np.asarray(sorted(lam.flatten())))\n\n# Plot\nplt.figure(figsize=(13,8))\nplt.semilogx(eigvals, color='#A5D75E')\nplt.xlabel(\"nth eigenvalue\")\nplt.ylabel(\"eigenvalue\")\nplt.show()\n\n\n\n\n\nFigure 1: The eigenvalues of \\(A\\) in descending order.\n\n\n\n\nThe condition number of \\(A\\) is equal to the ratio of the largest and smallest eigenvalues. The fact that the smallest eigenvalue is near zero and the largest is near one gives \\(A\\) a very large condition number and thus \\(A\\) is very ill-conditioned. In other words, recovering \\(x\\) from \\(y\\) in this manner is a very ill-posed problem.\n\n\nRegularized Inversion\nIn the case that our blurred image contains any amount of noise, we clearly cannot settle for simply computing \\(x = A^{-1} y\\). One way to tackle this problem is to instead look for the solution of\n\\[\nx^\\star = \\text{argmin}_x \\,\\, \\| A x - y \\|_2^2 + \\mathcal{R}(x)\n\\tag{1}\\]\nfor some regularization term \\(\\mathcal{R}(x)\\). Ignoring \\(\\mathcal{R}(x)\\), note that if we found an \\(x\\) such that \\(A x = y\\) then the objective would be equal to zero and \\(x = x^\\star\\). However, we have already seen that this solution would do us no good. This explains the role of \\(\\mathcal{R}(x)\\): to make the problem less ill-posed, i.e. well-posed. For now we will consider the choice\n\\[\n\\mathcal{R}(x) = \\gamma \\| L x \\|_2^2 = \\gamma x^T L^T L x.\n\\]\nwhere \\(L\\) encodes a discrete negative-Laplacian operator. This regularization term promotes smooth solutions for the reconstructed image \\(x\\), and the degree of this smoothness is controlled by the parameter \\(\\gamma > 0\\). For convenience, we will define the discrete gradient \\(L\\) to use periodic boundary conditions which will make \\(L^T L\\) a BCCB matrix that can be diagonalized by the discrete Fourier transform as\n\\[\nL^T L = F^H \\Pi F\n\\]\nfor some diagonal matrix \\(\\Pi\\).\nNote that with this choice of \\(\\mathcal{R}(x)\\), if we set the gradient of the objective function in (Equation 1) equal to zero we obtain the equation\n\\[\n2 A^T(Ax - y) + 2 \\gamma L^T L x = 0\n\\]\nfor the minimizer, which is solved by\n\\[\nx = \\left( A^T A + \\gamma L^T L \\right)^{-1} A^T y\n\\]\nThis is where our choice of BCCB matrices throughout will pay off. While it looks like we will need to solve a sparse linear system, we can actually compute the solution using only FFTs and elementwise operations thanks for our diagonalization. Inserting \\(A = F^H \\Lambda F\\) and \\(L = F^H \\Pi F\\), we have\n\\[\n\\begin{align*}\nx &= \\left( A^T A + \\gamma L^T L \\right)^{-1} A^T y \\\\\n&= \\left( F^H \\Lambda F F^H \\Lambda F + \\gamma F^H \\Pi F  \\right)^{-1} F^H \\Lambda F y \\\\\n&= \\left( F^H \\Lambda^2 F + \\gamma F^H \\Pi F \\right)^{-1} F^H \\Lambda F y \\\\\n&= \\left( F^H \\left( \\Lambda^2 + \\gamma \\Pi  \\right) F \\right)^{-1} F^H \\Lambda F y \\\\\n&= F^H \\left( \\Lambda^2 + \\gamma \\Pi  \\right)^{-1} F F^H \\Lambda F y \\\\\n&= F^H \\left( \\Lambda^2 + \\gamma \\Pi  \\right)^{-1} \\Lambda F y.\n\\end{align*}\n\\]\nNote that the only matrix inverse remaining is that of a diagonal matrix, so we can easily compute the solution. Let’s try this out.\n\nCode\n%%capture\n\nimport scipy.sparse as sp\n\n# First we need to construct our L matrix\n\n# Construct our L matrix\nd_mat_vert = sp.eye(N)\nd_mat_vert.setdiag(-1,k=-1)\nd_mat_vert = sp.csc_matrix(d_mat_vert)\nd_mat_vert[0,-1] = -1 # For periodic BCs\neye_horiz = sp.eye(M)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(M)\nd_mat_horiz.setdiag(-1,k=-1)\nd_mat_horiz = sp.csc_matrix(d_mat_horiz)\nd_mat_horiz[0,-1] = -1 # For periodic BCs\neye_vert = sp.eye(N)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\nd_mat = sp.vstack([d_mat_one, d_mat_two])\nL_mat = d_mat.T @ d_mat\n\n# Next, we need to compute $\\Pi$ like we did earlier with $\\Lambda$.\nrand_img = np.random.randn(M,N)\nmat_prod_rand_img = np.fft.fft2( (L_mat @ np.fft.ifft2(rand_img, norm='ortho').flatten() ).reshape((M,N)), norm='ortho')\npi = mat_prod_rand_img/rand_img\npi = np.real(pi) # We know it should be real\npi_inv = 1/pi\n\n# Now compute the regularized solution\ngamma = 0.00005*(1/M)\n\nsoln = np.fft.fft2(noisy_blurred_img, norm='ortho')\nsoln *= lam\nsoln *= 1/( (lam**2) + (gamma*(pi*2)) )\nsoln = np.fft.ifft2(soln, norm='ortho')\nsoln = np.real(soln)\n\n# Save the solution\nim = Image.fromarray(255*soln)\nim = im.convert('RGB')\nim.save(\"regularized_deblurred_grand_canyon.jpg\")\n\n\nThis is much better than what we got by instead computing \\(x = A^{-1} y\\). Note that I didn’t talk about how to pick \\(\\gamma\\), which in this case I tuned by hand.\nIn the next post, we’ll use CuPy to approach this same problem but with a different form of the regularizer \\(\\mathcal{R}(x)\\).\n\n\n\n\n\nReferences\n\n[1] Hansen, P. C., Nagy, J. G., O’Leary, D. P. and Miller, R. L. (2008). Deblurring images: Matrices, spectra and filtering."
  }
]