[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jonathan Lindbloom",
    "section": "",
    "text": "I am an applied mathematics PhD student at Dartmouth College, originally from Dallas, Texas. I like learning about computational methods for solving (inverse) problems and quantifying uncertainty."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Let’s see if we can sample this directly using a hierarchical representation.\n\n\n\n\n\n\nJun 15, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jonathan Lindbloom",
    "section": "",
    "text": "I am an applied mathematics PhD student at Dartmouth College, originally from Dallas, Texas. I like learning about computational methods for solving (inverse) problems and quantifying uncertainty."
  },
  {
    "objectID": "posts/test.html",
    "href": "posts/test.html",
    "title": "Test post",
    "section": "",
    "text": "I think it does."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Jonathan Lindbloom",
    "section": "Education",
    "text": "Education\nDartmouth College | Hanover, NH PhD in Mathematics | Sept 2021 - Present\nSouthern Methodist University | St. Paul MA B.A in Economics | Aug 2017 - May 2021"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Spet 2012 - April 2018"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jonathan Lindbloom",
    "section": "Education",
    "text": "Education\nDartmouth College | Hanover, NH\nPhD in Mathematics\nSept 2021 - Present\nSouthern Methodist University | Dallas, TX\nBS in Mathematics, BBA in Finance\nAug 2017 - May 2021"
  },
  {
    "objectID": "posts/test2.html",
    "href": "posts/test2.html",
    "title": "Sampling a Cauchy Markov Random Field Prior",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 2: A line plot on a polar axis\n\n\n\n\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\nDoes this work?"
  },
  {
    "objectID": "posts/hierarchical_cmrf.html",
    "href": "posts/hierarchical_cmrf.html",
    "title": "Sampling a Cauchy Markov Random Field",
    "section": "",
    "text": "Our main reference for this post will be [3] which is a comprehensive text on the subject. Our plan of attack is to:\n\nSample from a 1D GMRF,\nBuild off of this to sample a 1D CMRF using a hierarchical representation of the Cauchy distribution,\nBuild off of this to sample from a 2D GMRF and CMRF.\n\n\nGaussian Markov Random Fields\nA GMRF can be thought of as a multivariate random variable\n\\[\nX \\sim \\mathcal{N}\\left( \\mu, Q^{-1} \\right)\n\\]\nwith mean vector \\(\\mu\\) and precision \\(Q\\), where \\(Q\\) is a sparse banded matrix (or is under some re-ordering of the components). This is just one view, as there are many other perspectives/interpretations (see [3]). Here the precision \\(Q\\) is the inverse of the covariance matrix, \\(Q = \\Sigma^{-1}\\). The reason we care about the sparsity of \\(Q\\) rather than that of the covariance \\(\\Sigma\\) is because the precision matrix encodes the conditional independence structure of the field, whereas the covariance matrix will be dense even if the precision is sparse. The sparsity of \\(Q\\) is what allows us to do efficient computations with the corresponding GMRF and avoid the “Big \\(N\\) Problem” that more general Gaussian fields suffer from.\n\n\nCode\nimport numpy as np\nimport scipy.sparse as sp\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.facecolor'] = '#0F2537'\nplt.rcParams['text.color'] = 'white'\nplt.rcParams['axes.labelcolor'] = 'white'\nplt.rcParams['xtick.color'] = 'white'\nplt.rcParams['ytick.color'] = 'white'\n\n# Build precision and covariance\nprec_mat = 2*sp.eye(10)\nprec_mat.setdiag(-1,k=-1)\nprec_mat.setdiag(-1,k=1)\nprec_mat = prec_mat.toarray()\ncov_mat = np.linalg.inv(prec_mat)\n\nfig, axs = plt.subplots(1, 2, figsize=(10,5))\naxs[0].imshow(prec_mat, cmap='Greens_r')\naxs[0].set_title(\"Precision\")\naxs[1].imshow(cov_mat, cmap='Greens_r')\naxs[1].set_title(\"Covariance\")\n\nplt.show()\n\n\n\n\n\nFigure 1: A sparse precision matrix will generally invert to a dense covariance matrix.\n\n\n\n\nThe unnormalized density function for the GMRF is given as\n\\[\n\\pi(x) \\propto \\exp\\left[ - \\frac{1}{2} \\left( x - \\mu \\right)^T Q \\left( x - \\mu \\right) \\right].\n\\]\n\n\n1. Sampling a 1D GMRF\nThe GMRF we will consider is that with \\(\\mu = 0\\) and precision \\(Q = D^T D\\), where\n\\[\nD = \\left(\\begin{array}{ccccc}\n1  &    &          &          &   \\\\\n-1 & 1  &          &          &   \\\\\n   & -1 & 1        &          &   \\\\\n   &    & \\ddots & \\ddots &   \\\\\n   &    &          & -1       & 1\n\\end{array}\\right)\n\\]\nis an \\(N \\times N\\) matrix. Intuitively, this precision corresponds to a Gaussian random-walk on the line whose increments and initial value have distribution \\(\\Delta x_i, x_0 \\sim \\mathcal{N}(0,1)\\). One way to sample this GMRF is to use the “covariance method”, which is to compute the Cholesky factorization \\(\\Sigma = L L^T\\) and then get a sample via\n\\[\nx = \\mu + L z\n\\]\nwhere \\(z \\sim \\mathcal{N}(0, I)\\). Let’s try this with \\(N = 100\\) points.\n\n\nCode\nN = 100\n\n# Build D matrix\nd_mat = sp.eye(N)\nd_mat.setdiag(-1, k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\n# Build precision and covariance\nprec_mat = d_mat.T @ d_mat\nprec_mat = prec_mat.toarray()\ncov_mat = np.linalg.inv(prec_mat)\n\n# Get Cholesky factor of covariance\nchol_fac = np.linalg.cholesky(cov_mat)\n\n# Compute sample\nnp.random.seed(0)\nz = np.random.normal(size=N)\nsample_cov_method = chol_fac @ z\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.plot(sample_cov_method, \"o-\", color=\"#A5D75E\")\n#axs.set_title(\"A single draw from the random walk on the line\")\nplt.show()\n\n\n\n\n\nFigure 2: A single draw from a 1D GMRF, using the covariance method.\n\n\n\n\nEasy enough. However, a central concern when working with GMRFs is how well the method scales with the dimension \\(N\\). Now let’s do a naive experiment where we time how long it takes to compute a single sample as we increase \\(N\\).\n\n\nCode\nimport time\nfrom sklearn import datasets, linear_model\n\nns = np.logspace(2.5, 4, num=50)\nns = np.rint(ns).astype(np.int32)\n\n\ndef draw_single_sample_cov_method(n):\n    \"\"\"Draws a single sample from the RW model of size n,\n    using the covariance method.\n    \"\"\"\n    # Build D matrix\n    d_mat = sp.eye(n)\n    d_mat.setdiag(-1,k=-1)\n\n    # Build precision and covariance\n    prec_mat = d_mat.T @ d_mat\n    prec_mat = prec_mat.toarray()\n    cov_mat = np.linalg.inv(prec_mat)\n\n    # Draw a sample\n    np.random.seed(0)\n    chol_fac = np.linalg.cholesky(cov_mat)\n    z = np.random.normal(size=n)\n    sample = chol_fac @ z\n    \n    return None\n\ntimes = []\nfor n in ns:\n    start = time.time()\n    draw_single_sample_cov_method(n)\n    end = time.time()\n    elapsed = end - start\n    times.append(elapsed)\n\nregr = linear_model.LinearRegression()\nregr.fit(np.log(ns)[:, None], np.log(times)[:, None])\nslope = regr.coef_[0][0]\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.loglog(ns, times, \"o-\", color=\"#A5D75E\")\naxs.set_xlabel(\"N\")\naxs.set_ylabel(\"Time (milliseconds)\")\naxs.set_title(f\"Slope = {slope:.1f}\")\nplt.show()\n\n\n\n\n\nFigure 3: Time to generate a single sample vs. number of points N, using the covariance method.\n\n\n\n\nGenerating a sample appears to cost \\(\\mathcal{O}(N^\\alpha)\\) with \\(\\alpha \\approx 2\\). The bottleneck in this method comes from the matrix inverse and Cholesky factorizations, which since the covariance is dense we should each expect to cost \\(\\mathcal{O}(N^3)\\) – for whatever reason it seems we manage to do better than this though.\nWe can do better than this by instead using the “precision method” that takes advantage of the sparsity of \\(Q\\). Inserting the Cholesky factorization \\(Q = L L ^T\\) into the previous covariance method, the precision method is to compute a sample via\n\\[\nx = \\mu + L^{-T} z\n\\]\nwith \\(z \\sim \\mathcal{N}(0,I)\\). However, it will be important how we compute the Cholesky factorization. If we just use np.linalg.cholesky which handles dense matrices, we shouldn’t expect to do any better. A nice property of sparse matrices is that if a matrix has a lower bandwidth of \\(p\\), then its lower Cholesky factor \\(L\\) also has bandwidth \\(p\\). There are specific algorithms that have been designed to take advantage of this fact, such as those implemented in sksparse.cholmod which we will use.\n\n\nCode\nfrom sksparse.cholmod import cholesky\n\nN = 100\n\n# Build D matrix\nd_mat = sp.eye(N)\nd_mat.setdiag(-1, k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\n# Build precision\nprec_mat = d_mat.T @ d_mat\nprec_mat = sp.csc_matrix(prec_mat)\n\n# Compute (sparse) cholesky factorization\n# The natural ordering method doesn't permute the nodes (our precision is already sparse)\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n# Solve L^T x = z\nnp.random.seed(0)\nz = np.random.normal(size=N)\nsample_prec_method = factor.solve_Lt(z, use_LDLt_decomposition=False)\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.plot(sample_prec_method, \"o-\", color=\"#A5D75E\")\n#axs.set_title(\"A single draw from the random walk on the line\")\nplt.show()\n\n\n\n\n\nFigure 4: A single draw from a 1D GMRF, using the precision method.\n\n\n\n\n\n\nCode\nimport time\nfrom sklearn import datasets, linear_model\n\nns = np.logspace(2.5, 7, num=50)\nns = np.rint(ns).astype(np.int32)\n\n\ndef draw_single_sample_prec_method(n):\n    \"\"\"Draws a single sample from the RW model of size n,\n    using the covariance method.\n    \"\"\"\n    d_mat = sp.eye(n)\n    d_mat.setdiag(-1, k=-1)\n    d_mat = sp.csr_matrix(d_mat)\n\n    prec_mat = d_mat.T @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n\n    factor = cholesky(prec_mat, ordering_method='natural')\n\n    np.random.seed(0)\n    z = np.random.normal(size=n)\n    sample = factor.solve_Lt(z, use_LDLt_decomposition=False)\n    \n    return None\n\ntimes = []\nfor n in ns:\n    start = time.time()\n    draw_single_sample_prec_method(n)\n    end = time.time()\n    elapsed = end - start\n    times.append(elapsed)\n\nregr = linear_model.LinearRegression()\nregr.fit(np.log(ns)[:, None], np.log(times)[:, None])\nslope = regr.coef_[0][0]\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.loglog(ns, times, \"o-\", color=\"#A5D75E\")\naxs.set_xlabel(\"N\")\naxs.set_ylabel(\"Time (milliseconds)\")\naxs.set_title(f\"Slope = {slope:.1f}\")\nplt.show()\n\n\n\n\n\nFigure 5: Time to generate a single sample vs. number of points N, using the precision method.\n\n\n\n\nNote that the precision method is significantly faster and has cost about \\(\\mathcal{O}(N)\\), as it leverages a sparse Cholesky factorization and a sparse linear system solver. For the rest of this post, we will use the precision method for drawing samples.\n\n\nHierarchical representation of the Cauchy distribution\nIn order to sample from a CMRF, we will use a scale mixture of normal representation for the Cauchy distribution2. Suppose we wish to sample\n\\[\nx \\sim \\text{Cauchy}(1).\n\\]\nwhere \\(s = 1\\) is the scale parameter of the Cauchy distribution. We can achieve this using the hierarchical representation\n\\[\n\\begin{align*}\n\\lambda &\\sim \\text{Gamma}(\\nu/2,\\nu/2) \\\\\nx \\, | \\, \\lambda &\\sim \\mathcal{N}(0, \\lambda^{-1})\n\\end{align*}\n\\]\nwith the choice \\(\\nu=1\\). For general \\(\\nu\\), this gives us \\(x \\sim \\text{Student-}t_\\nu\\) for which the Cauchy distribution is a special case. With this sampling scheme, we have that the marginal density for \\(x\\),\n\\[\n\\pi(x) = \\int \\pi(x \\, | \\lambda) \\, \\pi(\\lambda) \\, d\\lambda,\n\\]\nis the density for a \\(\\text{Cauchy}(1)\\)-distributed random variable. If we want to control the scale parameter of the Cauchy distribution, we can modify this as \\[\\begin{align*}\n\\lambda &\\sim \\text{Gamma}(1/2,s^2/2) \\\\\nx \\, | \\, \\lambda &\\sim \\mathcal{N}(0, \\lambda^{-1})\n\\end{align*}\\]\n\n\n\n\n2. Sampling a 1D CMRF\nNow let’s put this hierarchical representation to work. To obtain a CMRF from a GMRF, we will sample \\[\\begin{align*}\n\\lambda_1, \\ldots, \\lambda_N &\\sim \\text{Gamma}(1/2, s^2/2) \\\\\nH &= \\text{diag}(\\lambda_1, \\ldots, \\lambda_N) \\\\\nx \\, | \\, H &\\sim \\mathcal{N}\\left(\\mu, Q^{-1} = \\left( D^T H D \\right)^{-1} \\right)\n\\end{align*}\\] where \\(H\\) is a diagonal matrix collecting the hyper-parameters and \\(D\\) is the same differencing matrix as before. The marginal density for \\(x\\), \\[\\begin{equation*}\n\\pi(x) = \\int \\pi(x | \\lambda_1, \\ldots, \\lambda_N) \\pi(\\lambda_1, \\ldots, \\lambda_N) \\, d\\lambda_1 \\, \\ldots \\, d\\lambda_N\n\\end{equation*}\\] is the density function for a CMRF.\n\n\nCode\nfrom sksparse.cholmod import cholesky\nfrom scipy.stats import gamma\n\nN = 100\ns = 1\n\n# Build D matrix\nd_mat = sp.eye(N)\nd_mat.setdiag(-1, k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\n# Sample the hyperparameters\nnp.random.seed(0)\nh_mat = gamma.rvs(1/2, scale=(s**2)/2, size=N)\nh_mat = sp.diags(h_mat)\n\n# Build precision\nprec_mat = d_mat.T @ h_mat @ d_mat\nprec_mat = sp.csc_matrix(prec_mat)\n\n# Compute (sparse) cholesky factorization\n# The natural ordering method doesn't permute the nodes (our precision is already sparse)\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n# Solve L^T x = z\nz = np.random.normal(size=N)\nsample_prec_method = factor.solve_Lt(z, use_LDLt_decomposition=False)\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.plot(sample_prec_method, \"o-\", color=\"#A5D75E\")\n#axs.set_title(\"A single draw from the random walk on the line\")\nplt.show()\n\n\n\n\n\nFigure 6: A single draw from a 1D CMRF.\n\n\n\n\n\n\nCode\nN = 100\ns = 1\n\nd_mat = sp.eye(N)\nd_mat.setdiag(-1,k=-1)\n\nnp.random.seed(0)\nh_mat = gamma.rvs(1/2, scale=(s**2)/2, size=N)\nh_mat = sp.diags(h_mat)\n\nprec_mat = d_mat.T @  h_mat @ d_mat\nprec_mat = prec_mat.toarray()\ncov_mat = np.linalg.inv(prec_mat)\n\nfig, axs = plt.subplots(1, 2, figsize=(10,5))\naxs[0].imshow(prec_mat, cmap='Greens_r')\naxs[0].set_title(\"Precision\")\naxs[1].imshow(cov_mat, cmap='Greens_r')\naxs[1].set_title(\"Covariance\")\n\nplt.show()\n\n\n\n\n\nFigure 7: Precision and covariance matrices corresponding to the sample in Figure 6\n\n\n\n\nCompared to our sample from a GMRF from earlier, the sample from the CMRF has infrequent massive jumps. This makes sense, since our CMRF is modeling increments as being Cauchy-distributed and the Cauchy distribution has infinite variance. Let’s look at some more samples from each.\n\n\nCode\nfrom sksparse.cholmod import cholesky\n\nN = 100\nn_samples = 10\nnp.random.seed(1)\n\n# Arrays for storing samples\ngmrf_samples = np.zeros((n_samples, N))\ncmrf_samples = np.zeros((n_samples, N))\n\n# Build D matrix\nd_mat = sp.eye(N)\nd_mat.setdiag(-1, k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\nfig, axs = plt.subplots(n_samples, 2, figsize=(20,20))\n\nfor j in range(n_samples):\n\n    # Draw normal increments\n    z = np.random.normal(size=N)\n\n    # Get GMRF sample\n    prec_mat = d_mat.T @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n    factor = cholesky(prec_mat, ordering_method='natural')\n    gmrf_samples[j,:] = factor.solve_Lt(z, use_LDLt_decomposition=False)\n\n    # Get CMRF sample\n    h_mat = gamma.rvs(1/2, scale=(s**2)/2, size=N)\n    h_mat = sp.diags(h_mat)\n    prec_mat = d_mat.T @  h_mat @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n    factor = cholesky(prec_mat, ordering_method='natural')\n    cmrf_samples[j,:] = factor.solve_Lt(z, use_LDLt_decomposition=False)\n\n    # Plot\n    axs[j,0].plot(gmrf_samples[j,:], color=\"#A5D75E\")\n    axs[j,1].plot(cmrf_samples[j,:], color=\"#A5D75E\")\n    axs[j,0].set_xticks([])\n    axs[j,1].set_xticks([])\n\naxs[0,0].set_title(\"GMRF Samples\")\naxs[0,1].set_title(\"CMRF Samples\")\n\nplt.show()\n\n\n\n\n\nFigure 8: 10 samples each from our GMRF/CMRF. Each row uses the same draw \\(z \\sim \\mathcal{N}(0,I)\\).\n\n\n\n\n\n\n3. Sampling a 2D GMRF/CMRF\nMaking the jump to a 2D GMRF is not too difficult. Whereas in our earlier examples for a 1D random walk with \\(N\\) points our precision matrix \\(Q\\) was \\(N \\times N\\), in 2D on an \\(M \\times N\\) grid our precision matrix will need to be an \\(M^2 \\times N^2\\) matrix. The 2D GMRF we will consider is an “anisotropic difference prior” that is an extension of our 1D GMRF from earlier, with \\(\\mu = 0\\) and precision \\[\\begin{equation*}\nQ = F^T F.\n\\end{equation*}\\] Here \\[\\begin{equation*}\nF = \\begin{bmatrix}\n           D_1 \\\\\n           D_2\n         \\end{bmatrix},\n\\end{equation*}\\] where the \\(D_{1/2}\\) are given by \\[\\begin{align*}\nD_1 &= I_N \\otimes D, \\\\\nD_2 &= D \\otimes I_M,\n\\end{align*}\\] and \\(D\\) is the same matrix from our 1D example.\n\n\nCode\nM, N = 5, 5\n\n# Build D matrices\nd_mat_vert = sp.eye(M)\nd_mat_vert.setdiag(-1,k=-1)\neye_horiz = sp.eye(N)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(N)\nd_mat_horiz.setdiag(-1,k=-1)\neye_vert = sp.eye(M)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\nd_mat = sp.vstack([d_mat_one, d_mat_two])\nprec_mat = d_mat.T @ d_mat\n\nfig, axs = plt.subplots(1,2,figsize=(10,10))\naxs[0].imshow(d_mat.toarray(), cmap='Greens_r')\naxs[0].set_title('F')\naxs[1].imshow(prec_mat.toarray(), cmap='Greens_r')\naxs[1].set_title('Q')\n\nplt.show()\n\n\n\n\n\nFigure 9: The matrices \\(F\\) and \\(Q\\) for a \\(20 \\times 20\\) lattice.\n\n\n\n\nNow that we have our precision matrix, we can sample the GMRF just as we did in the 1D case.\n\n\nCode\nM, N = 200, 200\n\n# Build D matrices\nd_mat_vert = sp.eye(M)\nd_mat_vert.setdiag(-1,k=-1)\neye_horiz = sp.eye(N)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(N)\nd_mat_horiz.setdiag(-1,k=-1)\neye_vert = sp.eye(M)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\nd_mat = sp.vstack([d_mat_one, d_mat_two])\n\n# Build precision\nprec_mat = d_mat.T @ d_mat\nprec_mat = sp.csc_matrix(prec_mat)\n\n# Get Cholesky factor\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n# Draw a sample\nnp.random.seed(0)\nz = np.random.normal(size=(M,N)).flatten()\nsample = factor.solve_Lt(z, use_LDLt_decomposition=False)\nsample = sample.reshape((M,N))\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nplt.figure(figsize=(8,8))\nax = plt.gca()\nim = ax.imshow(sample)\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\nplt.colorbar(im, cax=cax)\nplt.show()\n\n\n\n\n\nFigure 10: A sample of our GMRF on a \\(200 \\times 200\\) lattice.\n\n\n\n\nLet’s also make a “movie” for a sequence of samples.\n\nCode\nfrom moviepy.editor import ImageSequenceClip\n\nn_samples = 10\nsamples = np.zeros((n_samples, M, N))\n\n# Draw samples\nnp.random.seed(0)\nfor j in range(n_samples):\n    z = np.random.normal(size=(M,N)).flatten()\n    samples[j,:,:] = factor.solve_Lt(z, use_LDLt_decomposition=False).reshape((M,N))\n\nfps = int(0.1*n_samples)\nmin_val = np.amin(samples)\nmax_val = np.amax(samples)\n\nsamples -= min_val\nsamples /= max_val - min_val\n\ncolored_samples = np.zeros((n_samples, M, N, 4))\ncmap = plt.get_cmap()\nfor j in range(n_samples):\n    #print(cmap(samples[j,:,:]))\n    colored_samples[j,:,:,:] = cmap(samples[j,:,:])\n\ncolored_samples *= 255\n\nclip = ImageSequenceClip(list(colored_samples), fps=fps)\nclip.write_gif('gmrf_samples.gif', fps=fps, verbose=False, logger=None)\n\n\nNow that we can sample from a 2D GMRF, it is straightforward to add in hyper-parameters that turn our GMRF into a CMRF. To do this, we sample \\[\\begin{align*}\n\\lambda_1, \\ldots, \\lambda_{2N^2} &\\sim \\text{Gamma}(1/2, s^2/2) \\\\\nH \\, | \\, \\lambda_1, \\ldots, \\lambda_{2N^2} &= \\text{diag}(\\lambda_1, \\ldots, \\lambda_{2N^2}) \\\\\nx \\, | \\, H &\\sim \\mathcal{N}\\left(\\mu, Q^{-1} = \\left( F^T H F \\right)^{-1} \\right).\n\\end{align*}\\] Let’s try sampling this.\n\n\nCode\nfrom scipy.stats import gamma\n\nM, N = 200, 200\ns = 1.0\n\n# Build D matrices\nd_mat_vert = sp.eye(M)\nd_mat_vert.setdiag(-1,k=-1)\neye_horiz = sp.eye(N)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(N)\nd_mat_horiz.setdiag(-1,k=-1)\neye_vert = sp.eye(M)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\nd_mat = sp.vstack([d_mat_one, d_mat_two])\n\n# Sample hyperparamters\nh_mat = gamma.rvs(1/2, scale=1/((s**2)/2), size=2*M*N)\nh_mat = sp.diags(h_mat)\n\n# Build precision\nprec_mat = d_mat.T @ h_mat @ d_mat\nprec_mat = sp.csc_matrix(prec_mat)\n\n# Get Cholesky factor\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n# Draw a sample\nnp.random.seed(0)\nz = np.random.normal(size=(M,N)).flatten()\nsample = factor.solve_Lt(z, use_LDLt_decomposition=False)\nsample = sample.reshape((M,N))\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nplt.figure(figsize=(8,8))\nax = plt.gca()\nim = ax.imshow(sample)\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\nplt.colorbar(im, cax=cax)\nplt.show()\n\n\n\n\n\nFigure 11: A sample of our CMRF on a \\(200 \\times 200\\) lattice.\n\n\n\n\nLet’s make another movie while we’re at it.\n\nCode\nfrom moviepy.editor import ImageSequenceClip\n\nn_samples = 10\nsamples = np.zeros((n_samples, M, N))\n\n# Draw samples\nnp.random.seed(0)\nfor j in range(n_samples):\n\n    # Sample hyperparamters\n    h_mat = gamma.rvs(1/2, scale=1/((s**2)/2), size=2*M*N)\n    h_mat = sp.diags(h_mat)\n\n    # Build precision\n    prec_mat = d_mat.T @ h_mat @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n\n    # Get Cholesky factor\n    factor = cholesky(prec_mat, ordering_method='natural')\n\n    z = np.random.normal(size=(M,N)).flatten()\n    samples[j,:,:] = factor.solve_Lt(z, use_LDLt_decomposition=False).reshape((M,N))\n\nfps = int(0.1*n_samples)\nmin_val = np.amin(samples)\nmax_val = np.amax(samples)\n\nsamples -= min_val\nsamples /= max_val - min_val\n\ncolored_samples = np.zeros((n_samples, M, N, 4))\ncmap = plt.get_cmap()\nfor j in range(n_samples):\n    #print(cmap(samples[j,:,:]))\n    colored_samples[j,:,:,:] = cmap(samples[j,:,:])\n\ncolored_samples *= 255\n\nclip = ImageSequenceClip(list(colored_samples), fps=fps)\nclip.write_gif('cmrf_samples.gif', fps=fps, verbose=False, logger=None)\n\n\nSo we have achieved our goal. In a future post, we will put this to work as a prior in an inverse problem.\n\n\nBonus: some extra MRFs\nWhile we used a scale mixture of normal representation for the Cauchy distribution, we can also obtain other distributions as scale mixtures of normals. For example, we can simulate a MRF where the increments follow a Laplace distribution by sampling \\[\\begin{align*}\n\\beta_1, \\ldots, \\beta_{2N^2} &\\sim \\text{Exponential} \\\\\n\\lambda_i \\, | \\, \\beta_i &= \\frac{1}{2\\beta_i} \\\\\nH \\, | \\, \\lambda_1, \\ldots, \\lambda_{2N^2} &= \\text{diag}(\\lambda_1, \\ldots, \\lambda_{2N^2}) \\\\\nx \\, | \\, H &\\sim \\mathcal{N}\\left(\\mu, Q^{-1} = \\left( F^T H F \\right)^{-1} \\right).\n\\end{align*}\\]\nSimilarly, we can get a MRF increments with Horshoe-distributed [4] increments by sampling \\[\\begin{align*}\n\\beta_1, \\ldots, \\beta_{2N^2} &\\sim \\text{C}^+(0, 1) \\\\\n\\lambda_i \\, | \\, \\beta_i, \\tau &= \\frac{1}{\\beta_i^2 \\tau^2} \\\\\nH \\, | \\, \\lambda_1, \\ldots, \\lambda_{2N^2} &= \\text{diag}(\\lambda_1, \\ldots, \\lambda_{2N^2}) \\\\\nx \\, | \\, H &\\sim \\mathcal{N}\\left(\\mu, Q^{-1} = \\left( F^T H F \\right)^{-1} \\right)\n\\end{align*}\\] where \\(\\tau\\) is some fixed hyper-hyper-parameter and \\(C^+\\) denotes the half-Cauchy distribution. Below are some movies of samples from a Laplace MRF and Horshoe MRF, respectively.\n\nCode\nfrom moviepy.editor import ImageSequenceClip\n\nn_samples = 10\nsamples = np.zeros((n_samples, M, N))\n\n# Draw samples\nnp.random.seed(0)\nfor j in range(n_samples):\n\n    # Sample hyperparamters\n    h_mat = (1/(2*np.random.exponential(size=2*M*N)))\n    h_mat = sp.diags(h_mat)\n\n    # Build precision\n    prec_mat = d_mat.T @ h_mat @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n\n    # Get Cholesky factor\n    factor = cholesky(prec_mat, ordering_method='natural')\n\n    z = np.random.normal(size=(M,N)).flatten()\n    samples[j,:,:] = factor.solve_Lt(z, use_LDLt_decomposition=False).reshape((M,N))\n\nfps = int(0.1*n_samples)\nmin_val = np.amin(samples)\nmax_val = np.amax(samples)\n\nsamples -= min_val\nsamples /= max_val - min_val\n\ncolored_samples = np.zeros((n_samples, M, N, 4))\ncmap = plt.get_cmap()\nfor j in range(n_samples):\n    #print(cmap(samples[j,:,:]))\n    colored_samples[j,:,:,:] = cmap(samples[j,:,:])\n\ncolored_samples *= 255\n\nclip = ImageSequenceClip(list(colored_samples), fps=fps)\nclip.write_gif('lmrf_samples.gif', fps=fps, verbose=False, logger=None)\n\n\nfrom scipy.stats import halfcauchy\n\ntau = 1.0\n\nn_samples = 10\nsamples = np.zeros((n_samples, M, N))\n\n# Draw samples\nnp.random.seed(0)\nfor j in range(n_samples):\n\n    # Sample hyperparamters\n    h_mat = (  1/(  (tau*halfcauchy.rvs(size=2*M*N))**2)  )\n    h_mat = sp.diags(h_mat)\n\n    # Build precision\n    prec_mat = d_mat.T @ h_mat @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n\n    # Get Cholesky factor\n    factor = cholesky(prec_mat, ordering_method='natural')\n\n    z = np.random.normal(size=(M,N)).flatten()\n    samples[j,:,:] = factor.solve_Lt(z, use_LDLt_decomposition=False).reshape((M,N))\n\nfps = int(0.1*n_samples)\nmin_val = np.amin(samples)\nmax_val = np.amax(samples)\n\nsamples -= min_val\nsamples /= max_val - min_val\n\ncolored_samples = np.zeros((n_samples, M, N, 4))\ncmap = plt.get_cmap()\nfor j in range(n_samples):\n    #print(cmap(samples[j,:,:]))\n    colored_samples[j,:,:,:] = cmap(samples[j,:,:])\n\ncolored_samples *= 255\n\nclip = ImageSequenceClip(list(colored_samples), fps=fps)\nclip.write_gif('hmrf_samples.gif', fps=fps, verbose=False, logger=None)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n[1] Chada, N. K., Roininen, L. and Suuronen, J. (2021). Cauchy markov random field priors for bayesian inversion.\n\n\n[2] Markkanen, M., Roininen, L., Huttunen, J. M.  J. and Lasanen, S. (2019). Cauchy difference priors for edge-preserving bayesian inversion. Journal of Inverse and Ill-posed Problems 27 225–40.\n\n\n[3] Rue, H. and Held, L. (2005). Gaussian markov random fields: Theory and applications.\n\n\n[4] Carvalho, C. M., Polson, N. G. and Scott, J. G. (2009). Handling sparsity via the horseshoe. In AISTATS.\n\nFootnotes\n\n\nNOT using a Markov Chain Monte Carlo (MCMC) method.↩︎\nSee Chapter 4 of [3], also here↩︎"
  },
  {
    "objectID": "posts/hierarchical_cmrf.html#section",
    "href": "posts/hierarchical_cmrf.html#section",
    "title": "Sampling a Cauchy Markov Random Field",
    "section": "",
    "text": "Q^{-1} = (LLT){-1} = L^{-T} L^{-1} ##\n\n\nCode\n# Build precision\nd_mat = sp.eye(N)\nd_mat.setdiag(-1,k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\nprec_mat = d_mat.T @ d_mat\nfactor = cholesky(prec_mat, ordering_method='natural')\nfactor.cholesky_inplace(prec_mat)\n\nplt.imshow(prec_mat.toarray())\nplt.show()\n\n\n\n\n\n\nCode\nL = factor.L()\nL = L.toarray()\nLtinv = np.linalg.inv(L).T\n\n\n\nCode\nplt.imshow(L)\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.imshow(Ltinv)\nplt.show()\n\n\n\n\n\n\nCode\n# Build precision\nprec_mat = d_mat.T @ d_mat\n\n# Compute (sparse) cholesky factorization\n# The natural ordering method doesn't permute the nodes (our precision is already sparse)\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n\nCode\ninv_mat = factor.inv().toarray()\n\n\n\nCode\ncov_mat\n\n\narray([[1., 1., 1., 1., 1.],\n       [1., 2., 2., 2., 2.],\n       [1., 2., 3., 3., 3.],\n       [1., 2., 3., 4., 4.],\n       [1., 2., 3., 4., 5.]])\n\n\n\n\nCode\ninv_mat\n\n\narray([[1., 1., 1., 1., 1.],\n       [1., 2., 2., 2., 2.],\n       [1., 2., 3., 3., 3.],\n       [1., 2., 3., 4., 4.],\n       [1., 2., 3., 4., 5.]])\n\n\n\nCode\n# #| label: fig-random_walk_prec_cov\n# #| fig-cap: Precision and covariance matrices for the random walk.\n\n# N = 10\n# d_mat = sp.eye(N)\n# d_mat.setdiag(-1,k=-1)\n# d_mat = sp.csr_matrix(d_mat)\n\n# prec_mat = d_mat.T @ d_mat\n# prec_mat = prec_mat.toarray()\n# cov_mat = np.linalg.inv(prec_mat)\n\n# fig, axs = plt.subplots(1, 2, figsize=(10,5))\n# axs[0].imshow(prec_mat, cmap='Greens_r')\n# axs[0].set_title(\"Precision\")\n# axs[1].imshow(cov_mat, cmap='Greens_r')\n# axs[1].set_title(\"Covariance\")\n\n# plt.show()\n\n\n\nCode\n# #| label: fig-random_walk_draw\n# #| fig-cap: A single draw from a 1D GMRF.\n\n# np.random.seed(0)\n# chol_fac = np.linalg.cholesky(cov_mat)\n# z = np.random.normal(size=N)\n# sample = chol_fac @ z\n\n# fig, axs = plt.subplots(figsize=(10,5))\n# axs.plot(sample, \"o-\", color=\"#A5D75E\")\n# #axs.set_title(\"A single draw from the random walk on the line\")\n# plt.show()\n\n\n\n\n\nThese examples with the number of points \\(N = 10\\) worked fine. Now let’s do a naive experiment where we time how long it takes to compute a single sample as we increase \\(N\\).\n\n\nCode\nimport time\nfrom sklearn import datasets, linear_model\n\nns = np.logspace(2.5, 4, num=50)\nns = np.rint(ns).astype(np.int32)\n\ndef draw_single_sample(n):\n    \"\"\"Draws a single sample from the RW model of size n.\n    \"\"\"\n    d_mat = sp.eye(n)\n    d_mat.setdiag(-1,k=-1)\n\n    prec_mat = d_mat.T @ d_mat\n    prec_mat = prec_mat.toarray()\n    cov_mat = np.linalg.inv(prec_mat)\n\n    chol_fac = np.linalg.cholesky(cov_mat)\n    z = np.random.normal(size=n)\n    sample = chol_fac @ z\n    \n    return None\n\n\ntimes = []\nfor n in ns:\n    start = time.time()\n    draw_single_sample(n)\n    end = time.time()\n    elapsed = end - start\n    times.append(elapsed)\n\nregr = linear_model.LinearRegression()\nregr.fit(np.log(ns)[:, None], np.log(times)[:, None])\nslope = regr.coef_[0][0]\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.loglog(ns, times, \"o-\", color=\"#A5D75E\")\naxs.set_xlabel(\"N\")\naxs.set_ylabel(\"Time (seconds)\")\naxs.set_title(f\"Time to generate one sample, increasing N (slope = {slope:.1f})\")\nplt.show()\n\n\n\n\n\nFigure 5: Time to generate a single sample vs. number of points N.\n\n\n\n\nThe bottleneck we see here comes from the Cholesky factorization \\(Q = L L^T\\).\n\nCode\nfrom scipy.stats import gamma\n\n\n\nCode\nN = 1000\neta = 100\n\nd_mat = sp.eye(N)\nd_mat.setdiag(-1,k=-1)\n\nweights = gamma.rvs(1/2, scale=(eta**2)/2, size=N)\nweights_diag = sp.diags(weights)\n\nprec_mat = d_mat.T @  weights_diag @ d_mat\nprec_mat = prec_mat.toarray()\ncov_mat = np.linalg.inv(prec_mat)\n\nfig, axs = plt.subplots(1, 2, figsize=(10,5))\naxs[0].imshow(prec_mat, cmap='Greens_r')\naxs[0].set_title(\"Precision\")\naxs[1].imshow(cov_mat, cmap='Greens_r')\naxs[1].set_title(\"Covariance\")\n\nplt.show()\n\n\n\n\n\n\n\nCode\nchol_fac = np.linalg.cholesky(cov_mat)\nz = np.random.normal(size=N)\nsample = chol_fac @ z\n\nplt.figure(figsize=(10,5))\nplt.plot(sample)\nplt.show()\n\n\n\n\n\n\nCode\nN = 4\neta = 10\n\nd_mat = sp.eye(N)\nd_mat.setdiag(-1,k=-1)\n\nweights = gamma.rvs(1/2, scale=(eta**2)/2, size=N)\nweights_diag = sp.diags(weights)\n\nprec_mat = d_mat.T @  weights_diag @ d_mat\nprec_mat = prec_mat.toarray()\ncov_mat = np.linalg.inv(prec_mat)\n\n\n\nCode\nd_mat.toarray()\n\n\narray([[ 1.,  0.,  0.,  0.],\n       [-1.,  1.,  0.,  0.],\n       [ 0., -1.,  1.,  0.],\n       [ 0.,  0., -1.,  1.]])\n\n\n\n\nCode\nd_mat.T.toarray()\n\n\narray([[ 1., -1.,  0.,  0.],\n       [ 0.,  1., -1.,  0.],\n       [ 0.,  0.,  1., -1.],\n       [ 0.,  0.,  0.,  1.]])\n\n\n\n\nCode\n(d_mat.T @ d_mat).toarray()\n\n\narray([[ 2., -1.,  0.,  0.],\n       [-1.,  2., -1.,  0.],\n       [ 0., -1.,  2., -1.],\n       [ 0.,  0., -1.,  1.]])\n\n\n\n\nCode\nweights_diag.toarray()\n\n\narray([[ 7.55762179,  0.        ,  0.        ,  0.        ],\n       [ 0.        , 22.33463962,  0.        ,  0.        ],\n       [ 0.        ,  0.        , 63.03078966,  0.        ],\n       [ 0.        ,  0.        ,  0.        , 12.59011706]])"
  }
]