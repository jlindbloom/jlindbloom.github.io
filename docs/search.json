[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jonathan Lindbloom",
    "section": "",
    "text": "I am an applied mathematics PhD student at Dartmouth College, originally from Dallas, Texas. I like learning about computational methods for solving (inverse) problems and quantifying uncertainty."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "BCCB matrices.\n\n\n\n\n\n\nJul 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nLet’s see if we can sample this directly using a hierarchical representation.\n\n\n\n\n\n\nJun 15, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jonathan Lindbloom",
    "section": "",
    "text": "I am an applied mathematics PhD student at Dartmouth College, originally from Dallas, Texas. I like learning about computational methods for solving (inverse) problems and quantifying uncertainty."
  },
  {
    "objectID": "posts/test.html",
    "href": "posts/test.html",
    "title": "Test post",
    "section": "",
    "text": "I think it does."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Jonathan Lindbloom",
    "section": "Education",
    "text": "Education\nDartmouth College | Hanover, NH PhD in Mathematics | Sept 2021 - Present\nSouthern Methodist University | St. Paul MA B.A in Economics | Aug 2017 - May 2021"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Spet 2012 - April 2018"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jonathan Lindbloom",
    "section": "Education",
    "text": "Education\nDartmouth College | Hanover, NH\nPhD in Mathematics\nSept 2021 - Present\nSouthern Methodist University | Dallas, TX\nBS in Mathematics, BBA in Finance\nAug 2017 - May 2021"
  },
  {
    "objectID": "posts/test2.html",
    "href": "posts/test2.html",
    "title": "Sampling a Cauchy Markov Random Field Prior",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 2: A line plot on a polar axis\n\n\n\n\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\nDoes this work?"
  },
  {
    "objectID": "posts/hierarchical_cmrf.html",
    "href": "posts/hierarchical_cmrf.html",
    "title": "Sampling a Cauchy Markov Random Field",
    "section": "",
    "text": "Our main reference for this post will be [3] which is a comprehensive text on the subject. Our plan of attack is to:\n\nSample from a 1D GMRF,\nBuild off of this to sample a 1D CMRF using a hierarchical representation of the Cauchy distribution,\nBuild off of this to sample from a 2D GMRF and CMRF.\n\n\nGaussian Markov Random Fields\nA GMRF can be thought of as a multivariate random variable\n\\[\nX \\sim \\mathcal{N}\\left( \\mu, Q^{-1} \\right)\n\\]\nwith mean vector \\(\\mu\\) and precision \\(Q\\), where \\(Q\\) is a sparse banded matrix (or is under some re-ordering of the components). This is just one view, as there are many other perspectives/interpretations (see [3]). Here the precision \\(Q\\) is the inverse of the covariance matrix, \\(Q = \\Sigma^{-1}\\). The reason we care about the sparsity of \\(Q\\) rather than that of the covariance \\(\\Sigma\\) is because the precision matrix encodes the conditional independence structure of the field, whereas the covariance matrix will be dense even if the precision is sparse. The sparsity of \\(Q\\) is what allows us to do efficient computations with the corresponding GMRF and avoid the “Big \\(N\\) Problem” that more general Gaussian fields suffer from.\n\n\nCode\nimport numpy as np\nimport scipy.sparse as sp\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.facecolor'] = '#0F2537'\nplt.rcParams['text.color'] = 'white'\nplt.rcParams['axes.labelcolor'] = 'white'\nplt.rcParams['xtick.color'] = 'white'\nplt.rcParams['ytick.color'] = 'white'\n\n# Build precision and covariance\nprec_mat = 2*sp.eye(10)\nprec_mat.setdiag(-1,k=-1)\nprec_mat.setdiag(-1,k=1)\nprec_mat = prec_mat.toarray()\ncov_mat = np.linalg.inv(prec_mat)\n\nfig, axs = plt.subplots(1, 2, figsize=(10,5))\naxs[0].imshow(prec_mat, cmap='Greens_r')\naxs[0].set_title(\"Precision\")\naxs[1].imshow(cov_mat, cmap='Greens_r')\naxs[1].set_title(\"Covariance\")\n\nplt.show()\n\n\n\n\n\nFigure 1: A sparse precision matrix will generally invert to a dense covariance matrix.\n\n\n\n\nThe unnormalized density function for the GMRF is given as\n\\[\n\\pi(x) \\propto \\exp\\left[ - \\frac{1}{2} \\left( x - \\mu \\right)^T Q \\left( x - \\mu \\right) \\right].\n\\]\n\n\n1. Sampling a 1D GMRF\nThe GMRF we will consider is that with \\(\\mu = 0\\) and precision \\(Q = D^T D\\), where\n\\[\nD = \\left(\\begin{array}{ccccc}\n1  &    &          &          &   \\\\\n-1 & 1  &          &          &   \\\\\n   & -1 & 1        &          &   \\\\\n   &    & \\ddots & \\ddots &   \\\\\n   &    &          & -1       & 1\n\\end{array}\\right)\n\\]\nis an \\(N \\times N\\) matrix. Intuitively, this precision corresponds to a Gaussian random-walk on the line whose increments and initial value have distribution \\(\\Delta x_i, x_0 \\sim \\mathcal{N}(0,1)\\). One way to sample this GMRF is to use the “covariance method”, which is to compute the Cholesky factorization \\(\\Sigma = L L^T\\) and then get a sample via\n\\[\nx = \\mu + L z\n\\]\nwhere \\(z \\sim \\mathcal{N}(0, I)\\). Let’s try this with \\(N = 100\\) points.\n\n\nCode\nN = 100\n\n# Build D matrix\nd_mat = sp.eye(N)\nd_mat.setdiag(-1, k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\n# Build precision and covariance\nprec_mat = d_mat.T @ d_mat\nprec_mat = prec_mat.toarray()\ncov_mat = np.linalg.inv(prec_mat)\n\n# Get Cholesky factor of covariance\nchol_fac = np.linalg.cholesky(cov_mat)\n\n# Compute sample\nnp.random.seed(0)\nz = np.random.normal(size=N)\nsample_cov_method = chol_fac @ z\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.plot(sample_cov_method, \"o-\", color=\"#A5D75E\")\n#axs.set_title(\"A single draw from the random walk on the line\")\nplt.show()\n\n\n\n\n\nFigure 2: A single draw from a 1D GMRF, using the covariance method.\n\n\n\n\nEasy enough. However, a central concern when working with GMRFs is how well the method scales with the dimension \\(N\\). Now let’s do a naive experiment where we time how long it takes to compute a single sample as we increase \\(N\\).\n\n\nCode\nimport time\nfrom sklearn import datasets, linear_model\n\nns = np.logspace(2.5, 4, num=50)\nns = np.rint(ns).astype(np.int32)\n\n\ndef draw_single_sample_cov_method(n):\n    \"\"\"Draws a single sample from the RW model of size n,\n    using the covariance method.\n    \"\"\"\n    # Build D matrix\n    d_mat = sp.eye(n)\n    d_mat.setdiag(-1,k=-1)\n\n    # Build precision and covariance\n    prec_mat = d_mat.T @ d_mat\n    prec_mat = prec_mat.toarray()\n    cov_mat = np.linalg.inv(prec_mat)\n\n    # Draw a sample\n    np.random.seed(0)\n    chol_fac = np.linalg.cholesky(cov_mat)\n    z = np.random.normal(size=n)\n    sample = chol_fac @ z\n    \n    return None\n\ntimes = []\nfor n in ns:\n    start = time.time()\n    draw_single_sample_cov_method(n)\n    end = time.time()\n    elapsed = end - start\n    times.append(elapsed)\n\nregr = linear_model.LinearRegression()\nregr.fit(np.log(ns)[:, None], np.log(times)[:, None])\nslope = regr.coef_[0][0]\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.loglog(ns, times, \"o-\", color=\"#A5D75E\")\naxs.set_xlabel(\"N\")\naxs.set_ylabel(\"Time (milliseconds)\")\naxs.set_title(f\"Slope = {slope:.1f}\")\nplt.show()\n\n\n\n\n\nFigure 3: Time to generate a single sample vs. number of points N, using the covariance method.\n\n\n\n\nGenerating a sample appears to cost \\(\\mathcal{O}(N^\\alpha)\\) with \\(\\alpha \\approx 2.4\\). The bottleneck in this method comes from the matrix inverse and Cholesky factorizations, which since the covariance is dense we should each expect to cost \\(\\mathcal{O}(N^3)\\) – for whatever reason it seems we manage to do better than this though.\nWe can do better than this by instead using the “precision method” that takes advantage of the sparsity of \\(Q\\). Inserting the Cholesky factorization \\(Q = L L ^T\\) into the previous covariance method, the precision method is to compute a sample via\n\\[\nx = \\mu + L^{-T} z\n\\]\nwith \\(z \\sim \\mathcal{N}(0,I)\\). However, it will be important how we compute the Cholesky factorization. If we just use np.linalg.cholesky which handles dense matrices, we shouldn’t expect to do any better. A nice property of sparse matrices is that if a matrix has a lower bandwidth of \\(p\\), then its lower Cholesky factor \\(L\\) also has bandwidth \\(p\\). There are specific algorithms that have been designed to take advantage of this fact, such as those implemented in sksparse.cholmod which we will use.\n\n\nCode\nfrom sksparse.cholmod import cholesky\n\nN = 100\n\n# Build D matrix\nd_mat = sp.eye(N)\nd_mat.setdiag(-1, k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\n# Build precision\nprec_mat = d_mat.T @ d_mat\nprec_mat = sp.csc_matrix(prec_mat)\n\n# Compute (sparse) cholesky factorization\n# The natural ordering method doesn't permute the nodes (our precision is already sparse)\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n# Solve L^T x = z\nnp.random.seed(0)\nz = np.random.normal(size=N)\nsample_prec_method = factor.solve_Lt(z, use_LDLt_decomposition=False)\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.plot(sample_prec_method, \"o-\", color=\"#A5D75E\")\n#axs.set_title(\"A single draw from the random walk on the line\")\nplt.show()\n\n\n\n\n\nFigure 4: A single draw from a 1D GMRF, using the precision method.\n\n\n\n\n\n\nCode\nimport time\nfrom sklearn import datasets, linear_model\n\nns = np.logspace(2.5, 7, num=50)\nns = np.rint(ns).astype(np.int32)\n\n\ndef draw_single_sample_prec_method(n):\n    \"\"\"Draws a single sample from the RW model of size n,\n    using the covariance method.\n    \"\"\"\n    d_mat = sp.eye(n)\n    d_mat.setdiag(-1, k=-1)\n    d_mat = sp.csr_matrix(d_mat)\n\n    prec_mat = d_mat.T @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n\n    factor = cholesky(prec_mat, ordering_method='natural')\n\n    np.random.seed(0)\n    z = np.random.normal(size=n)\n    sample = factor.solve_Lt(z, use_LDLt_decomposition=False)\n    \n    return None\n\ntimes = []\nfor n in ns:\n    start = time.time()\n    draw_single_sample_prec_method(n)\n    end = time.time()\n    elapsed = end - start\n    times.append(elapsed)\n\nregr = linear_model.LinearRegression()\nregr.fit(np.log(ns)[:, None], np.log(times)[:, None])\nslope = regr.coef_[0][0]\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.loglog(ns, times, \"o-\", color=\"#A5D75E\")\naxs.set_xlabel(\"N\")\naxs.set_ylabel(\"Time (milliseconds)\")\naxs.set_title(f\"Slope = {slope:.1f}\")\nplt.show()\n\n\n\n\n\nFigure 5: Time to generate a single sample vs. number of points N, using the precision method.\n\n\n\n\nNote that the precision method is significantly faster and has cost about \\(\\mathcal{O}(N)\\), as it leverages a sparse Cholesky factorization and a sparse linear system solver. For the rest of this post, we will use the precision method for drawing samples.\n\n\nHierarchical representation of the Cauchy distribution\nIn order to sample from a CMRF, we will use a scale mixture of normal representation for the Cauchy distribution2. Suppose we wish to sample\n\\[\nx \\sim \\text{Cauchy}(1).\n\\]\nwhere \\(s = 1\\) is the scale parameter of the Cauchy distribution. We can achieve this using the hierarchical representation\n\\[\n\\begin{align*}\n\\lambda &\\sim \\text{Gamma}(\\nu/2,\\nu/2) \\\\\nx \\, | \\, \\lambda &\\sim \\mathcal{N}(0, \\lambda^{-1})\n\\end{align*}\n\\]\nwith the choice \\(\\nu=1\\). For general \\(\\nu\\), this gives us \\(x \\sim \\text{Student-}t_\\nu\\) for which the Cauchy distribution is a special case. With this sampling scheme, we have that the marginal density for \\(x\\),\n\\[\n\\pi(x) = \\int \\pi(x \\, | \\lambda) \\, \\pi(\\lambda) \\, d\\lambda,\n\\]\nis the density for a \\(\\text{Cauchy}(1)\\)-distributed random variable. If we want to control the scale parameter of the Cauchy distribution, we can modify this as \\[\\begin{align*}\n\\lambda &\\sim \\text{Gamma}(1/2,s^2/2) \\\\\nx \\, | \\, \\lambda &\\sim \\mathcal{N}(0, \\lambda^{-1})\n\\end{align*}\\]\n\n\n\n\n2. Sampling a 1D CMRF\nNow let’s put this hierarchical representation to work. To obtain a CMRF from a GMRF, we will sample \\[\\begin{align*}\n\\lambda_1, \\ldots, \\lambda_N &\\sim \\text{Gamma}(1/2, s^2/2) \\\\\nH &= \\text{diag}(\\lambda_1, \\ldots, \\lambda_N) \\\\\nx \\, | \\, H &\\sim \\mathcal{N}\\left(\\mu, Q^{-1} = \\left( D^T H D \\right)^{-1} \\right)\n\\end{align*}\\] where \\(H\\) is a diagonal matrix collecting the hyper-parameters and \\(D\\) is the same differencing matrix as before. The marginal density for \\(x\\), \\[\\begin{equation*}\n\\pi(x) = \\int \\pi(x | \\lambda_1, \\ldots, \\lambda_N) \\pi(\\lambda_1, \\ldots, \\lambda_N) \\, d\\lambda_1 \\, \\ldots \\, d\\lambda_N\n\\end{equation*}\\] is the density function for a CMRF.\n\n\nCode\nfrom sksparse.cholmod import cholesky\nfrom scipy.stats import gamma\n\nN = 100\ns = 1\n\n# Build D matrix\nd_mat = sp.eye(N)\nd_mat.setdiag(-1, k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\n# Sample the hyperparameters\nnp.random.seed(0)\nh_mat = gamma.rvs(1/2, scale=(s**2)/2, size=N)\nh_mat = sp.diags(h_mat)\n\n# Build precision\nprec_mat = d_mat.T @ h_mat @ d_mat\nprec_mat = sp.csc_matrix(prec_mat)\n\n# Compute (sparse) cholesky factorization\n# The natural ordering method doesn't permute the nodes (our precision is already sparse)\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n# Solve L^T x = z\nz = np.random.normal(size=N)\nsample_prec_method = factor.solve_Lt(z, use_LDLt_decomposition=False)\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.plot(sample_prec_method, \"o-\", color=\"#A5D75E\")\n#axs.set_title(\"A single draw from the random walk on the line\")\nplt.show()\n\n\n\n\n\nFigure 6: A single draw from a 1D CMRF.\n\n\n\n\n\n\nCode\nN = 100\ns = 1\n\nd_mat = sp.eye(N)\nd_mat.setdiag(-1,k=-1)\n\nnp.random.seed(0)\nh_mat = gamma.rvs(1/2, scale=(s**2)/2, size=N)\nh_mat = sp.diags(h_mat)\n\nprec_mat = d_mat.T @  h_mat @ d_mat\nprec_mat = prec_mat.toarray()\ncov_mat = np.linalg.inv(prec_mat)\n\nfig, axs = plt.subplots(1, 2, figsize=(10,5))\naxs[0].imshow(prec_mat, cmap='Greens_r')\naxs[0].set_title(\"Precision\")\naxs[1].imshow(cov_mat, cmap='Greens_r')\naxs[1].set_title(\"Covariance\")\n\nplt.show()\n\n\n\n\n\nFigure 7: Precision and covariance matrices corresponding to the sample in Figure 6\n\n\n\n\nCompared to our sample from a GMRF from earlier, the sample from the CMRF has infrequent massive jumps. This makes sense, since our CMRF is modeling increments as being Cauchy-distributed and the Cauchy distribution has infinite variance. Let’s look at some more samples from each.\n\n\nCode\nfrom sksparse.cholmod import cholesky\n\nN = 100\nn_samples = 10\nnp.random.seed(1)\n\n# Arrays for storing samples\ngmrf_samples = np.zeros((n_samples, N))\ncmrf_samples = np.zeros((n_samples, N))\n\n# Build D matrix\nd_mat = sp.eye(N)\nd_mat.setdiag(-1, k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\nfig, axs = plt.subplots(n_samples, 2, figsize=(20,20))\n\nfor j in range(n_samples):\n\n    # Draw normal increments\n    z = np.random.normal(size=N)\n\n    # Get GMRF sample\n    prec_mat = d_mat.T @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n    factor = cholesky(prec_mat, ordering_method='natural')\n    gmrf_samples[j,:] = factor.solve_Lt(z, use_LDLt_decomposition=False)\n\n    # Get CMRF sample\n    h_mat = gamma.rvs(1/2, scale=(s**2)/2, size=N)\n    h_mat = sp.diags(h_mat)\n    prec_mat = d_mat.T @  h_mat @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n    factor = cholesky(prec_mat, ordering_method='natural')\n    cmrf_samples[j,:] = factor.solve_Lt(z, use_LDLt_decomposition=False)\n\n    # Plot\n    axs[j,0].plot(gmrf_samples[j,:], color=\"#A5D75E\")\n    axs[j,1].plot(cmrf_samples[j,:], color=\"#A5D75E\")\n    axs[j,0].set_xticks([])\n    axs[j,1].set_xticks([])\n\naxs[0,0].set_title(\"GMRF Samples\")\naxs[0,1].set_title(\"CMRF Samples\")\n\nplt.show()\n\n\n\n\n\nFigure 8: 10 samples each from our GMRF/CMRF. Each row uses the same draw \\(z \\sim \\mathcal{N}(0,I)\\).\n\n\n\n\n\n\n3. Sampling a 2D GMRF/CMRF\nMaking the jump to a 2D GMRF is not too difficult. Whereas in our earlier examples for a 1D random walk with \\(N\\) points our precision matrix \\(Q\\) was \\(N \\times N\\), in 2D on an \\(M \\times N\\) grid our precision matrix will need to be an \\(M^2 \\times N^2\\) matrix. The 2D GMRF we will consider is an “anisotropic difference prior” that is an extension of our 1D GMRF from earlier, with \\(\\mu = 0\\) and precision \\[\\begin{equation*}\nQ = F^T F.\n\\end{equation*}\\] Here \\[\\begin{equation*}\nF = \\begin{bmatrix}\n           D_1 \\\\\n           D_2\n         \\end{bmatrix},\n\\end{equation*}\\] where the \\(D_{1/2}\\) are given by \\[\\begin{align*}\nD_1 &= I_N \\otimes D, \\\\\nD_2 &= D \\otimes I_M,\n\\end{align*}\\] and \\(D\\) is the same matrix from our 1D example.\n\n\nCode\nM, N = 5, 5\n\n# Build D matrices\nd_mat_vert = sp.eye(M)\nd_mat_vert.setdiag(-1,k=-1)\neye_horiz = sp.eye(N)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(N)\nd_mat_horiz.setdiag(-1,k=-1)\neye_vert = sp.eye(M)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\nd_mat = sp.vstack([d_mat_one, d_mat_two])\nprec_mat = d_mat.T @ d_mat\n\nfig, axs = plt.subplots(1,2,figsize=(10,10))\naxs[0].imshow(d_mat.toarray(), cmap='Greens_r')\naxs[0].set_title('F')\naxs[1].imshow(prec_mat.toarray(), cmap='Greens_r')\naxs[1].set_title('Q')\n\nplt.show()\n\n\n\n\n\nFigure 9: The matrices \\(F\\) and \\(Q\\) for a \\(20 \\times 20\\) lattice.\n\n\n\n\nNow that we have our precision matrix, we can sample the GMRF just as we did in the 1D case.\n\n\nCode\nM, N = 200, 200\n\n# Build D matrices\nd_mat_vert = sp.eye(M)\nd_mat_vert.setdiag(-1,k=-1)\neye_horiz = sp.eye(N)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(N)\nd_mat_horiz.setdiag(-1,k=-1)\neye_vert = sp.eye(M)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\nd_mat = sp.vstack([d_mat_one, d_mat_two])\n\n# Build precision\nprec_mat = d_mat.T @ d_mat\nprec_mat = sp.csc_matrix(prec_mat)\n\n# Get Cholesky factor\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n# Draw a sample\nnp.random.seed(0)\nz = np.random.normal(size=(M,N)).flatten()\nsample = factor.solve_Lt(z, use_LDLt_decomposition=False)\nsample = sample.reshape((M,N))\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nplt.figure(figsize=(8,8))\nax = plt.gca()\nim = ax.imshow(sample)\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\nplt.colorbar(im, cax=cax)\nplt.show()\n\n\n\n\n\nFigure 10: A sample of our GMRF on a \\(200 \\times 200\\) lattice.\n\n\n\n\nLet’s also make a “movie” for a sequence of samples.\n\nCode\nfrom moviepy.editor import ImageSequenceClip\n\nn_samples = 10\nsamples = np.zeros((n_samples, M, N))\n\n# Draw samples\nnp.random.seed(0)\nfor j in range(n_samples):\n    z = np.random.normal(size=(M,N)).flatten()\n    samples[j,:,:] = factor.solve_Lt(z, use_LDLt_decomposition=False).reshape((M,N))\n\nfps = int(0.1*n_samples)\nmin_val = np.amin(samples)\nmax_val = np.amax(samples)\n\nsamples -= min_val\nsamples /= max_val - min_val\n\ncolored_samples = np.zeros((n_samples, M, N, 4))\ncmap = plt.get_cmap()\nfor j in range(n_samples):\n    #print(cmap(samples[j,:,:]))\n    colored_samples[j,:,:,:] = cmap(samples[j,:,:])\n\ncolored_samples *= 255\n\nclip = ImageSequenceClip(list(colored_samples), fps=fps)\nclip.write_gif('gmrf_samples.gif', fps=fps, verbose=False, logger=None)\n\n\nNow that we can sample from a 2D GMRF, it is straightforward to add in hyper-parameters that turn our GMRF into a CMRF. To do this, we sample \\[\\begin{align*}\n\\lambda_1, \\ldots, \\lambda_{2N^2} &\\sim \\text{Gamma}(1/2, s^2/2) \\\\\nH \\, | \\, \\lambda_1, \\ldots, \\lambda_{2N^2} &= \\text{diag}(\\lambda_1, \\ldots, \\lambda_{2N^2}) \\\\\nx \\, | \\, H &\\sim \\mathcal{N}\\left(\\mu, Q^{-1} = \\left( F^T H F \\right)^{-1} \\right).\n\\end{align*}\\] Let’s try sampling this.\n\n\nCode\nfrom scipy.stats import gamma\n\nM, N = 200, 200\ns = 1.0\n\n# Build D matrices\nd_mat_vert = sp.eye(M)\nd_mat_vert.setdiag(-1,k=-1)\neye_horiz = sp.eye(N)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(N)\nd_mat_horiz.setdiag(-1,k=-1)\neye_vert = sp.eye(M)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\nd_mat = sp.vstack([d_mat_one, d_mat_two])\n\n# Sample hyperparamters\nh_mat = gamma.rvs(1/2, scale=1/((s**2)/2), size=2*M*N)\nh_mat = sp.diags(h_mat)\n\n# Build precision\nprec_mat = d_mat.T @ h_mat @ d_mat\nprec_mat = sp.csc_matrix(prec_mat)\n\n# Get Cholesky factor\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n# Draw a sample\nnp.random.seed(0)\nz = np.random.normal(size=(M,N)).flatten()\nsample = factor.solve_Lt(z, use_LDLt_decomposition=False)\nsample = sample.reshape((M,N))\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nplt.figure(figsize=(8,8))\nax = plt.gca()\nim = ax.imshow(sample)\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\nplt.colorbar(im, cax=cax)\nplt.show()\n\n\n\n\n\nFigure 11: A sample of our CMRF on a \\(200 \\times 200\\) lattice.\n\n\n\n\nLet’s make another movie while we’re at it.\n\nCode\nfrom moviepy.editor import ImageSequenceClip\n\nn_samples = 10\nsamples = np.zeros((n_samples, M, N))\n\n# Draw samples\nnp.random.seed(0)\nfor j in range(n_samples):\n\n    # Sample hyperparamters\n    h_mat = gamma.rvs(1/2, scale=1/((s**2)/2), size=2*M*N)\n    h_mat = sp.diags(h_mat)\n\n    # Build precision\n    prec_mat = d_mat.T @ h_mat @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n\n    # Get Cholesky factor\n    factor = cholesky(prec_mat, ordering_method='natural')\n\n    z = np.random.normal(size=(M,N)).flatten()\n    samples[j,:,:] = factor.solve_Lt(z, use_LDLt_decomposition=False).reshape((M,N))\n\nfps = int(0.1*n_samples)\nmin_val = np.amin(samples)\nmax_val = np.amax(samples)\n\nsamples -= min_val\nsamples /= max_val - min_val\n\ncolored_samples = np.zeros((n_samples, M, N, 4))\ncmap = plt.get_cmap()\nfor j in range(n_samples):\n    #print(cmap(samples[j,:,:]))\n    colored_samples[j,:,:,:] = cmap(samples[j,:,:])\n\ncolored_samples *= 255\n\nclip = ImageSequenceClip(list(colored_samples), fps=fps)\nclip.write_gif('cmrf_samples.gif', fps=fps, verbose=False, logger=None)\n\n\nSo we have achieved our goal. In a future post, we will put this to work as a prior in an inverse problem.\n\n\nBonus: some extra MRFs\nWhile we used a scale mixture of normal representation for the Cauchy distribution, we can also obtain other distributions as scale mixtures of normals. For example, we can simulate a MRF where the increments follow a Laplace distribution by sampling \\[\\begin{align*}\n\\beta_1, \\ldots, \\beta_{2N^2} &\\sim \\text{Exponential} \\\\\n\\lambda_i \\, | \\, \\beta_i &= \\frac{1}{2\\beta_i} \\\\\nH \\, | \\, \\lambda_1, \\ldots, \\lambda_{2N^2} &= \\text{diag}(\\lambda_1, \\ldots, \\lambda_{2N^2}) \\\\\nx \\, | \\, H &\\sim \\mathcal{N}\\left(\\mu, Q^{-1} = \\left( F^T H F \\right)^{-1} \\right).\n\\end{align*}\\]\nSimilarly, we can get a MRF increments with Horshoe-distributed [4] increments by sampling \\[\\begin{align*}\n\\beta_1, \\ldots, \\beta_{2N^2} &\\sim \\text{C}^+(0, 1) \\\\\n\\lambda_i \\, | \\, \\beta_i, \\tau &= \\frac{1}{\\beta_i^2 \\tau^2} \\\\\nH \\, | \\, \\lambda_1, \\ldots, \\lambda_{2N^2} &= \\text{diag}(\\lambda_1, \\ldots, \\lambda_{2N^2}) \\\\\nx \\, | \\, H &\\sim \\mathcal{N}\\left(\\mu, Q^{-1} = \\left( F^T H F \\right)^{-1} \\right)\n\\end{align*}\\] where \\(\\tau\\) is some fixed hyper-hyper-parameter and \\(C^+\\) denotes the half-Cauchy distribution. Below are some movies of samples from a Laplace MRF and Horshoe MRF, respectively.\n\nCode\nfrom moviepy.editor import ImageSequenceClip\n\nn_samples = 10\nsamples = np.zeros((n_samples, M, N))\n\n# Draw samples\nnp.random.seed(0)\nfor j in range(n_samples):\n\n    # Sample hyperparamters\n    h_mat = (1/(2*np.random.exponential(size=2*M*N)))\n    h_mat = sp.diags(h_mat)\n\n    # Build precision\n    prec_mat = d_mat.T @ h_mat @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n\n    # Get Cholesky factor\n    factor = cholesky(prec_mat, ordering_method='natural')\n\n    z = np.random.normal(size=(M,N)).flatten()\n    samples[j,:,:] = factor.solve_Lt(z, use_LDLt_decomposition=False).reshape((M,N))\n\nfps = int(0.1*n_samples)\nmin_val = np.amin(samples)\nmax_val = np.amax(samples)\n\nsamples -= min_val\nsamples /= max_val - min_val\n\ncolored_samples = np.zeros((n_samples, M, N, 4))\ncmap = plt.get_cmap()\nfor j in range(n_samples):\n    #print(cmap(samples[j,:,:]))\n    colored_samples[j,:,:,:] = cmap(samples[j,:,:])\n\ncolored_samples *= 255\n\nclip = ImageSequenceClip(list(colored_samples), fps=fps)\nclip.write_gif('lmrf_samples.gif', fps=fps, verbose=False, logger=None)\n\n\nfrom scipy.stats import halfcauchy\n\ntau = 1.0\n\nn_samples = 10\nsamples = np.zeros((n_samples, M, N))\n\n# Draw samples\nnp.random.seed(0)\nfor j in range(n_samples):\n\n    # Sample hyperparamters\n    h_mat = (  1/(  (tau*halfcauchy.rvs(size=2*M*N))**2)  )\n    h_mat = sp.diags(h_mat)\n\n    # Build precision\n    prec_mat = d_mat.T @ h_mat @ d_mat\n    prec_mat = sp.csc_matrix(prec_mat)\n\n    # Get Cholesky factor\n    factor = cholesky(prec_mat, ordering_method='natural')\n\n    z = np.random.normal(size=(M,N)).flatten()\n    samples[j,:,:] = factor.solve_Lt(z, use_LDLt_decomposition=False).reshape((M,N))\n\nfps = int(0.1*n_samples)\nmin_val = np.amin(samples)\nmax_val = np.amax(samples)\n\nsamples -= min_val\nsamples /= max_val - min_val\n\ncolored_samples = np.zeros((n_samples, M, N, 4))\ncmap = plt.get_cmap()\nfor j in range(n_samples):\n    #print(cmap(samples[j,:,:]))\n    colored_samples[j,:,:,:] = cmap(samples[j,:,:])\n\ncolored_samples *= 255\n\nclip = ImageSequenceClip(list(colored_samples), fps=fps)\nclip.write_gif('hmrf_samples.gif', fps=fps, verbose=False, logger=None)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n[1] Chada, N. K., Roininen, L. and Suuronen, J. (2021). Cauchy markov random field priors for bayesian inversion.\n\n\n[2] Markkanen, M., Roininen, L., Huttunen, J. M.  J. and Lasanen, S. (2019). Cauchy difference priors for edge-preserving bayesian inversion. Journal of Inverse and Ill-posed Problems 27 225–40.\n\n\n[3] Rue, H. and Held, L. (2005). Gaussian markov random fields: Theory and applications.\n\n\n[4] Carvalho, C. M., Polson, N. G. and Scott, J. G. (2009). Handling sparsity via the horseshoe. In AISTATS.\n\nFootnotes\n\n\nNOT using a Markov Chain Monte Carlo (MCMC) method.↩︎\nSee Chapter 4 of [3], also here↩︎"
  },
  {
    "objectID": "posts/hierarchical_cmrf.html#section",
    "href": "posts/hierarchical_cmrf.html#section",
    "title": "Sampling a Cauchy Markov Random Field",
    "section": "",
    "text": "Q^{-1} = (LLT){-1} = L^{-T} L^{-1} ##\n\n\nCode\n# Build precision\nd_mat = sp.eye(N)\nd_mat.setdiag(-1,k=-1)\nd_mat = sp.csr_matrix(d_mat)\n\nprec_mat = d_mat.T @ d_mat\nfactor = cholesky(prec_mat, ordering_method='natural')\nfactor.cholesky_inplace(prec_mat)\n\nplt.imshow(prec_mat.toarray())\nplt.show()\n\n\n\n\n\n\nCode\nL = factor.L()\nL = L.toarray()\nLtinv = np.linalg.inv(L).T\n\n\n\nCode\nplt.imshow(L)\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.imshow(Ltinv)\nplt.show()\n\n\n\n\n\n\nCode\n# Build precision\nprec_mat = d_mat.T @ d_mat\n\n# Compute (sparse) cholesky factorization\n# The natural ordering method doesn't permute the nodes (our precision is already sparse)\nfactor = cholesky(prec_mat, ordering_method='natural')\n\n\nCode\ninv_mat = factor.inv().toarray()\n\n\n\nCode\ncov_mat\n\n\narray([[1., 1., 1., 1., 1.],\n       [1., 2., 2., 2., 2.],\n       [1., 2., 3., 3., 3.],\n       [1., 2., 3., 4., 4.],\n       [1., 2., 3., 4., 5.]])\n\n\n\n\nCode\ninv_mat\n\n\narray([[1., 1., 1., 1., 1.],\n       [1., 2., 2., 2., 2.],\n       [1., 2., 3., 3., 3.],\n       [1., 2., 3., 4., 4.],\n       [1., 2., 3., 4., 5.]])\n\n\n\nCode\n# #| label: fig-random_walk_prec_cov\n# #| fig-cap: Precision and covariance matrices for the random walk.\n\n# N = 10\n# d_mat = sp.eye(N)\n# d_mat.setdiag(-1,k=-1)\n# d_mat = sp.csr_matrix(d_mat)\n\n# prec_mat = d_mat.T @ d_mat\n# prec_mat = prec_mat.toarray()\n# cov_mat = np.linalg.inv(prec_mat)\n\n# fig, axs = plt.subplots(1, 2, figsize=(10,5))\n# axs[0].imshow(prec_mat, cmap='Greens_r')\n# axs[0].set_title(\"Precision\")\n# axs[1].imshow(cov_mat, cmap='Greens_r')\n# axs[1].set_title(\"Covariance\")\n\n# plt.show()\n\n\n\nCode\n# #| label: fig-random_walk_draw\n# #| fig-cap: A single draw from a 1D GMRF.\n\n# np.random.seed(0)\n# chol_fac = np.linalg.cholesky(cov_mat)\n# z = np.random.normal(size=N)\n# sample = chol_fac @ z\n\n# fig, axs = plt.subplots(figsize=(10,5))\n# axs.plot(sample, \"o-\", color=\"#A5D75E\")\n# #axs.set_title(\"A single draw from the random walk on the line\")\n# plt.show()\n\n\n\n\n\nThese examples with the number of points \\(N = 10\\) worked fine. Now let’s do a naive experiment where we time how long it takes to compute a single sample as we increase \\(N\\).\n\n\nCode\nimport time\nfrom sklearn import datasets, linear_model\n\nns = np.logspace(2.5, 4, num=50)\nns = np.rint(ns).astype(np.int32)\n\ndef draw_single_sample(n):\n    \"\"\"Draws a single sample from the RW model of size n.\n    \"\"\"\n    d_mat = sp.eye(n)\n    d_mat.setdiag(-1,k=-1)\n\n    prec_mat = d_mat.T @ d_mat\n    prec_mat = prec_mat.toarray()\n    cov_mat = np.linalg.inv(prec_mat)\n\n    chol_fac = np.linalg.cholesky(cov_mat)\n    z = np.random.normal(size=n)\n    sample = chol_fac @ z\n    \n    return None\n\n\ntimes = []\nfor n in ns:\n    start = time.time()\n    draw_single_sample(n)\n    end = time.time()\n    elapsed = end - start\n    times.append(elapsed)\n\nregr = linear_model.LinearRegression()\nregr.fit(np.log(ns)[:, None], np.log(times)[:, None])\nslope = regr.coef_[0][0]\n\nfig, axs = plt.subplots(figsize=(10,5))\naxs.loglog(ns, times, \"o-\", color=\"#A5D75E\")\naxs.set_xlabel(\"N\")\naxs.set_ylabel(\"Time (seconds)\")\naxs.set_title(f\"Time to generate one sample, increasing N (slope = {slope:.1f})\")\nplt.show()\n\n\n\n\n\nFigure 5: Time to generate a single sample vs. number of points N.\n\n\n\n\nThe bottleneck we see here comes from the Cholesky factorization \\(Q = L L^T\\).\n\nCode\nfrom scipy.stats import gamma\n\n\n\nCode\nN = 1000\neta = 100\n\nd_mat = sp.eye(N)\nd_mat.setdiag(-1,k=-1)\n\nweights = gamma.rvs(1/2, scale=(eta**2)/2, size=N)\nweights_diag = sp.diags(weights)\n\nprec_mat = d_mat.T @  weights_diag @ d_mat\nprec_mat = prec_mat.toarray()\ncov_mat = np.linalg.inv(prec_mat)\n\nfig, axs = plt.subplots(1, 2, figsize=(10,5))\naxs[0].imshow(prec_mat, cmap='Greens_r')\naxs[0].set_title(\"Precision\")\naxs[1].imshow(cov_mat, cmap='Greens_r')\naxs[1].set_title(\"Covariance\")\n\nplt.show()\n\n\n\n\n\n\n\nCode\nchol_fac = np.linalg.cholesky(cov_mat)\nz = np.random.normal(size=N)\nsample = chol_fac @ z\n\nplt.figure(figsize=(10,5))\nplt.plot(sample)\nplt.show()\n\n\n\n\n\n\nCode\nN = 4\neta = 10\n\nd_mat = sp.eye(N)\nd_mat.setdiag(-1,k=-1)\n\nweights = gamma.rvs(1/2, scale=(eta**2)/2, size=N)\nweights_diag = sp.diags(weights)\n\nprec_mat = d_mat.T @  weights_diag @ d_mat\nprec_mat = prec_mat.toarray()\ncov_mat = np.linalg.inv(prec_mat)\n\n\n\nCode\nd_mat.toarray()\n\n\narray([[ 1.,  0.,  0.,  0.],\n       [-1.,  1.,  0.,  0.],\n       [ 0., -1.,  1.,  0.],\n       [ 0.,  0., -1.,  1.]])\n\n\n\n\nCode\nd_mat.T.toarray()\n\n\narray([[ 1., -1.,  0.,  0.],\n       [ 0.,  1., -1.,  0.],\n       [ 0.,  0.,  1., -1.],\n       [ 0.,  0.,  0.,  1.]])\n\n\n\n\nCode\n(d_mat.T @ d_mat).toarray()\n\n\narray([[ 2., -1.,  0.,  0.],\n       [-1.,  2., -1.,  0.],\n       [ 0., -1.,  2., -1.],\n       [ 0.,  0., -1.,  1.]])\n\n\n\n\nCode\nweights_diag.toarray()\n\n\narray([[ 7.55762179,  0.        ,  0.        ,  0.        ],\n       [ 0.        , 22.33463962,  0.        ,  0.        ],\n       [ 0.        ,  0.        , 63.03078966,  0.        ],\n       [ 0.        ,  0.        ,  0.        , 12.59011706]])"
  },
  {
    "objectID": "posts/simple_image_deblurring.html",
    "href": "posts/simple_image_deblurring.html",
    "title": "A Simple Image De-blurring Example",
    "section": "",
    "text": "Problem Setup\nWe will start by converting the following 3024 x 4032 pixel image of the Grand Canyon\n\nto grayscale to get\n\nand then apply a Gaussian blur to get\n\n\nCode\nimport numpy as np\nfrom scipy.ndimage import gaussian_filter\nfrom PIL import Image, ImageOps\n\n# Load image\nimg = Image.open(\"grand_canyon.jpg\")\nimg = ImageOps.grayscale(img)\nimg = np.asarray(img)\nimg = img/255\nM, N = img.shape\n\nim = Image.fromarray(255*img)\nim = im.convert('RGB')\nim.save(\"grayscale_grand_canyon.jpg\")\n\n# Define blur operator using sigma\nblur_sigma = 20.0\nblur_op = lambda x: gaussian_filter(x, blur_sigma, mode='wrap')\n\n# Apply blur and save\nblurred_img = blur_op(img)\nim = Image.fromarray(255*blurred_img)\nim = im.convert('RGB')\nim.save(\"blurred_grand_canyon.jpg\")\n\n\n\nBCCB Blurring\nLet’s describe what we just did. We began with an initial image \\(x\\), and then applied a Gaussian blurring operator \\(A\\) to obtain the observation\n\\[\ny = A x.\n\\]\nNote that the blurring is a linear operator and thus can be represented by a matrix \\(A\\), even though we never actually form the matrix \\(A\\). Note that we have also used the mode = 'wrap' option, which specifies a periodic boundary condition for the blurring. We choose this because this choice makes \\(A\\) a block circulant with circulant blocks (BCCB) matrix, which have several nice properties. One such property is that we can diagonalize the matrix \\(A\\) via the two-dimensional discrete Fourier transform, i.e.,\n\\[\nA = F^H  \\Lambda  F\n\\]\nwhere \\(\\Lambda\\) is a diagonal matrix. This makes solving systems involving \\(A\\) fairly straightforward and cheap to do.\nIf we want to de-blur the image \\(y\\), one thing we might try is to do\n\\[\nx = A^{-1}  y.\n\\]\nFrom our diagonalization and the fact that \\(F^{-1} = F^H\\), we can compute this as\n\\[\n\\begin{align*}\nx &= \\left( F^H  \\Lambda F \\right)^{-1} y \\\\\n&= F^H  \\Lambda^{-1}  F y.\n\\end{align*}\n\\]\nBut what is \\(\\Lambda^{-1}\\)? We can compute this by picking a random vector \\(v\\), and doing elementwise division between \\(v\\) and \\(F A F^H v\\). Let’s do this, and then use the result to compute \\(x = A^{-1} y\\).\n\n\nCode\n# Draw a random image\nrand_img = np.random.randn(M,N)\nmat_prod_rand_img = np.fft.fft2(blur_op(np.fft.ifft2(rand_img, norm='ortho')), norm='ortho')\nlam = mat_prod_rand_img/rand_img\nlam = np.real(lam) # We know it should be real\nlam_inv = 1/lam\n\n# Now use to invert, and save result\nx_approx = np.fft.ifft2(lam_inv*np.fft.fft2(blurred_img, norm='ortho'), norm='ortho')\nx_approx = np.real(x_approx)\n\nim = Image.fromarray(255*x_approx)\nim = im.convert('RGB')\nim.save(\"deblurred_grand_canyon_direct_inversion.jpg\")\n\n\nSo that actually worked out pretty well. In reality, our image would likely be contaminated by at least some small amount of noise. Let’s see what happens if we apply our same inversion procedure to\n\\[\n\\begin{align*}\ny &= A x + \\varepsilon, \\\\\n\\varepsilon &\\sim \\mathcal{N}\\left(0, \\sigma^2 I \\right)\n\\end{align*}\n\\]\nwith a comically small noise standard deviation of \\(\\sigma = 0.000000001\\). Our new observation is\n\nwhich is visually indistinguishable from our previous one with no noise added to the blurred image. However, if we compute \\(x = A^{-1} y\\) in this case we obtain\n\nwhich is nonsense.\n\nCode\nnoise_sigma = 0.000000001\n\n# Add a little bit of noise and save\nnp.random.seed(0) # Set seed\nnoisy_blurred_img = blur_op(img) + noise_sigma*np.random.randn(M,N) \n\nim = Image.fromarray(255*noisy_blurred_img)\nim = im.convert('RGB')\nim.save(\"noisy_blurred_grand_canyon.jpg\")\n\n# Now invert using the same method as before and save result\nx_approx = np.fft.ifft2(lam_inv*np.fft.fft2(noisy_blurred_img, norm='ortho'), norm='ortho')\nx_approx = np.real(x_approx)\n\nim = Image.fromarray(255*x_approx)\nim = im.convert('RGB')\nim.save(\"noisy_deblurred_grand_canyon_direct_inversion.jpg\")\n\nThe reason this happens is because although our blurring operator \\(A\\) is invertible, it is extremely ill-conditioned and thus will yield unexpected results such as this. Note that the diagonal entries of \\(\\Lambda\\) are in fact the eigenvalues of \\(A\\), so examining these in Figure 1 can give us a sense of why the matrix is ill-conditioned.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.facecolor'] = '#0F2537'\nplt.rcParams['text.color'] = 'white'\nplt.rcParams['axes.labelcolor'] = 'white'\nplt.rcParams['xtick.color'] = 'white'\nplt.rcParams['ytick.color'] = 'white'\n\n# Compute eigvalues in descending order\neigvals = np.flip(np.asarray(sorted(lam.flatten())))\n\n# Plot\nplt.figure(figsize=(13,8))\nplt.semilogx(eigvals, color='#A5D75E')\nplt.xlabel(\"nth eigenvalue\")\nplt.ylabel(\"eigenvalue\")\nplt.show()\n\n\n\n\n\nFigure 1: The eigenvalues of \\(A\\) in descending order.\n\n\n\n\nThe condition number of \\(A\\) is equal to the ratio of the largest and smallest eigenvalues. The fact that the smallest eigenvalue is near zero and the largest is near one gives \\(A\\) a very large condition number and thus \\(A\\) is very ill-conditioned. In other words, recovering \\(x\\) from \\(y\\) in this manner is a very ill-posed problem.\n\n\nRegularized Inversion\nIn the case that our blurred image contains any amount of noise, we clearly cannot settle for simply computing \\(x = A^{-1} y\\). One way to tackle this problem is to instead look for the solution of\n\\[\nx^\\star = \\text{argmin}_x \\,\\, \\| A x - y \\|_2^2 + \\mathcal{R}(x)\n\\tag{1}\\]\nfor some regularization term \\(\\mathcal{R}(x)\\). Ignoring \\(\\mathcal{R}(x)\\), note that if we found an \\(x\\) such that \\(A x = y\\) then the objective would be equal to zero and \\(x = x^\\star\\). However, we have already seen that this solution would do us no good. This explains the role of \\(\\mathcal{R}(x)\\): to make the problem less ill-posed, i.e. well-posed. For now we will consider the choice\n\\[\n\\mathcal{R}(x) = \\gamma \\| L x \\|_2^2 = \\gamma x^T L^T L x.\n\\]\nwhere \\(L\\) encodes a discrete negative-Laplacian operator. This regularization term promotes smooth solutions for the reconstructed image \\(x\\), and the degree of this smoothness is controlled by the parameter \\(\\gamma > 0\\). For convenience, we will define the discrete gradient \\(L\\) to use periodic boundary conditions which similarly to our blurring matrix will make \\(L\\) a BCCB matrix that can be diagonalized by the discrete Fourier transform as\n\\[\nL = F^H \\Pi F\n\\]\nfor some diagonal matrix \\(\\Pi\\).\nNote that with this choice of \\(\\mathcal{R}(x)\\), if we set the gradient of the objective function in (Equation 1) equal to zero we obtain the equation\n\\[\n2 A^T(Ax - y) + 2 \\gamma L^T L x = 0\n\\]\nfor the minimizer, which is solved by\n\\[\nx = \\left( A^T A + \\gamma L^T L \\right)^{-1} A^T y\n\\]\nThis is where our choice of BCCB matrices throughout will pay off. While it looks like we will need to solve a sparse linear system, we can actually compute the solution using only FFTs and elementwise operations thanks for our diagonalization. Inserting \\(A = F^H \\Lambda F\\) and \\(L = F^H \\Pi F\\), we have\n\\[\n\\begin{align*}\nx &= \\left( A^T A + \\gamma L^T L \\right)^{-1} A^T y \\\\\n&= \\left( F^H \\Lambda F F^H \\Lambda F + \\gamma F^H \\Pi F F^H \\Pi F \\right)^{-1} F^H \\Lambda F y \\\\\n&= \\left( F^H \\Lambda^2 F + \\gamma F^H \\Pi^2 F \\right)^{-1} F^H \\Lambda F y \\\\\n&= \\left( F^H \\left( \\Lambda^2 + \\gamma \\Pi^2  \\right) F \\right)^{-1} F^H \\Lambda F y \\\\\n&= F^H \\left( \\Lambda^2 + \\gamma \\Pi^2  \\right)^{-1} F F^H \\Lambda F y \\\\\n&= F^H \\left( \\Lambda^2 + \\gamma \\Pi^2  \\right)^{-1} \\Lambda F y.\n\\end{align*}\n\\]\nNote that the only matrix inverse remaining is that of a diagonal matrix, so we can easily compute the solution. Let’s try this out.\n\nCode\n%%capture\n\nimport scipy.sparse as sp\n\n# First we need to construct our L matrix\n\n# Construct our L matrix\nd_mat_vert = sp.eye(N)\nd_mat_vert.setdiag(-1,k=-1)\nd_mat_vert = sp.csc_matrix(d_mat_vert)\nd_mat_vert[0,-1] = -1 # For periodic BCs\neye_horiz = sp.eye(M)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(M)\nd_mat_horiz.setdiag(-1,k=-1)\nd_mat_horiz = sp.csc_matrix(d_mat_horiz)\nd_mat_horiz[0,-1] = -1 # For periodic BCs\neye_vert = sp.eye(N)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\nd_mat = sp.vstack([d_mat_one, d_mat_two])\nL_mat = d_mat.T @ d_mat\n\n# Next, we need to compute $\\Pi$ like we did earlier with $\\Lambda$.\nrand_img = np.random.randn(M,N)\nmat_prod_rand_img = np.fft.fft2( (L_mat @ np.fft.ifft2(rand_img, norm='ortho').flatten() ).reshape((M,N)), norm='ortho')\npi = mat_prod_rand_img/rand_img\npi = np.real(pi) # We know it should be real\npi_inv = 1/pi\n\n# Now compute the regularized solution\ngamma = 1/N\n\nsoln = np.fft.fft2(noisy_blurred_img, norm='ortho')\nsoln *= lam\nsoln *= 1/( (lam**2) + (gamma*(pi**2)) )\nsoln = np.fft.ifft2(soln, norm='ortho')\nsoln = np.real(soln)\n\n# Save the solution\nim = Image.fromarray(255*soln)\nim = im.convert('RGB')\nim.save(\"regularized_deblurred_grand_canyon.jpg\")\n\n\nThis is much better than what we got by instead computing \\(x = A^{-1} y\\). In the next post, we’ll use CuPy to approach this same problem but with a different form of the regularizer \\(\\mathcal{R}(x)\\)."
  }
]