[
  {
    "objectID": "posts/simple_image_deblurring.html",
    "href": "posts/simple_image_deblurring.html",
    "title": "A Simple Image De-blurring Example",
    "section": "",
    "text": "The goal of this post is to write some code for solving a simple image de-blurring problem.\n\nProblem Setup\nWe will start by converting the following 3024 x 4032 pixel image of the Grand Canyon\n\nto grayscale to get\n\nand then apply a Gaussian blur to get\n\n\nimport numpy as np\nfrom scipy.ndimage import gaussian_filter\nfrom PIL import Image, ImageOps\n\n# Load image\nimg = Image.open(\"grand_canyon.jpg\")\nimg = ImageOps.grayscale(img)\nimg = np.asarray(img)\nimg = img/255\nM, N = img.shape\n\nim = Image.fromarray(255*img)\nim = im.convert('RGB')\nim.save(\"grayscale_grand_canyon.jpg\")\n\n# Define blur operator using sigma\nblur_sigma = 20.0\nblur_op = lambda x: gaussian_filter(x, blur_sigma, mode='wrap')\n\n# Apply blur and save\nblurred_img = blur_op(img)\nim = Image.fromarray(255*blurred_img)\nim = im.convert('RGB')\nim.save(\"blurred_grand_canyon.jpg\")\n\n\n\nBCCB Blurring\nLet’s describe what we just did. We began with an initial image \\(x\\), and then applied a Gaussian blurring operator \\(A\\) to obtain the observation \\[\ny = A x.\n\\] Note that the blurring is a linear operator and thus can be represented by a matrix \\(A\\), even though we never actually form the matrix \\(A\\). Note that we have also used the mode = 'wrap' option, which specifies a periodic boundary condition for the blurring. We choose this because this choice makes \\(A\\) a block circulant with circulant blocks (BCCB) matrix, which have several nice properties (see [1] for a nice overview of structured matrices and image de-blurring). One such property is that we can diagonalize the matrix \\(A\\) via the two-dimensional discrete Fourier transform, i.e., \\[\nA = F^H  \\Lambda  F\n\\] where \\(\\Lambda\\) is a diagonal matrix. This makes solving systems involving \\(A\\) fairly straightforward and cheap to do.\nIf we want to de-blur the image \\(y\\), one thing we might try is to do \\[\nx = A^{-1}  y.\n\\] From our diagonalization and the fact that \\(F^{-1} = F^H\\), we can compute this as \\[\n\\begin{align*}\nx &= \\left( F^H  \\Lambda F \\right)^{-1} y \\\\\n&= F^H  \\Lambda^{-1}  F y.\n\\end{align*}\n\\] But what is \\(\\Lambda^{-1}\\)? We can compute this by picking a random vector \\(v\\), and doing elementwise division between \\(v\\) and \\(F A F^H v\\). Let’s do this, and then use the result to compute \\(x = A^{-1} y\\).\n\n\n# Draw a random image\nrand_img = np.random.randn(M,N)\nmat_prod_rand_img = np.fft.fft2(blur_op(np.fft.ifft2(rand_img, norm='ortho')), norm='ortho')\nlam = mat_prod_rand_img/rand_img\nlam = np.real(lam) # We know it should be real\nlam_inv = 1/lam\n\n# Now use to invert, and save result\nx_approx = np.fft.ifft2(lam_inv*np.fft.fft2(blurred_img, norm='ortho'), norm='ortho')\nx_approx = np.real(x_approx)\n\nim = Image.fromarray(255*x_approx)\nim = im.convert('RGB')\nim.save(\"deblurred_grand_canyon_direct_inversion.jpg\")\n\n\nSo that actually worked out pretty well. In reality, our image would likely be contaminated by at least some small amount of noise. Let’s see what happens if we apply our same inversion procedure to \\[\n\\begin{align*}\ny &= A x + \\varepsilon, \\\\\n\\varepsilon &\\sim \\mathcal{N}\\left(0, \\sigma^2 I \\right)\n\\end{align*}\n\\] with a comically small noise standard deviation of \\(\\sigma = 0.000000001\\). Our new observation is\n\nwhich is visually indistinguishable from our previous one with no noise added to the blurred image. However, if we compute \\(x = A^{-1} y\\) in this case we obtain\n\nwhich is nonsense.\n\nnoise_sigma = 0.000000001\n\n# Add a little bit of noise and save\nnp.random.seed(0) # Set seed\nnoisy_blurred_img = blur_op(img) + noise_sigma*np.random.randn(M,N) \n\nim = Image.fromarray(255*noisy_blurred_img)\nim = im.convert('RGB')\nim.save(\"noisy_blurred_grand_canyon.jpg\")\n\n# Now invert using the same method as before and save result\nx_approx = np.fft.ifft2(lam_inv*np.fft.fft2(noisy_blurred_img, norm='ortho'), norm='ortho')\nx_approx = np.real(x_approx)\n\nim = Image.fromarray(255*x_approx)\nim = im.convert('RGB')\nim.save(\"noisy_deblurred_grand_canyon_direct_inversion.jpg\")\n\nThe reason this happens is because although our blurring operator \\(A\\) is invertible, it is extremely ill-conditioned and thus will yield unexpected results such as this. Note that the diagonal entries of \\(\\Lambda\\) are in fact the eigenvalues of \\(A\\), so examining these in Figure 1 can give us a sense of why the matrix is ill-conditioned.\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.facecolor'] = '#0F2537'\nplt.rcParams['text.color'] = 'white'\nplt.rcParams['axes.labelcolor'] = 'white'\nplt.rcParams['xtick.color'] = 'white'\nplt.rcParams['ytick.color'] = 'white'\n\n# Compute eigvalues in descending order\neigvals = np.flip(np.asarray(sorted(lam.flatten())))\n\n# Plot\nplt.figure(figsize=(13,8))\nplt.semilogx(eigvals, color='#A5D75E')\nplt.xlabel(\"nth eigenvalue\")\nplt.ylabel(\"eigenvalue\")\nplt.show()\n\n\n\n\n\n\n\nFigure 1: The eigenvalues of \\(A\\) in descending order.\n\n\n\n\n\nThe condition number of \\(A\\) is equal to the ratio of the largest and smallest eigenvalues. The fact that the smallest eigenvalue is near zero and the largest is near one gives \\(A\\) a very large condition number and thus \\(A\\) is very ill-conditioned. In other words, recovering \\(x\\) from \\(y\\) in this manner is a very ill-posed problem.\n\n\nRegularized Inversion\nIn the case that our blurred image contains any amount of noise, we clearly cannot settle for simply computing \\(x = A^{-1} y\\). One way to tackle this problem is to instead look for the solution of \\[\nx^\\star = \\text{argmin}_x \\,\\, \\| A x - y \\|_2^2 + \\mathcal{R}(x)\n\\tag{1}\\] for some regularization term \\(\\mathcal{R}(x)\\). Ignoring \\(\\mathcal{R}(x)\\), note that if we found an \\(x\\) such that \\(A x = y\\) then the objective would be equal to zero and \\(x = x^\\star\\). However, we have already seen that this solution would do us no good. This explains the role of \\(\\mathcal{R}(x)\\): to make the problem less ill-posed, i.e. well-posed. For now we will consider the choice \\[\n\\mathcal{R}(x) = \\gamma \\| L x \\|_2^2 = \\gamma x^T L^T L x.\n\\] where \\(L\\) encodes a discrete negative-Laplacian operator. This regularization term promotes smooth solutions for the reconstructed image \\(x\\), and the degree of this smoothness is controlled by the parameter \\(\\gamma &gt; 0\\). For convenience, we will define the discrete gradient \\(L\\) to use periodic boundary conditions which will make \\(L^T L\\) a BCCB matrix that can be diagonalized by the discrete Fourier transform as \\[\nL^T L = F^H \\Pi F\n\\] for some diagonal matrix \\(\\Pi\\).\nNote that with this choice of \\(\\mathcal{R}(x)\\), if we set the gradient of the objective function in (Equation 1) equal to zero we obtain the equation \\[\n2 A^T(Ax - y) + 2 \\gamma L^T L x = 0\n\\] for the minimizer, which is solved by \\[\nx = \\left( A^T A + \\gamma L^T L \\right)^{-1} A^T y\n\\] This is where our choice of BCCB matrices throughout will pay off. While it looks like we will need to solve a sparse linear system, we can actually compute the solution using only FFTs and elementwise operations thanks for our diagonalization. Inserting \\(A = F^H \\Lambda F\\) and \\(L = F^H \\Pi F\\), we have \\[\n\\begin{align*}\nx &= \\left( A^T A + \\gamma L^T L \\right)^{-1} A^T y \\\\\n&= \\left( F^H \\Lambda F F^H \\Lambda F + \\gamma F^H \\Pi F  \\right)^{-1} F^H \\Lambda F y \\\\\n&= \\left( F^H \\Lambda^2 F + \\gamma F^H \\Pi F \\right)^{-1} F^H \\Lambda F y \\\\\n&= \\left( F^H \\left( \\Lambda^2 + \\gamma \\Pi  \\right) F \\right)^{-1} F^H \\Lambda F y \\\\\n&= F^H \\left( \\Lambda^2 + \\gamma \\Pi  \\right)^{-1} F F^H \\Lambda F y \\\\\n&= F^H \\left( \\Lambda^2 + \\gamma \\Pi  \\right)^{-1} \\Lambda F y.\n\\end{align*}\n\\] Note that the only matrix inverse remaining is that of a diagonal matrix, so we can easily compute the solution. Let’s try this out.\n\n%%capture\n\nimport scipy.sparse as sp\n\n# First we need to construct our L matrix\n\n# Construct our L matrix\nd_mat_vert = sp.eye(N)\nd_mat_vert.setdiag(-1,k=-1)\nd_mat_vert = sp.csc_matrix(d_mat_vert)\nd_mat_vert[0,-1] = -1 # For periodic BCs\neye_horiz = sp.eye(M)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(M)\nd_mat_horiz.setdiag(-1,k=-1)\nd_mat_horiz = sp.csc_matrix(d_mat_horiz)\nd_mat_horiz[0,-1] = -1 # For periodic BCs\neye_vert = sp.eye(N)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\nd_mat = sp.vstack([d_mat_one, d_mat_two])\nL_mat = d_mat.T @ d_mat\n\n# Next, we need to compute $\\Pi$ like we did earlier with $\\Lambda$.\nrand_img = np.random.randn(M,N)\nmat_prod_rand_img = np.fft.fft2( (L_mat @ np.fft.ifft2(rand_img, norm='ortho').flatten() ).reshape((M,N)), norm='ortho')\npi = mat_prod_rand_img/rand_img\npi = np.real(pi) # We know it should be real\npi_inv = 1/pi\n\n# Now compute the regularized solution\ngamma = 0.00005*(1/M)\n\nsoln = np.fft.fft2(noisy_blurred_img, norm='ortho')\nsoln *= lam\nsoln *= 1/( (lam**2) + (gamma*(pi*2)) )\nsoln = np.fft.ifft2(soln, norm='ortho')\nsoln = np.real(soln)\n\n# Save the solution\nim = Image.fromarray(255*soln)\nim = im.convert('RGB')\nim.save(\"regularized_deblurred_grand_canyon.jpg\")\n\n\nThis is much better than what we got by instead computing \\(x = A^{-1} y\\). Note that I didn’t talk about how to pick \\(\\gamma\\), which in this case I tuned by hand.\nIn the next post, we’ll use CuPy to approach this same problem but with a different form of the regularizer \\(\\mathcal{R}(x)\\).\n\n\n\n\n\nReferences\n\n[1] Hansen, P. C., Nagy, J. G., O’Leary, D. P. and Miller, R. L. (2008). Deblurring images: Matrices, spectra and filtering."
  },
  {
    "objectID": "posts/auxiliary_magic.html",
    "href": "posts/auxiliary_magic.html",
    "title": "Auxiliary Magic",
    "section": "",
    "text": "Last time we looked at an image de-blurring problem, which we solved by finding \\[\nx^\\star = \\text{argmin}_x \\,\\, \\| A x - y \\|_2^2 + \\mathcal{R}(x)\n\\tag{1}\\] where the regularization term was \\[\n\\mathcal{R}(x) = \\gamma \\| L x \\|_2^2 = \\gamma x^T L^T L x.\n\\] In this post, our goal is to: - describe a hierarchical prior and a method that can give us a better image reconstruction, - walk through a “magic” trick that will speed up our method, - and look at using CuPy to accelerate our reconstruction using a GPU.\n\nProbabilistic Inverse Problems\nWhile the problem posed in Equation 1 is completely deterministic, we can actually think of it as haven arisen from a probabilistic model. Suppose that \\[\n\\begin{align*}\nx &\\sim \\mathcal{N}\\left( 0, \\left(\\gamma L^T L \\right)^{-1} \\right), \\\\\ny \\, | \\, x &\\sim \\mathcal{N}\\left( A x, I \\right).\n\\end{align*}\n\\] Then our corresponding density functions are \\[\n\\begin{align*}\n\\pi(x) &\\propto \\exp\\left\\{ - \\gamma x^T L^T L  x \\right\\}, \\\\\n\\pi(y \\, | \\, x) &\\propto \\exp\\left\\{ - \\| A x - y \\|_2^2 \\right\\},\n\\end{align*}\n\\] and by Bayes’ theorem the posterior density for \\(x \\, | \\, y\\) is given as \\[\n\\pi(x \\, | \\, y) \\propto \\exp\\left\\{ - \\| A x - y \\|_2^2 \\right\\} \\times \\exp\\left\\{ - \\gamma x^T L^T L  x \\right\\}.\n\\] The MAP estimate of \\(x\\) is then given as \\[\nx^\\star = \\text{argmax}_x \\,\\, \\pi(x \\, | \\, y),\n\\] which is equivalent to \\[\n\\begin{align*}\nx^\\star &= \\text{argmin}_x \\,\\, - \\log \\pi(x \\, | \\, y) \\\\\n&= \\text{argmin}_x \\,\\, \\gamma \\| L x \\|_2^2 = \\gamma x^T L^T L x,\n\\end{align*}\n\\] which is exactly Equation 1. The role of \\(\\mathcal{R}(x)\\) can then be seen as contributing a prior of \\[\n\\pi(x) \\propto \\exp\\left\\{ - \\mathcal{R}(x) \\right\\}\n\\] into the inference problem.\n\n\nA Hierarchical Prior\nOne reason it can be useful to think probabilistically is because we can motivate different choices of the regularizer \\(\\mathcal{R}(x)\\). If we pick \\[\nLx \\sim \\mathcal{N}\\left(0, \\frac{1}{\\gamma} I \\right)\n\\] as our prior (which corresponds to \\(\\mathcal{R}(x) = \\gamma x^T L^T L x\\)), then we are saying that we believe that the discrete gradients in our image are distributed according to zero-mean Gaussian with variance \\(\\gamma^{-1}\\). We can tweak the strength of the prior by adjusting \\(\\gamma\\) and in turn its influence on our reconstructed image, but note that the same \\(\\gamma\\) governs of the entire discrete gradient in the image. Thus we might think to introduce a hierarchical prior on the discrete gradient that could try to (loosely) capture the fact that in some regions in an image the discrete gradient will be much larger than it is elsewhere. Define the prior \\[\n\\begin{align*}\n\\beta^H_{i,j}, \\beta^V_{i,j} &\\sim \\Gamma \\left( c, d \\right), \\\\\nL x &\\sim \\mathcal{N} \\left(0, B_{\\beta} \\right),\n\\end{align*}\n\\tag{2}\\] which has density \\[\n\\pi(x, \\beta) = \\pi(x \\, | \\, \\beta) \\pi(\\beta) \\propto \\det \\left( B_{\\beta} \\right)^{1/2} \\exp\\left\\{ - x^T L^T B_{\\beta} L x  \\right\\} \\times \\pi(\\beta).\n\\] Here \\(\\left( \\cdot \\right)^{V/H}\\) represent the fact that we are assigining two different hyper-parameter to govern the gradient in each the vertical and horizontal directions, \\(\\Gamma\\left(c, d \\right)\\) represents the gamma density function, \\[\nB_{\\beta} = \\text{diag}\\left( \\beta^V_{1,1}, \\ldots, \\beta^V_{m,n}, \\beta^H_{1,1}, \\ldots, \\beta^H_{m,n} \\right),\n\\] and \\[\n\\pi(\\beta) \\propto \\left( \\prod_{i,j}^{mn} \\Gamma( \\beta_{i,j}^H | c, d) \\right) \\times \\left( \\prod_{i,j}^{mn} \\Gamma( \\beta_{i,j}^V | c, d) \\right)\n\\] meaning that all hyper-parameters are assumed to be independent of one another. The reason we use a \\(\\Gamma\\) distribution for the hyper-parameter is because it is a conjugate prior for a Gaussian, meaning that we can determine certain relevant conditional distributions analytically.\n\n\nThe New Posterior\nUsing our prior Equation 2, the full posterior in our original model (with an extra factor of \\(\\frac{1}{2}\\), as well as a noise precision parameter \\(\\alpha\\)) is given as \\[\n\\pi(x, \\beta \\, | \\, y) \\propto \\exp\\left\\{ - \\frac{\\alpha}{2} \\| A x - y \\|_2^2 \\right\\} \\times \\det \\left( B_{\\beta} \\right)^{1/2} \\exp\\left\\{ - \\frac{1}{2} x^T L^T B_{\\beta} L x  \\right\\} \\times \\pi(\\beta).\n\\tag{3}\\] In the sampling setting, a common approach that could be used to draw samples from this posterior is Gibbs sampling, which involves iteratively drawing from the conditionals of each variable in the density given all of the others. In this case, our conditionals would be \\[\n\\begin{align*}\n\\pi(x \\, | \\, \\beta, y) &\\propto \\exp\\left\\{- \\frac{\\alpha}{2}\\| A x - y \\|_2^2 \\right\\} \\times  \\exp\\left\\{ - x^T L^T B_{\\beta} L x  \\right\\}, \\\\\n\\pi(\\beta \\, | \\, x, y) &\\propto \\det \\left( B_{\\beta} \\right)^{1/2} \\exp\\left\\{ - x^T L^T B_{\\beta} L x  \\right\\} \\times \\pi(\\beta).\n\\end{align*}\n\\] Due to conjugacy, we can recognize the first conditional to be a Gaussian and the second to be a Gamma distribution. Specifically, we have \\(x \\, | \\, \\beta, y \\sim \\mathcal{N}\\left(\\mu_1, Q_1^{-1} \\right)\\) with \\[\n\\begin{align*}\nQ_1 &= \\frac{1}{\\alpha} A^T A + L^T B_{\\beta} L, \\\\\n\\mu_1 &= Q_1^{-1} \\left( \\frac{1}{\\alpha} A^T y \\right).\n\\end{align*}\n\\] For \\(\\beta \\, | \\, x, y\\) we have \\[\n\\begin{align*}\n\\beta_{i,j} = \\Gamma\\left( \\frac{1}{2} + c, \\frac{1}{2}[L x]_{i,j}^2 + d \\right),\n\\end{align*}\n\\] where how you deal with \\(\\beta^V\\)/\\(\\beta^H\\) becomes clear when you think about the shapes of these operations.\nRather than code a Gibbs sampling algorithm, we will consider the (BCD) of [1], which is essentially an optimization technique that iteratively draws from the mean of each the conditionals. We iterate \\[\n\\begin{align*}\nx^{k+1} &= \\mathbb{E}_{\\pi}\\left( x \\, | \\, \\beta^k, y \\right), \\\\\n\\beta^{k+1} &= \\mathbb{E}_{\\pi}\\left( \\beta \\, | \\, x^{k+1}, y \\right),\n\\end{align*}\n\\] until the solution converges. One thing to note is that we will generally not settle into a global minimum, since the posterior (with no restrictions on \\(c\\) and \\(d\\)) is not log-concave due to the use of a Gamma hyper-prior on the \\(\\beta_{i,j}\\).\n\n\nAn Inconvenience\nFor the purposes of this post, we will assume that \\(A\\) is a BCCB blurring matrix and that \\(L\\) is an approximation to the discrete gradient with periodic boundary conditions such that \\(L^T L\\) is also BCCB. These were the nice assumptions in the previous post that allowed us to reduce all of our linear system solves to FFTs/IFFs and diagonal matrix operations, as well as efficiently sample in the Fourier domain.\nNote that while we have an analytic expression for the distribution \\(x \\, | \\beta, y\\), we can no longer use our BCCB assumptions to avoid linear system solves. Letting \\[\n\\begin{align*}\nA &= F^H \\Lambda F, \\\\\nL^T L &= F^H \\Pi F,\n\\end{align*}\n\\] inserting this into \\(Q_1^{-1} = \\left( A^T A + L^T B_{\\beta} L \\right)^{-1}\\) we now get \\[\n\\begin{align*}\nQ_1^{-1} &= \\left( \\frac{1}{\\alpha} A^T A +  L^T B_{\\beta} L \\right)^{-1} \\\\\n&= \\left( \\frac{1}{\\alpha} F^H \\Lambda F F^H \\Lambda F +  L^T B_{\\beta} L \\right)^{-1} \\\\\n&= \\left( \\frac{1}{\\alpha} F^H \\Lambda^2 F +  L^T B_{\\beta} L \\right)^{-1}  \\\\\n\\end{align*}\n\\] where unlike before we are now stuck. The problem is that \\(B_{\\beta}\\) is “sandwiched” in-between \\(L^T\\) and \\(L\\), which prevents us from using the diagonalization of \\(L^T L\\). While it is not the end of the world to have to solve a linear system using a sparse solver or some other method, we still really would like to use the BCCB assumption to our advantage if we can.\n\n\nAn Auxiliary Variable Magic Trick\nIt turns out there is a way to get around this, via a “magic trick” of [2]. I also discuss this in this post on Gaussian posterior formulas. For the moment let us fix \\(B_{\\beta}\\) to be a constant matrix. Out of the blue, define a new random variable via \\[\n\\begin{align*}\n    u \\, | \\, x, y &\\sim \\mathcal{N}\\left( H x, Q^{-1}  \\right),\n\\end{align*}\n\\] where we are free to choose \\(H\\) and \\(Q\\) (as long as they define a valid distribution). Then, if we consider the joint density of \\(u, x \\, | \\, y\\) we have that \\[\n\\begin{align*}\n\\pi(u, x \\, | \\, y) &= \\pi(u \\, | \\, x, y) \\times \\pi(x \\, | \\, y) \\times \\pi(x) \\\\\n&\\propto \\exp\\left\\{ -\\frac{1}{2} \\left( H x - u \\right)^T Q \\left( H x - u \\right)  \\right\\} \\times \\exp\\left\\{ -\\frac{1}{2} x^T L^T B L x  \\right\\} \\times \\pi(x \\, | \\, y) \\\\\n&\\propto \\exp\\left\\{ -\\frac{1}{2} \\left[ \\left( H x - u \\right)^T Q \\left( H x - u \\right) + x^T L^T B L x  \\right] \\right\\} \\times \\pi(x \\, | \\, y) \\\\\n&\\propto \\exp\\left\\{ -\\frac{1}{2} \\left[ x^T H^T Q H x + u^T Q u - 2 x^T H^T Q u + x^T L^T B L x  \\right] \\right\\} \\times \\pi(x \\, | \\, y) \\\\\n&\\propto \\exp\\left\\{ -\\frac{1}{2} \\left[ x^T P x + u^T Q u - 2 x^T H^T Q u  \\right] \\right\\} \\times \\pi(x \\, | \\, y) \\\\\n\\end{align*}\n\\] where we have defined \\[\nP = H^T Q H + L^T B L.\n\\] We are free to choose \\(H\\) and \\(Q\\), so let’s make the magic choice \\[\n\\begin{align*}\nH &= L, \\\\\nQ &= \\frac{1}{\\lambda} I  - B.\n\\end{align*}\n\\] Here \\(\\lambda &gt; 0\\) is a constant we are free to choose, so long as \\(\\lambda &lt; \\frac{1}{\\| B \\|}\\) which makes \\(Q\\) positive semi-definite as is required of a valid precision matrix. Inserting this choice and conditioning on \\(u\\), we obtain \\[\nP = \\frac{1}{\\lambda} L^T L\n\\] and \\[\n\\begin{align*}\n\\pi(x \\, | \\, u,  y) &\\propto \\exp\\left\\{ -\\frac{1}{2} \\left[ \\frac{1}{\\lambda} x^T L^T L x - 2 x^T L^T Q u  \\right] \\right\\} \\times \\pi(x \\, | \\, y). \\\\\n\\end{align*}\n\\] Now, we define yet another random variable \\[\nv = Qu = \\left( \\frac{1}{\\lambda} I  - B \\right) u,\n\\] insert into the density above to get \\[\n\\begin{align*}\n\\pi(x \\, | \\, v,  y) &\\propto \\exp\\left\\{ -\\frac{1}{2} \\left[ \\frac{1}{\\lambda} x^T L^T L x - 2 x^T L^T v \\right] \\right\\} \\times \\pi(x \\, | \\, y), \\\\\n\\end{align*}\n\\] and finally complete the square in the exponential to finally arrive at \\[\n\\begin{align*}\n\\pi(x \\, | \\, v,  y) &\\propto \\exp\\left\\{ -\\frac{1}{2 \\lambda} \\| L x - \\lambda v  \\|_2^2  \\right\\} \\times \\pi(x \\, | \\, y), \\\\\n\\end{align*}\n\\] which is the density of the Gaussian \\(\\mathcal{N}\\left( \\mu_2, Q_2^{-1} \\right)\\) where \\[\n\\begin{align*}\nQ_2 &= \\frac{1}{\\lambda} L^T L + \\frac{1}{\\alpha} A^T A, \\\\\n\\mu_2 &= Q_2^{-1} \\left( \\frac{1}{\\alpha} A^T y +  L^T \\left( \\frac{1}{\\lambda} I \\right) \\left( \\lambda v \\right)  \\right) \\\\\n&= Q_2^{-1} \\left( \\frac{1}{\\alpha} A^T y +  L^T  v \\right).\n\\end{align*}\n\\] Note that we could do this since the missing term in the square was independent of \\(x\\). Now we have what we wanted: the matrix \\(B\\) has disappeared after conditioning on \\(v\\), and we can now take advantage of our BCCB assumption for this conditional. This is all only useful so long as the conditional distribution $v , | , x, y $ is also nice, which is the Gaussian \\(\\mathcal{N}\\left( \\mu_3, Q_3^{-1} \\right)\\) where \\[\n\\begin{align*}\nQ_3^{-1} &= \\frac{1}{\\lambda} I - B, \\\\\n\\mu_3 &= \\left( \\frac{1}{\\lambda} I - B \\right) L x,\n\\end{align*}\n\\] which is nice to work with.\n\n\nThe Augmented Hierarchical Model\nIt turns out that when we allow \\(B = B_{\\beta}\\) to vary with hyper-parameters, everything we just did stays exactly the same, except we must choose a new \\(\\lambda=  \\lambda(B_{\\beta})\\) such that \\(\\lambda &lt; \\frac{1}{\\| B_\\beta \\|}\\) for the current values of the hyper-parameters. We have already discussed the conditionals \\(x \\, | \\, v, \\beta, y\\) and \\(v \\, | \\, x, \\beta, y\\) which are the same as before, but it turns out that the conditional \\(\\beta \\, | \\, x, v, y\\) is not trivial. As far as I know, this conditional is no longer known directly. So is all of our work for nothing? Fortunately, no, due to a technique called partially collapsed Gibbs sampling (PCGS) [3]. Note that while we don’t know an analytic form for \\(\\beta \\, | \\, x, v, y\\), we do know the form of \\(\\beta \\, | \\, x, y\\) which is the same as it was in the model before we introduced \\(v\\). PCGS is designed to take advantage of this by allowing us to only need access to \\(\\beta \\, | \\, x, y\\), with the price being that the order in which we iterate through the conditionals now matters.\nThus our discussion leads us to a modified version of the BCD algorithm that uses an auxiliary variable to simplify the calculations for the update of each variable: \\[\n\\begin{align*}\nx^{k+1} &= \\mathbb{E}_{\\pi}\\left( x \\, | \\, v^k, \\beta^k, y \\right), \\\\\nv^{k+1} &= \\mathbb{E}_{\\pi}\\left( v \\, | \\, x^k, \\beta^k, y \\right), \\\\\n\\beta^{k+1} &= \\mathbb{E}_{\\pi}\\left( \\beta \\, | \\, x^{k+1}, y \\right).\n\\end{align*}\n\\] We will test this out by building off our code from my previous post on de-blurring.\n\n\n\nThe Code\nThe first thing we will do is some initial set-up for the problem such as setting up our differencing matrix and computing the diagonalizations of \\(A\\) and \\(L^T L\\). We will use the same blurred image from the previous post.\n\n%%capture\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.ndimage import gaussian_filter\nfrom PIL import Image, ImageOps\n\n\n# Load blurred image\nblurred_img = Image.open(\"blurred_grand_canyon.jpg\")\nblurred_img = ImageOps.grayscale(blurred_img)\nblurred_img = np.asarray(blurred_img)\nblurred_img = blurred_img/255\nM, N = blurred_img.shape\n\n\n# Define blur operator using sigma\nblur_sigma = 20.0\nblur_op = lambda x: gaussian_filter(x, blur_sigma, mode='wrap')\n\n\n# Compute diagonalization for the blurring operator\nrand_img = np.random.randn(M,N)\nmat_prod_rand_img = np.fft.fft2(blur_op(np.fft.ifft2(rand_img, norm='ortho')), norm='ortho')\nlam = mat_prod_rand_img/rand_img\nlam = np.real(lam) # We know it should be real\nlam_inv = 1/lam\n\n\n# Construct our differencing matrices\nd_mat_vert = sp.eye(N)\nd_mat_vert.setdiag(-1,k=-1)\nd_mat_vert = sp.csc_matrix(d_mat_vert)\nd_mat_vert[0,-1] = -1 # For periodic BCs\neye_horiz = sp.eye(M)\nd_mat_one = sp.kron(eye_horiz, d_mat_vert)\n\nd_mat_horiz = sp.eye(M)\nd_mat_horiz.setdiag(-1,k=-1)\nd_mat_horiz = sp.csc_matrix(d_mat_horiz)\nd_mat_horiz[0,-1] = -1 # For periodic BCs\neye_vert = sp.eye(N)\nd_mat_two = sp.kron(d_mat_horiz, eye_vert)\n\n\n# Make L, L^T, and L^T L\nL = sp.vstack([d_mat_one, d_mat_two])\nLt = L.T\nLtL = Lt @ L \n\n\n# Next, we need to compute $\\Pi$ like we did earlier with $\\Lambda$.\nrand_img = np.random.randn(M,N)\nmat_prod_rand_img = np.fft.fft2( (LtL @ np.fft.ifft2(rand_img, norm='ortho').flatten() ).reshape((M,N)), norm='ortho')\npi = mat_prod_rand_img/rand_img\npi = np.real(pi) # We know it should be real\npi_inv = 1/pi\n\nIn order to quickly perform the FFTs/IFFTs needed for the algorithm, we will leverage a GPU via CuPy. I had previously tried doing this algorithm on CPUs with just NumPy alone and found that it took about one iteration per second, whereas with CuPy we can get about 500 iterations per 30 seconds (about 17 iterations per second).\nIn terms of NumPy vs. CuPy, the main things to keep in mind are that 1. you must explicitly transfer arrays between the CPU and GPU, and 2. while on the GPU, you must use functions that act on CuPy arrays.\nBoth of these are easy to do. For this post, I will suffix all arrays that live on the GPU with _gpu. Let’s bring all of the things we just computed onto the GPU:\n\nimport cupy as cp\n\n# Bring diagonalizations Pi and Lambda onto GPU\npi_gpu = cp.asarray(pi)\nlam_gpu = cp.asarray(lam)\n\n# Bring L, Lt, and LtL onto the GPU\nL_gpu = cp.sparse.csc_matrix(L)\nLt_gpu = cp.sparse.csc_matrix(Lt)\nLtL_gpu = cp.sparse.csc_matrix(LtL)\n\nNote that scipy.sparse matrices are different from cupy.sparse matrices, and that we bring them onto the GPU by calling the CuPy constructor on the SciPy (CPU) version of the matrix.\nTo apply our blurring operator to a vector on the GPU, we will also need to use the CuPy version of gaussian_filter:\n\nfrom cupyx.scipy.ndimage import gaussian_filter as cupy_gaussian_filter \n\nblur_op_gpu = lambda x: cupy_gaussian_filter(x, blur_sigma, mode='wrap')\n\nLet’s make our arrays for \\(x\\), \\(v\\), \\(y\\), and \\(\\beta\\):\n\nx_gpu = cp.asarray(blurred_img) # Start x at the observed image\ny_gpu = cp.asarray(blurred_img) # The observation\nv_gpu = cp.asarray( L @ blurred_img.flatten() ) # Start v at Lx, this gives it the correct shape\nbetas_gpu = 10*cp.ones(2*M*N)\n\nRemember that we need to pick a new value of \\(\\lambda\\) each time we update \\(\\beta\\). We will make the choice \\[\n\\lambda(\\beta) = \\frac{1}{2}\\left( \\frac{1}{\\| B_{\\beta} \\|} + 0\\right),\n\\] which should be half of the maximum value we would be allowed to pick for \\(\\lambda\\). I am not sure how picking \\(\\lambda\\) on the high or low side affects the performance, but I would guess that higher is better. In our code, I will refer to this \\(\\lambda\\) as lam_aux.\n\nlam_aux = ((1/cp.amax(betas_gpu)) + 0)/2\n\nWe also need to set some noise precision \\(\\alpha = \\sigma^{-2}\\), and we can go ahead and pre-compute \\(\\frac{1}{\\alpha} A^T y\\) which shows up in our formula for the conditional for \\(x\\).\n\nnoise_sigma = 0.1\nnoise_prec = 1/(noise_sigma**2)\nAty_gpu = noise_prec*blur_op_gpu(y_gpu)\n\nAnd finally, we must select the hyper-hyper-parametes \\(c\\) and \\(d\\). We will examine two choices, the first being the “uniformative” choice \\[\n\\begin{align*}\nc &= 1.0, \\\\\nd &= 10^{-20},\n\\end{align*}\n\\] which corresponds to a flat prior.\n\nc = 1.0\nd = 1e-20\n\nNow we are ready to perform the BCD algorithm for the reconstruction of \\(x\\) from \\(y\\). I will not go into the details, as all of the operations are just the implementation of the steps we outlined earlier.\n\nfrom fastprogress import progress_bar\n\n# Set some number of iterations\nn_iterations = 500\n\n# Now do BCD\nfor j in progress_bar(range(n_iterations)):\n    \n    ### Update x\n    Ltv_gpu = ( Lt_gpu @ v_gpu ).reshape((M,N))\n    b = Aty_gpu + Ltv_gpu\n    mu_x = cp.fft.fft2( b, norm='ortho')\n    diag_fourier_vec = 1/( (1/lam_aux)*(pi_gpu) + (noise_prec)*(lam_gpu**2) )\n    mu_x = diag_fourier_vec*mu_x\n    mu_x = cp.real(cp.fft.ifft2(mu_x, norm='ortho'))\n    x_gpu = mu_x\n    \n    ### Update betas\n    gamma_alpha = c + 0.5\n    gamma_beta = 0.5*( ( L_gpu @ x_gpu.flatten() )**2) + d \n    betas_gpu = gamma_alpha/gamma_beta\n\n    ### Update lam_aux\n    lam_aux = ( (1/cp.amax(betas_gpu) ) + 0)/2\n    \n    # Make sure that this is always true\n    assert np.all((1/lam_aux) - betas_gpu &gt; 0), \"invalid choice of lam_aux was made\"\n    \n    ### Update v\n    v_gpu = ( (1/lam_aux) - betas_gpu )*( L_gpu @ x_gpu.flatten() )\n    \n# Bring arrays back to the cpu\nx = cp.asnumpy(x_gpu)\nv = cp.asnumpy(v_gpu)\nbetas = cp.asnumpy(betas_gpu)\n\n\n    \n        \n      \n      100.00% [500/500 00:29&lt;00:00]\n    \n    \n\n\nLet’s look at the reconstruction, as well as \\(\\beta\\). Here I have plotted the betas on an inverse scale, so brighter pixels should correspond to a higher variance parameter. I have also artificially clipped the array to its 95th percentile, since there are some extremely large precision values that make the entire plot monotone.\n\nimport matplotlib.pyplot as plt\n\nplt.rcParams['axes.facecolor'] = '#0F2537'\nplt.rcParams['text.color'] = 'white'\nplt.rcParams['axes.labelcolor'] = 'white'\nplt.rcParams['xtick.color'] = 'white'\nplt.rcParams['ytick.color'] = 'white'\n\nplot_name = \"uninformative_prior\"\nplot_which = \"Uniformative prior\"\n\nplt.figure(figsize=(13,13))\nplt.imshow(np.real(x), cmap='gray')\nplt.title(plot_which + \" \" + \"BCD reconstruction\")\nplt.show()\n\n\n\n\n\n\n\n\n\nbetas = betas.reshape((2,M,N))\nbetas_one, betas_two = betas[0,:,:], betas[1,:,:]\n\n\nplt.figure(figsize=(13,13))\nplt.imshow(np.clip(betas_one, a_min=None, a_max = np.quantile(betas_one.flatten(), 0.95)), cmap='Greens_r')\nplt.title(plot_which + \" \" + \"betas (horizontal)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(13,13))\nplt.imshow(np.clip(betas_two, a_min=None, a_max = np.quantile(betas_two.flatten(), 0.95)), cmap='Greens_r')\nplt.title(plot_which + \" \" + \"betas (vertical)\")\nplt.show()\n\n\n\n\n\n\n\n\nNotice how you can (somewhat) trace out some of the edges in the image by the inferred variance parameters. Here is a comparison of the original, blurred, and reconstructed images:\n\n# Load original image\nimg = Image.open(\"grand_canyon.jpg\")\nimg = ImageOps.grayscale(img)\nimg = np.asarray(img)\nimg = img/255\n\nfig, axs = plt.subplots(1, 3, figsize=(26,26))\n\naxs[0].imshow(img, cmap='gray')\naxs[0].set_title(\"Original image\")\naxs[1].imshow(np.real(x), cmap='gray')\naxs[1].set_title(plot_which + \" \" + \"BCD reconstruction\")\naxs[2].imshow(blurred_img, cmap='gray')\naxs[2].set_title(\"Blurred image\")\n\nplt.show()\n\n\n\n\n\n\n\n\nThe last thing I will point out is that by changing the \\(c\\) and \\(d\\) hyper-hyper-parameters, you can change the degree of sparsity in the prior on \\(L x\\). As we did in a previous post, we can recover a prior that is marginally Cauchy with a special choice of \\(c\\) and \\(d\\). Let’s repeat the reconstruction with the choice \\[\n\\begin{align*}\nc &= 0.5, \\\\\nd &= 0.5(0.0005^2).\n\\end{align*}\n\\]\n\nnoise_sigma = 0.01\nc = 0.5\nd = 0.5*(0.0005**2)\n\nx_gpu = cp.asarray(blurred_img) # Start x at the observed image\ny_gpu = cp.asarray(blurred_img) # The observation\nv_gpu = cp.asarray( L @ blurred_img.flatten() ) # Start v at Lx, this gives it the correct shape\nbetas_gpu = 10*cp.ones(2*M*N)\n\nlam_aux = ((1/cp.amax(betas_gpu)) + 0)/2\nnoise_prec = 1/(noise_sigma**2)\nAty_gpu = noise_prec*blur_op_gpu(y_gpu)\n\n\n# Set some number of iterations\nn_iterations = 500\n\n# Now do BCD\nfor j in progress_bar(range(n_iterations)):\n    \n    ### Update x\n    Ltv_gpu = ( Lt_gpu @ v_gpu ).reshape((M,N))\n    b = Aty_gpu + Ltv_gpu\n    mu_x = cp.fft.fft2( b, norm='ortho')\n    diag_fourier_vec = 1/( (1/lam_aux)*(pi_gpu) + (noise_prec)*(lam_gpu**2) )\n    mu_x = diag_fourier_vec*mu_x\n    mu_x = cp.real(cp.fft.ifft2(mu_x, norm='ortho'))\n    x_gpu = mu_x\n    \n    ### Update betas\n    gamma_alpha = c + 0.5\n    gamma_beta = 0.5*( ( L_gpu @ x_gpu.flatten() )**2) + d \n    betas_gpu = gamma_alpha/gamma_beta\n\n    ### Update lam_aux\n    lam_aux = ( (1/cp.amax(betas_gpu) ) + 0)/2\n    \n    # Make sure that this is always true\n    assert np.all((1/lam_aux) - betas_gpu &gt; 0), \"invalid choice of lam_aux was made\"\n    \n    ### Update v\n    v_gpu = ( (1/lam_aux) - betas_gpu )*( L_gpu @ x_gpu.flatten() )\n    \n# Bring arrays back to the cpu\nx = cp.asnumpy(x_gpu)\nv = cp.asnumpy(v_gpu)\nbetas = cp.asnumpy(betas_gpu)\n\n\n    \n        \n      \n      100.00% [500/500 00:29&lt;00:00]\n    \n    \n\n\n\nplot_name = \"strong_cauchy_prior\"\nplot_which = \"Strong Cauchy prior\"\n\nplt.figure(figsize=(13,13))\nplt.imshow(np.real(x), cmap='gray')\nplt.title(plot_which + \" \" + \"BCD reconstruction\")\nplt.show()\n\n\n\n\n\n\n\n\n\nbetas = betas.reshape((2,M,N))\nbetas_one, betas_two = betas[0,:,:], betas[1,:,:]\n\n\nplt.figure(figsize=(13,13))\nplt.imshow(np.clip(betas_one, a_min=None, a_max = np.quantile(betas_one.flatten(), 0.95)), cmap='Greens')\nplt.title(plot_which + \" \" + \"betas (horizontal)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(13,13))\nplt.imshow(np.clip(betas_two, a_min=None, a_max = np.quantile(betas_two.flatten(), 0.95)), cmap='Greens')\nplt.title(plot_which + \" \" + \"betas (vertical)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(1, 3, figsize=(26,26))\n\naxs[0].imshow(img, cmap='gray')\naxs[0].set_title(\"Original image\")\naxs[1].imshow(np.real(x), cmap='gray')\naxs[1].set_title(plot_which + \" \" + \"BCD reconstruction\")\naxs[2].imshow(blurred_img, cmap='gray')\naxs[2].set_title(\"Blurred image\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n[1] Glaubitz, J., Gelb, A. and Song, G. (2022). Generalized sparse bayesian learning and application to image reconstruction.\n\n\n[2] Marnissi, Y., Chouzenoux, E., Benazza-Benyahia, A. and Pesquet, J.-C. (2018). An auxiliary variable method for markov chain monte carlo algorithms in high dimension. Entropy 20.\n\n\n[3] Dyk, D. A. van and Park, T. (2008). Partially collapsed gibbs samplers. Journal of the American Statistical Association 103 790–6."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Here is a list of links to some miscellaneous Python packages I have developed (or are still developing).\ngpcg\nAn implementation of a conjugate gradient algorithm (GPCG) for solving bound-constrained quadratic programs.\ntracelogdetdiag\nSome implementations of randomized, matrix-free algorithms for estimating matrix traces, log determinants, diagonals, and diagonals of inverses.\nrjpo\nAn implementation of a reversible jump perturbation optimization (RJPO) method for sampling high-dimensional Gaussians.\nrunningstatistics\nA simple module for computing the sample statistics of an array in a running/online fashion."
  },
  {
    "objectID": "office_hours.html",
    "href": "office_hours.html",
    "title": "Office Hours",
    "section": "",
    "text": "This fall term I will be holding office hours for Math 3 on Mondays, Tuesdays, and Wednesdays from 1:00 – 2:00 PM. Any changes to these times will be communicated in class and through Canvas. My office is located at Kemeny Hall, Room 219."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Auxiliary Magic\n\n\n\n\n\nHierarchical regularization via an auxiliary variable magic trick.\n\n\n\n\n\nJul 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nA Simple Image De-blurring Example\n\n\n\n\n\nFun with BCCB matrices.\n\n\n\n\n\nJul 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSampling a Cauchy Markov Random Field\n\n\n\n\n\nLet’s see if we can sample this directly using a hierarchical representation.\n\n\n\n\n\nJun 15, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jonathan Lindbloom",
    "section": "",
    "text": "I am a 4th year applied mathematics PhD candidate at Dartmouth College, originally from Dallas, Texas. I study computational methods for solving (inverse) problems and uncertainty quantification. My advisor is Anne Gelb."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jonathan Lindbloom",
    "section": "Education",
    "text": "Education\nDartmouth College\nPhD in Mathematics\nSept 2021 — Present\nDartmouth College\nMA in Mathematics\nSept 2021 — June 2023\nSouthern Methodist University\nBS in Mathematics, BBA in Finance\nAug 2017 — May 2021"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Preprints\nGeneralized sparsity-promoting solvers for Bayesian inverse problems: Versatile sparsifying transforms and unknown noise variances. Jonathan Lindbloom, Jan Glaubitz, and Anne Gelb. 2024.      arXiv\n \nComplex-valued signal recovery using the Bayesian LASSO. Dylan Green, Jonathan Lindbloom, and Anne Gelb. 2024.      arXiv\n \nConference papers\nComplex-Valued Image Recovery from Multiple Measurements. Dylan Green, Jonathan Lindbloom, and Anne Gelb. 2024 IEEE Conference on Computational Imaging Using Synthetic Apertures (CISA). 2024.      link\n \nTalks\nThe Conjugate Gradient Method: A Top 10 Algorithm of the 20th Century. Dartmouth Mathematics Graduate Student Seminar. Hanover, NH. May 2024.      Photo\n \nGeneralized Sparsity-Promoting Solvers and Uncertainty Quantification for Bayesian Inverse Problems. Fondazione CMCC Webinar. Bologna, IT. March 2024.      Slides         Video\n \nGeneralized Sparsity-Promoting Solvers and Samplers for Hierarchical Inverse Problems. SIAM Conference on Uncertainty Quantification. Trieste, IT. February 2024.      Slides\n \nA Brief Introduction to Matrix-Free and Randomized Matrix Computations. Dartmouth Mathematics Graduate Student Seminar. Hanover, NH. January 2024.\n \nOblique Projections and Low-Rank Structure in Inverse Problems. Dartmouth Mathematics Graduate Student Seminar. Hanover, NH. September 2023.      Slides\n \nComputational Strategies for Bayesian Inversion with Conditionally Gaussian Sparsity Priors. SIAM Conference on Computational Science and Engineering. Amsterdam, NL. March 2023.      Slides\n \nHierarchical Bayesian Inverse Problems. Dartmouth Mathematics Graduate Student Seminar. Hanover, NH. January 2023.      Slides\n \nGeneralized Hybrid Solvers for Sparsity-Promoting Bayesian Inverse Problems. Dartmouth Advancement Examination. Hanover, NH. November 2022.      Slides\n \nGeneralized Hybrid Solvers for Large-Scale Edge-Preserving Inversion. Dartmouth Sea Ice Modeling and Data Assimilation MURI Annual Meeting. Virtual. November 2022.      Slides         Video\n \nSurrogate Modeling for High Explosives Diameter Effect Calculations. Theoretical Division Lightning Talk Symposium. Los Alamos, NM. August 2022.\n \nMultiplicative Denoising with Uncertainty Quantification for Synthetic Aperture Radar Imaging. SIAM Conference on Uncertainty Quantification. Atlanta, GA. April 2022.      Slides\n \nTowards UQ for SAR Despeckling. Dartmouth Sea Ice Modeling and Data Assimilation MURI Annual Meeting. Virtual. October 2021.      Slides         Video\n \nUncertainty Quantification in High Explosives Equations of State. LANL Computational Physics Summer Workshop Final Presentations. Virtual. August 2021.      Slides\n \nPosters\nEfficient Gaussian Sampling for Sparsity-Promoting Bayesian Inversion. NENAD 2024. Hanover, NH. June 2024.\n \nSparsity-Promoting Multiplicative Denoising via Block Coordinate Descent. SIAM POD-UQ. Trieste, IT. February 2024.      Poster\n \nTechnical articles\nComplex-Valued Image Recovery from Multiple Measurements. Dylan Green, Jonathan Lindbloom, and Anne Gelb. 2024 IEEE Conference on Computational Imaging Using Synthetic Apertures (CISA). 2024.      link"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "This page summarizes my teaching activities.\nInstructor\n\n**Upcoming** Math 56: Computational and Experimental Mathematics, Dartmouth College, Winter 2025\nMath 3: Calculus, Dartmouth College, Fall 2023 [lecture notes]\n\nTeaching Assistant\n\nMath 22: Linear Algebra and Applications, Dartmouth College, Spring 2023\nMath 56: Computational and Experimental Mathematics, Dartmouth College, Winter 2023\nMath 3: Calculus, Dartmouth College, Winter 2023\nMath 3: Calculus, Dartmouth College, Winter 2022\nMath 3: Calculus, Dartmouth College, Fall 2021\n\nGrading Assistant\n\nMath 3311: Introduction to Proofs and Analysis, Southern Methodist University, Fall 2020 / Spring 2021\nMath 3304: Introduction to Linear Algebra, Southern Methodist University, Fall 2019 / Spring 2020 / Fall 2020\nMath 1337: Calculus I, Southern Methodist University, Fall 2018\n\nDirected Reading Programs\nI have enjoyed participating as a mentor in Dartmouth’s Directed Reading Program (DRP) for undergraduates. Past reading programs I have supervised include:\n\nMarkov Chain Monte Carlo and Bayesian Computation (Spring 2024, with Michael Bond and Vadin Thadhani)\nComputational Measure Transport (Winter 2024, with William O’Brien and Paul Chirkov) \nMarkov Chain Monte Carlo and Bayesian Computation (Spring 2023, with Daniel Carstensen)\nBayesian Modeling and Computation (Spring 2022, with Ivy Yan)\nNonlinear Dynamics, Chaos, and Ergodicity (Fall 2021, with Andrew White)\n\nIf you are interested in participating in a DRP (formally or informally), feel free to write me via email here."
  },
  {
    "objectID": "posts/hierarchical_cmrf.html",
    "href": "posts/hierarchical_cmrf.html",
    "title": "Sampling a Cauchy Markov Random Field",
    "section": "",
    "text": "The goal of this post is to write some code that can generate direct1 samples from a Cauchy Markov random field (CMRF) [1] [2] on a 2D regular lattice. A more general Markov Random Field (MRF) is just a multivariate probability distribution possessing a special conditional-independence property amongst the components. This property permits the use of efficient numerical methods developed to exploit this property. CMRFs are cousins of the better-known Gaussian Markov Random Fields (GMRFs), which are themselves special cases of Gaussian fields (a.k.a. Gaussian processes).\nOur main reference for this post will be [3] which is a comprehensive text on the subject. Our plan of attack is to:"
  },
  {
    "objectID": "posts/hierarchical_cmrf.html#footnotes",
    "href": "posts/hierarchical_cmrf.html#footnotes",
    "title": "Sampling a Cauchy Markov Random Field",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNOT using a Markov Chain Monte Carlo (MCMC) method.↩︎\nSee chapter 4 of [3], also here↩︎"
  }
]