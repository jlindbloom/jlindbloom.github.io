<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.587">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-07-07">
<meta name="description" content="Hierarchical regularization via an auxiliary variable magic trick.">

<title>Jonathan Lindbloom - Auxiliary Magic</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Jonathan Lindbloom</span>
  </a>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../research.html">Research</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html">Blog</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#probabilistic-inverse-problems" id="toc-probabilistic-inverse-problems" class="nav-link active" data-scroll-target="#probabilistic-inverse-problems">Probabilistic inverse problems</a></li>
  <li><a href="#a-hierarchical-prior" id="toc-a-hierarchical-prior" class="nav-link" data-scroll-target="#a-hierarchical-prior">A hierarchical prior</a></li>
  <li><a href="#the-new-posterior" id="toc-the-new-posterior" class="nav-link" data-scroll-target="#the-new-posterior">The new posterior</a></li>
  <li><a href="#an-inconvenience" id="toc-an-inconvenience" class="nav-link" data-scroll-target="#an-inconvenience">An inconvenience</a></li>
  <li><a href="#an-auxiliary-variable-magic-trick" id="toc-an-auxiliary-variable-magic-trick" class="nav-link" data-scroll-target="#an-auxiliary-variable-magic-trick">An auxiliary variable magic trick</a></li>
  <li><a href="#the-augmented-hierarchical-model" id="toc-the-augmented-hierarchical-model" class="nav-link" data-scroll-target="#the-augmented-hierarchical-model">The augmented hierarchical model</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Auxiliary Magic</h1>
</div>

<div>
  <div class="description">
    Hierarchical regularization via an auxiliary variable magic trick.
  </div>
</div>


<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 7, 2022</p>
    </div>
  </div>
    
  </div>
  

</header>

<p><a href="https://jlindbloom.github.io/posts/simple_image_deblurring.html">Last time</a> we looked at an image de-blurring problem, which we solved by finding</p>
<p><span id="eq-inv_prob_obj"><span class="math display">\[
x^\star = \text{argmin}_x \,\, \| A x - y \|_2^2 + \mathcal{R}(x)
\tag{1}\]</span></span></p>
<p>where the regularization term was</p>
<p><span class="math display">\[
\mathcal{R}(x) = \gamma \| L x \|_2^2 = \gamma x^T L^T L x.
\]</span></p>
<p>In this post, our goal is to: - describe a hierarchical prior and a method that can give us a better image reconstruction, - walk through a “magic” trick that will speed up our method, - and look at using <a href="https://cupy.dev/"><code>CuPy</code></a> to accelerate our reconstruction using a GPU.</p>
<section id="probabilistic-inverse-problems" class="level1">
<h1>Probabilistic inverse problems</h1>
<p>While the problem posed in <a href="#eq-inv_prob_obj">Equation&nbsp;1</a> is completely deterministic, we can actually think of it as haven arisen from a probabilistic model. Suppose that</p>
<p><span class="math display">\[
\begin{align*}
x &amp;\sim \mathcal{N}\left( 0, \left(\gamma L^T L \right)^{-1} \right), \\
y \, | \, x &amp;\sim \mathcal{N}\left( A x, I \right).
\end{align*}
\]</span></p>
<p>Then our corresponding density functions are</p>
<p><span class="math display">\[
\begin{align*}
\pi(x) &amp;\propto \exp\left\{ - \gamma x^T L^T L  x \right\}, \\
\pi(y \, | \, x) &amp;\propto \exp\left\{ - \| A x - y \|_2^2 \right\},
\end{align*}
\]</span></p>
<p>and by Bayes’ theorem the posterior density for <span class="math inline">\(x \, | \, y\)</span> is given as</p>
<p><span class="math display">\[
\pi(x \, | \, y) \propto \exp\left\{ - \| A x - y \|_2^2 \right\} \times \exp\left\{ - \gamma x^T L^T L  x \right\}.
\]</span></p>
<p>The <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">MAP estimate</a> of <span class="math inline">\(x\)</span> is then given as</p>
<p><span class="math display">\[
x^\star = \text{argmax}_x \,\, \pi(x \, | \, y),
\]</span></p>
<p>which is equivalent to</p>
<p><span class="math display">\[
\begin{align*}
x^\star &amp;= \text{argmin}_x \,\, - \log \pi(x \, | \, y) \\
&amp;= \text{argmin}_x \,\, \gamma \| L x \|_2^2 = \gamma x^T L^T L x,
\end{align*}
\]</span></p>
<p>which is exactly <a href="#eq-inv_prob_obj">Equation&nbsp;1</a>. The role of <span class="math inline">\(\mathcal{R}(x)\)</span> can then be seen as contributing a prior of</p>
<p><span class="math display">\[
\pi(x) \propto \exp\left\{ - \mathcal{R}(x) \right\}
\]</span></p>
<p>into the inference problem.</p>
</section>
<section id="a-hierarchical-prior" class="level1">
<h1>A hierarchical prior</h1>
<p>One reason it can be useful to think probabilistically is because we can motivate different choices of the regularizer <span class="math inline">\(\mathcal{R}(x)\)</span>. If we pick</p>
<p><span class="math display">\[
Lx \sim \mathcal{N}\left(0, \frac{1}{\gamma} I \right)
\]</span></p>
<p>as our prior (which corresponds to <span class="math inline">\(\mathcal{R}(x) = \gamma x^T L^T L x\)</span>), then we are saying that we believe that the discrete gradients in our image are distributed according to zero-mean Gaussian with variance <span class="math inline">\(\gamma^{-1}\)</span>. We can tweak the strength of the prior by adjusting <span class="math inline">\(\gamma\)</span> and in turn its influence on our reconstructed image, but note that the same <span class="math inline">\(\gamma\)</span> governs of the entire discrete gradient in the image. Thus we might think to introduce a hierarchical prior on the discrete gradient that could try to (loosely) capture the fact that in some regions in an image the discrete gradient will be much larger than it is elsewhere. Define the prior</p>
<p><span id="eq-hierarchical_prior"><span class="math display">\[
\begin{align*}
\beta^H_{i,j}, \beta^V_{i,j} &amp;\sim \Gamma \left( c, d \right), \\
L x &amp;\sim \mathcal{N} \left(0, B_{\beta} \right),
\end{align*}
\tag{2}\]</span></span></p>
<p>which has density</p>
<p><span class="math display">\[
\pi(x, \beta) = \pi(x \, | \, \beta) \pi(\beta) \propto \det \left( B_{\beta} \right)^{1/2} \exp\left\{ - x^T L^T B_{\beta} L x  \right\} \times \pi(\beta).
\]</span></p>
<p>Here <span class="math inline">\(\left( \cdot \right)^{V/H}\)</span> represent the fact that we are assigining two different hyper-parameter to govern the gradient in each the vertical and horizontal directions, <span class="math inline">\(\Gamma\left(c, d \right)\)</span> represents the <a href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma density function</a>,</p>
<p><span class="math display">\[
B_{\beta} = \text{diag}\left( \beta^V_{1,1}, \ldots, \beta^V_{m,n}, \beta^H_{1,1}, \ldots, \beta^H_{m,n} \right),
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\pi(\beta) \propto \left( \prod_{i,j}^{mn} \Gamma( \beta_{i,j}^H | c, d) \right) \times \left( \prod_{i,j}^{mn} \Gamma( \beta_{i,j}^V | c, d) \right)
\]</span></p>
<p>meaning that all hyper-parameters are assumed to be independent of one another. The reason we use a <span class="math inline">\(\Gamma\)</span> distribution for the hyper-parameter is because it is a <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior</a> for a Gaussian, meaning that we can determine certain relevant conditional distributions analytically.</p>
</section>
<section id="the-new-posterior" class="level1">
<h1>The new posterior</h1>
<p>Using our prior <a href="#eq-hierarchical_prior">Equation&nbsp;2</a>, the full posterior in our original model (with an extra factor of <span class="math inline">\(\frac{1}{2}\)</span>) is given as</p>
<p><span class="math display">\[
\pi(x, \beta \, | \, y) \propto \exp\left\{ - \frac{1}{2} \| A x - y \|_2^2 \right\} \times \det \left( B_{\beta} \right)^{1/2} \exp\left\{ - \frac{1}{2} x^T L^T B_{\beta} L x  \right\} \times \pi(\beta).
\]</span>{eq-hierarchical_posterior}</p>
<p>In the sampling setting, a common approach that could be used to draw samples from this posterior is <a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs sampling</a>, which involves iteratively drawing from the conditionals of each variable in the density given all of the others. In this case, our conditionals would be</p>
<p><span class="math display">\[
\begin{align*}
\pi(x \, | \, \beta, y) &amp;\propto \exp\left\{ - \| A x - y \|_2^2 \right\} \times  \exp\left\{ - x^T L^T B_{\beta} L x  \right\}, \\
\pi(\beta \, | \, x, y) &amp;\propto \det \left( B_{\beta} \right)^{1/2} \exp\left\{ - x^T L^T B_{\beta} L x  \right\} \times \pi(\beta).
\end{align*}
\]</span></p>
<p>Due to conjugacy, we can recognize the first conditional to be a Gaussian and the second to be a Gamma distribution. Specifically, we have <span class="math inline">\(x \, | \, \beta, y \sim \mathcal{N}\left(\mu_1, Q_1^{-1} \right)\)</span> with</p>
<p><span class="math display">\[
\begin{align*}
Q_1 &amp;= A^T A + L^T B_{\beta} L, \\
\mu_1 &amp;= Q_1^{-1} A^T y.
\end{align*}
\]</span></p>
<p>For <span class="math inline">\(\beta \, | \, x, y\)</span> we have</p>
<p><span class="math display">\[
\begin{align*}
\beta_{i,j} = \Gamma\left( \frac{1}{2} + c, \frac{1}{2}[L x]_{i,j}^2 + d \right),
\end{align*}
\]</span></p>
<p>where how you deal with <span class="math inline">\(\beta^V\)</span>/<span class="math inline">\(\beta^H\)</span> becomes clear when you think about the shapes of these operations.</p>
<p>Rather than code a Gibbs sampling algorithm, we will consider the (BCD) of <span class="citation" data-cites="Glaubitz2022">[<a href="#ref-Glaubitz2022" role="doc-biblioref">1</a>]</span>, which is essentially an optimization technique that iteratively draws from the mean of each the conditionals. We iterate</p>
<p><span class="math display">\[
\begin{align*}
x^{k+1} &amp;= \mathbb{E}_{\pi}\left( x \, | \, \beta^k, y \right), \\
\beta^{k+1} &amp;= \mathbb{E}_{\pi}\left( \beta \, | \, x^{k+1}, y \right),
\end{align*}
\]</span></p>
<p>until the solution converges. One thing to note is that we will generally not settle into a global minimum, since the posterior (with no restrictions on <span class="math inline">\(c\)</span> and <span class="math inline">\(d\)</span>) is not log-concave due to the use of a Gamma hyper-prior on the <span class="math inline">\(\beta_{i,j}\)</span>.</p>
</section>
<section id="an-inconvenience" class="level1">
<h1>An inconvenience</h1>
<p>For the purposes of this post, we will assume that <span class="math inline">\(A\)</span> is a BCCB blurring matrix and that <span class="math inline">\(L\)</span> is a BCCB approximation to the discrete gradient. These were the nice assumptions in the previous post that allowed us to reduce all of our linear system solves to FFTs/IFFs and diagonal matrix operations, as well as efficiently sample in the Fourier domain.</p>
<p>Note that while we have an analytic expression for the distribution <span class="math inline">\(x \, | \beta, y\)</span>, we can no longer use our BCCB assumptions to avoid linear system solves. Letting</p>
<p><span class="math display">\[
\begin{align*}
A &amp;= F^H \Lambda F, \\
L &amp;= F^H \Pi F,
\end{align*}
\]</span></p>
<p>inserting this into <span class="math inline">\(Q_1^{-1} = \left( A^T A + L^T B_{\beta} L \right)^{-1}\)</span> we now get</p>
<p><span class="math display">\[
\begin{align*}
Q_1^{-1} &amp;= \left( A^T A +  L^T B_{\beta} L \right)^{-1} \\
&amp;= \left( F^H \Lambda F F^H \Lambda F +  F^H \Pi F B_{\beta} F^H \Pi F \right)^{-1} \\
&amp;= F^H \left( \Lambda^2 F +  \Pi F^H B_{\beta} F \Pi \right)^{-1} F \\
&amp;= F^H \left( \Lambda^2 +  F^H B_{\beta} F \right)^{-1} F  \\
\end{align*}
\]</span></p>
<p>where unlike before we are now stuck. The problem is that <span class="math inline">\(B_{\beta}\)</span> is “sandwiched” in-between <span class="math inline">\(F^H\)</span> and <span class="math inline">\(F\)</span>, which prevents us from simplifying with <span class="math inline">\(F^H F = I\)</span>. While it is not the end of the world to have to solve a linear system using a sparse solver or some other method, we still really would like to use the BCCB assumption to our advantage if we can.</p>
</section>
<section id="an-auxiliary-variable-magic-trick" class="level1">
<h1>An auxiliary variable magic trick</h1>
<p>It turns out there is a way to get around this, via a “magic trick” of <span class="citation" data-cites="Marnissi2018">[<a href="#ref-Marnissi2018" role="doc-biblioref">2</a>]</span>. For the moment let us fix <span class="math inline">\(B_{\beta}\)</span> to be a constant matrix. Out of the blue, define a new random variable via</p>
<p><span class="math display">\[
\begin{align*}
    u \, | \, x, y &amp;\sim \mathcal{N}\left( H x, Q^{-1}  \right),
\end{align*}
\]</span></p>
<p>where we are free to choose <span class="math inline">\(H\)</span> and <span class="math inline">\(Q\)</span> (as long as they define a valid distribution). Then, if we consider the joint density of <span class="math inline">\(u, x \, | \, y\)</span> we have that</p>
<p><span class="math display">\[
\begin{align*}
\pi(u, x \, | \, y) &amp;= \pi(u \, | \, x, y) \times \pi(x \, | \, y) \times \pi(x) \\
&amp;\propto \exp\left\{ -\frac{1}{2} \left( H x - u \right)^T Q \left( H x - u \right)  \right\} \times \exp\left\{ -\frac{1}{2} x^T L^T B L x  \right\} \times \pi(x \, | \, y) \\
&amp;\propto \exp\left\{ -\frac{1}{2} \left[ \left( H x - u \right)^T Q \left( H x - u \right) + x^T L^T B L x  \right] \right\} \times \pi(x \, | \, y) \\
&amp;\propto \exp\left\{ -\frac{1}{2} \left[ x^T H^T Q H x + u^T Q u - 2 x^T H^T Q u + x^T L^T B L x  \right] \right\} \times \pi(x \, | \, y) \\
&amp;\propto \exp\left\{ -\frac{1}{2} \left[ x^T P x + u^T Q u - 2 x^T H^T Q u  \right] \right\} \times \pi(x \, | \, y) \\
\end{align*}
\]</span></p>
<p>where we have defined</p>
<p><span class="math display">\[
P = H^T Q H + L^T B L.
\]</span></p>
<p>We are free to choose <span class="math inline">\(H\)</span> and <span class="math inline">\(Q\)</span>, so let’s make the magic choice</p>
<p><span class="math display">\[
\begin{align*}
H &amp;= L, \\
Q &amp;= \frac{1}{\lambda} I  - B.
\end{align*}
\]</span></p>
<p>Here <span class="math inline">\(\lambda &gt; 0\)</span> is a constant we are free to choose, so long as <span class="math inline">\(\lambda &lt; \frac{1}{\| B \|}\)</span> which makes <span class="math inline">\(Q\)</span> positive semi-definite as is required of a valid precision matrix. Inserting this choice <em>and conditioning on</em> <span class="math inline">\(u\)</span>, we obtain</p>
<p><span class="math display">\[
P = \frac{1}{\lambda} L^T L
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{align*}
\pi(x \, | \, u,  y) &amp;\propto \exp\left\{ -\frac{1}{2} \left[ \frac{1}{\lambda} x^T L^T L x - 2 x^T L^T Q u  \right] \right\} \times \pi(x \, | \, y). \\
\end{align*}
\]</span></p>
<p>Now, we define yet another random variable</p>
<p><span class="math display">\[
v = Qu = \left( \frac{1}{\lambda} I  - B \right) u,
\]</span></p>
<p>insert into the density above to get</p>
<p><span class="math display">\[
\begin{align*}
\pi(x \, | \, v,  y) &amp;\propto \exp\left\{ -\frac{1}{2} \left[ \frac{1}{\lambda} x^T L^T L x - 2 x^T L^T v \right] \right\} \times \pi(x \, | \, y), \\
\end{align*}
\]</span></p>
<p>and finally complete the square in the exponential to finally arrive at</p>
<p><span class="math display">\[
\begin{align*}
\pi(x \, | \, v,  y) &amp;\propto \exp\left\{ -\frac{1}{2 \lambda} \| L x - \lambda v  \|_2^2  \right\} \times \pi(x \, | \, y), \\
\end{align*}
\]</span></p>
<p>which is the density of the Gaussian <span class="math inline">\(\mathcal{N}\left( \mu_2, Q_2^{-1} \right)\)</span> where</p>
<p><span class="math display">\[
\begin{align*}
Q_2 &amp;= \frac{1}{\lambda} L^T L + A^T A, \\
\mu_2 &amp;= Q_2^{-1} \left( A^T y +  L^T \left( \frac{1}{\lambda} I \right) \left( \lambda v \right)  \right) \\
&amp;= Q_2^{-1} \left( A^T y +  L^T  v \right).
\end{align*}
\]</span></p>
<p>Note that we could do this since the missing term in the square was independent of <span class="math inline">\(x\)</span>. Now we have what we wanted: <em>the matrix</em> <span class="math inline">\(B\)</span> <em>has disappeared</em> after conditioning on <span class="math inline">\(v\)</span>, and we can now take advantage of our BCCB assumption for this conditional. This is all only useful so long as the conditional distribution $v , | , x, y $ is also nice, which is the Gaussian <span class="math inline">\(\mathcal{N}\left( \mu_3, Q_3^{-1} \right)\)</span> where</p>
<p><span class="math display">\[
\begin{align*}
Q_3^{-1} &amp;= B, \\
\mu_3 &amp;= B L x,
\end{align*}
\]</span></p>
<p>which is nice to work with.</p>
</section>
<section id="the-augmented-hierarchical-model" class="level1">
<h1>The augmented hierarchical model</h1>
<p>It turns out that when we allow <span class="math inline">\(B = B_{\beta}\)</span> to vary with hyper-parameters, everything we just did stays exactly the same, except we must choose a new <span class="math inline">\(\lambda= \lambda(B_{\beta})\)</span> such that <span class="math inline">\(\lambda &lt; \frac{1}{\| B_\beta \|}\)</span> for the current values of the hyper-parameters. So applying this auxiliary variable magic trick to <strong>?@eq-hierarchical_posterior</strong>, we now consider the alternative posterior with density</p>
<p><span class="math display">\[
\pi(x, v, \beta \, | \, y) \propto \exp\left\{ - \frac{1}{2} \left( v - B_{\beta} L x \right)^T B_{\beta}^{-1} \left( v - B_{\beta} L x \right) \right\} \times \exp\left\{ - \frac{1}{2} \| A x - y \|_2^2 \right\} \times \det \left( B_{\beta} \right)^{1/2} \exp\left\{ - \frac{1}{2} x^T L^T B_{\beta} L x  \right\} \times \pi(\beta).
\]</span></p>
<p>We have already discussed the conditionals <span class="math inline">\(x \, | \, v, \beta, y\)</span> and <span class="math inline">\(v \, | \, x, \beta, y\)</span> which are the same as before, but the conditional that changes is <span class="math inline">\(\beta \, | \, x, v, y\)</span> which has density</p>
<p><span class="math display">\[
\pi(\beta \, | \, x, v, y) \propto \exp\left\{ - \frac{1}{2} \left( v - B_{\beta} L x \right)^T B_{\beta}^{-1} \left( v - B_{\beta} L x \right) \right\} \times \det \left( B_{\beta} \right)^{1/2} \exp\left\{ - \frac{1}{2} x^T L^T B_{\beta} L x  \right\} \times \pi(\beta).
\]</span></p>
<p>However, we can determine this by just updating the gamma hyper-prior twice so that this conditional is</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-Glaubitz2022" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline"><span class="smallcaps">Glaubitz</span>, J., <span class="smallcaps">Gelb</span>, A. and <span class="smallcaps">Song</span>, G. (2022). <a href="https://doi.org/10.48550/ARXIV.2201.07061">Generalized sparse bayesian learning and application to image reconstruction</a>.</div>
</div>
<div id="ref-Marnissi2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline"><span class="smallcaps">Marnissi</span>, Y., <span class="smallcaps">Chouzenoux</span>, E., <span class="smallcaps">Benazza-Benyahia</span>, A. and <span class="smallcaps">Pesquet</span>, J.-C. (2018). <a href="https://doi.org/10.3390/e20020110">An auxiliary variable method for markov chain monte carlo algorithms in high dimension</a>. <em>Entropy</em> <strong>20</strong>.</div>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>