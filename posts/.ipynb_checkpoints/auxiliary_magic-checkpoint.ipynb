{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Auxiliary Magic\n",
    "description: Hierarchical regularization via a trick.\n",
    "date: 7/7/2022\n",
    "draft: true\n",
    "bibliography: references.bib\n",
    "csl: annals_statistics.csl\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Last time](https://jlindbloom.github.io/posts/simple_image_deblurring.html) we looked at an image de-blurring problem, which we solved by finding\n",
    "$$\n",
    "x^\\star = \\text{argmin}_x \\,\\, \\| A x - y \\|_2^2 + \\mathcal{R}(x)\n",
    "$${#fig-inv_prob_obj}\n",
    "where the regularization term was\n",
    "$$\n",
    "\\mathcal{R}(x) = \\gamma \\| L x \\|_2^2 = \\gamma x^T L^T L x.\n",
    "$$\n",
    "In this post, our goal is to:\n",
    "- describe a hierarchical prior and a method that can give us a better image reconstruction,\n",
    "- walk through a \"magic\" trick that will speed up our method,\n",
    "- and look at using [`CuPy`](https://cupy.dev/) to accelerate our reconstruction using a GPU.\n",
    "\n",
    "# Probabilistic inverse problems\n",
    "\n",
    "While the problem posed in @inv_prob_obj is completely deterministic, we can actually think of it as haven arisen from a probabilistic model. Suppose that\n",
    "$$\n",
    "\\begin{align*}\n",
    "x &\\sim \\mathcal{N}\\left( 0, \\left(\\gamma L^T L \\right)^{-1} \\right), \\\\\n",
    "y \\, | \\, x &\\sim \\mathcal{N}\\left( A x, I \\right).\n",
    "\\end{align*}\n",
    "$$\n",
    "Then our corresponding density functions are\n",
    "$$\n",
    "\\pi(x) &\\propto \\exp\\left\\{ - \\gamma x^T L^T L  x \\right\\}, \\\\\n",
    "\\pi(y \\, | \\, x) &\\propto \\exp\\left\\{ - \\| A x - y \\|_2^2 \\right\\},\n",
    "$$\n",
    "and by Bayes' theorem the posterior density for $x \\, | \\, y$ is given as\n",
    "$$\n",
    "\\pi(x \\, | \\, y) &\\propto \\exp\\left\\{ - \\| A x - y \\|_2^2 \\right\\} \\times \\exp\\left\\{ - \\gamma x^T L^T L  x \\right\\}.\n",
    "$$\n",
    "The [MAP estimate](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) of $x$ is then given as\n",
    "$$\n",
    "x^\\star = \\text{argmax}_x \\,\\, \\pi(x \\, | \\, y),\n",
    "$$\n",
    "which is equivalent to \n",
    "$$\n",
    "\\begin{align*}\n",
    "x^\\star &= \\text{argmin}_x \\,\\, - \\log \\pi(x \\, | \\, y) \\\\\n",
    "&= \\text{argmin}_x \\,\\, \\gamma \\| L x \\|_2^2 = \\gamma x^T L^T L x,\n",
    "\\end{align*}\n",
    "$$\n",
    "which is exactly @inv_prob_obj. The role of $\\mathcal{R}(x)$ can then be seen as contributing a prior of\n",
    "$$\n",
    "\\pi(x) &\\propto \\exp\\left\\{ - \\mathcal{R}(x) \\right\\}\n",
    "$$\n",
    "into the inference problem. \n",
    "\n",
    "# A hierarchical prior\n",
    "\n",
    "One reason it can be useful to think probabilistically is because we can motivate different choices of the regularizer $\\mathcal{R}(x)$. If we pick\n",
    "$$\n",
    "Lx &\\sim \\mathcal{N}\\left(0, \\frac{1}{\\gamma} I \\right)\n",
    "$$\n",
    "as our prior (which corresponds to $\\mathcal{R}(x) = \\gamma x^T L^T L x$), then we are saying that we believe \\emph{a priori} that the discrete gradients in our image are distributed according to zero-mean Gaussian with variance $\\gamma^{-1}$. We can tweak the strength of the prior by adjusting $\\gamma$ and in turn its influence on our reconstructed image, but note that the same $\\gamma$ governs \\emph{all} of the entire discrete gradient in the image. Thus we might think to introduce a hierarchical prior on the discrete gradient that could try to (loosely) capture the fact that in some regions in an image the discrete gradient will be much larger than it is elsewhere. Define the prior\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta^H_{i,j}, \\beta^V_{i,j} &\\sim \\Gamma\\left( c, d \\right),\n",
    "Lx &\\sim \\mathcal{N}\\left(0, B_{\\beta} \\right),\n",
    "\\end{align}{#fig-inv_prob_obj}\n",
    "$$\n",
    "which has density \n",
    "$$\n",
    "\\pi(x, \\beta) = \\pi(x \\, | \\, \\beta) \\pi(\\beta) \\propto \\det \\left( B_{\\beta} \\right)^{1/2} \\exp\\left\\{ - x^T L^T B_{\\beta} L x  \\right\\} \\pi(\\beta).\n",
    "$$\\\n",
    "Here $\\left( \\cdot \\right)^{V/H}$ represent the fact that we are assigining two different hyper-parameter to govern the gradient in each the vertical and horizontal directions, $\\Gamma\\left(c, d \\right)$ represents the [gamma density function](https://en.wikipedia.org/wiki/Gamma_distribution), and\n",
    "$$\n",
    "\\pi(beta) &\\propto \\left( \\Prod_{i,j}^{mn} \\Gamma( \\beta_{i,j}^H | c, d) \\right) \\right) \\times \\left( \\Prod_{i,j}^{mn} \\Gamma( \\beta_{i,j}^V | c, d) \\right)\n",
    "$$\n",
    "meaning that all hyper-parameters are assumed to be independent of one another. The reason we use a $\\Gamma$ distribution for the hyper-parameter is because it is a [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior) for a Gaussian, meaning that we can determine certain relevant conditional distributions analytically. \n",
    "\n",
    "# The new posterior\n",
    "\n",
    "Using our prior\n",
    "\n",
    "\n",
    "will consider hierarchical regularization motivated from a hierarchical prior. \n",
    "\n",
    "hierarchical regularization via hierarchical priors.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, N = 3000, 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a random image\n",
    "rand_img = np.random.randn(M,N)\n",
    "for j in range(100):\n",
    "    mat_prod_rand_img = np.fft.fft2(rand_img, norm='ortho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "438a04a772f2aa2bb27a21d03b1b248c801ecadee1108c48b2389b52ed6b9e9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
